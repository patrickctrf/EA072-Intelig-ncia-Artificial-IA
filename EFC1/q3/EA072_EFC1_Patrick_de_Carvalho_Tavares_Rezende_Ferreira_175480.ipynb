{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "@webio": {
      "lastCommId": "4373987a9a754fee8a260ba619c4e8b3",
      "lastKernelId": "374183ba-9524-411d-89a2-c62ca5287fa9"
    },
    "celltoolbar": "Attachments",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "EA072_EFC1_Patrick_de_Carvalho_Tavares_Rezende_Ferreira_175480.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kelx6PRGvdIt",
        "colab_type": "text"
      },
      "source": [
        "# Patrick de Carvalho Tavares Rezende Ferreira - RA: 175480 - EFC2\n",
        "\n",
        "Repositório: https://github.com/patrickctrf/EA072-Inteligencia-Artificial-IA/tree/master/EFC1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxZXlOar-SvE",
        "colab_type": "code",
        "outputId": "e67ebc89-4682-4ccc-f62f-c2cfd4242cb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/PODE\\ APAGAR/EA072-EF1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/PODE APAGAR/EA072-EF1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odrxsaWtvdIx",
        "colab_type": "text"
      },
      "source": [
        "## Questão 1\n",
        "\n",
        "Inicialmente, executou-se 5 vezes o código sugerido inicial a fim de verificar o desempenho da proposta. Seu desempenho foi de:\n",
        "\n",
        "* Loss: 0.0733; Acurácia: 0.9775.\n",
        "\n",
        "Utilizando-se o método de tentativa e erro, foi criado um script que verificava o desempenho da rede para diferentes parâmetros alterados como dropout (0.1 a 0.6), número de camadas (1 a 4 intermediárias), épocas de treinamento (4 a 8) e número de neurônios por camada (256 a 512). O script executava esta mudança de parâmetros dentro de loops \"for\" para executar todas as combinações possíveis e tirava também a média das múltiplas execuções com mesmos parâmetros, a fim de se obter uma média de desempenho mais confiável. Os resultados desta varredura eram salvos ao final das execuções em um arquivo \"listas.txt\", permitindo ao usuários verificar qual a configuração obteve melhor desempenho.\n",
        "Foram utilizadas 4 threads - para varredura de redes de 1 a 4 camadas - durante o treinamento, a fim de promover paralelismo e diminuir o tempo requerido, que chegava a dezenas de horas.\n",
        "\n",
        "Para a proposta final deste modelo, os parâmetros que resultaram no melhor desempenho durante a varredura foram:\n",
        "\n",
        "* Camadas: 2; Neurônios por camada: 512; Dropout: 0.2; Épocas: 8.\n",
        "\n",
        "O desempenho médio obtido foi de:\n",
        "\n",
        "* Loss: 0.0693; Acurácia: 0.9828.\n",
        "\n",
        "Ambas as soluções consumiram um tempo de execução da ordem de poucos minutos e a diferença de desempenho foi cerca de 0,5% em ganho.\n",
        "\n",
        "Os arquivos utilizados foram (no diretório q1):\n",
        "\n",
        "Proposta Inicial: q1Inicial.py\n",
        "Script de Varredura de parâmetros: q1.py\n",
        "Proposta Final: q1Final.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV-GGfHzvdIz",
        "colab_type": "code",
        "outputId": "c7d3dbc6-a843-4853-b4f5-97309c729510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# q1Inicial.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        " tf.keras.layers.Flatten(),\n",
        " tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        " tf.keras.layers.Dropout(0.5),\n",
        " tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        " loss='sparse_categorical_crossentropy',\n",
        " metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "model.evaluate(x_test, y_test)\n",
        "model_json = model.to_json()\n",
        "json_file = open(\"model_MLP.json\", \"w\")\n",
        "json_file.write(model_json)\n",
        "json_file.close()\n",
        "model.save_weights(\"model_MLP.h5\")\n",
        "print(\"Model saved to disk\")\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.2698 - acc: 0.9197\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.1385 - acc: 0.9579\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.1080 - acc: 0.9668\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.0928 - acc: 0.9710\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.0821 - acc: 0.9736\n",
            "10000/10000 [==============================] - 1s 68us/sample - loss: 0.0640 - acc: 0.9797\n",
            "Model saved to disk\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/PODE APAGAR/EA072-EF1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SLFPiOhvdI4",
        "colab_type": "code",
        "outputId": "b6c97708-08d7-41e4-8361-2e1901266ef2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import threading\n",
        "\n",
        "myMutex = threading.Lock()\n",
        "value = \"teste\"\n",
        "\n",
        "numeroDeNeuronios = []\n",
        "numeroDeEpocas = []\n",
        "numeroDeCamadas = []\n",
        "numeroDeDropout = []\n",
        "taxaDeAcertos = []\n",
        "\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread1Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 60, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [8, 4]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de neuronios.\n",
        "\t\tfor neuronios in [256, 512]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(neuronios) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,4):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential([\n",
        "\t\t\t\t\t tf.keras.layers.Flatten(),\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\t\t\t\t\t])\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_MLP1.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_MLP1.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(neuronios)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread2Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 60, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [8, 4]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de neuronios.\n",
        "\t\tfor neuronios in [256, 512]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(neuronios) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,4):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential([\n",
        "\t\t\t\t\t tf.keras.layers.Flatten(),\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\t\t\t\t\t])\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_MLP2.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_MLP2.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(neuronios)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread3Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 60, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [8, 4]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de neuronios.\n",
        "\t\tfor neuronios in [256, 512]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(neuronios) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,4):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential([\n",
        "\t\t\t\t\t tf.keras.layers.Flatten(),\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\t\t\t\t\t])\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_MLP3.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_MLP3.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(neuronios)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread4Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 60, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [8, 4]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de neuronios.\n",
        "\t\tfor neuronios in [256, 512]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(neuronios) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,4):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential([\n",
        "\t\t\t\t\t tf.keras.layers.Flatten(),\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\t\t\t\t\t])\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_MLP4.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_MLP4.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(neuronios)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\t\n",
        "\tcamadas1 = threading.Thread(target=thread1Camadas,args=(1,))\n",
        "\tcamadas2 = threading.Thread(target=thread2Camadas,args=(2,))\n",
        "\tcamadas3 = threading.Thread(target=thread3Camadas,args=(3,))\n",
        "\tcamadas4 = threading.Thread(target=thread4Camadas,args=(4,))\n",
        "\t\n",
        "\tcamadas1.start()\n",
        "\tcamadas2.start()\n",
        "\tcamadas3.start()\n",
        "\tcamadas4.start()\n",
        "\t\n",
        "\ttry:\n",
        "\t\tcamadas4.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\ttry:\n",
        "\t\tcamadas3.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\ttry:\n",
        "\t\tcamadas2.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\ttry:\n",
        "\t\tcamadas1.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\tlistasFile = open(\"listasFFULLYCONNECTED.txt\", \"w\")\n",
        "\tlistasFile.write(str(numeroDeNeuronios) + \"\\n\")\n",
        "\tlistasFile.write(str(numeroDeEpocas) + \"\\n\")\n",
        "\tlistasFile.write(str(numeroDeCamadas) + \"\\n\")\n",
        "\tlistasFile.write(str(numeroDeDropout) + \"\\n\")\n",
        "\tlistasFile.write(str(taxaDeAcertos) + \"\\n\")\n",
        "\tlistasFile.close()\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "epocas: 8\n",
            "CAMADAS1: 256\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epocas: 8\n",
            "CAMADAS2: 256\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epocas: 8\n",
            "CAMADAS3: 256\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epocas: 8\n",
            "CAMADAS4: 256\n",
            "\n",
            "\n",
            "Epoch 1/8\n",
            " 6112/60000 [==>...........................] - ETA: 12s - loss: 0.6049 - acc: 0.8253Epoch 1/8\n",
            "Epoch 1/8 1760/60000 [..............................] - ETA: 32s - loss: 1.0388 - acc: 0.7000\n",
            "12608/60000 [=====>........................] - ETA: 11s - loss: 0.4459 - acc: 0.8698\n",
            "60000/60000 [==============================] - 23s 379us/sample - loss: 0.2369 - acc: 0.9305\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] - 26s 434us/sample - loss: 0.2229 - acc: 0.9330\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 2/8\n",
            " 7616/60000 [==>...........................]60000/60000 [==============================] - ETA: 23s - loss: 0.1076 - acc: 0.9645 - 29s 476us/sample - loss: 0.2287 - acc: 0.9308\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] 3328/60000 [>.............................] - 30s 492us/sample - loss: 0.2444 - acc: 0.9270\n",
            " - ETA: 26s - loss: 0.1149 - acc: 0.9645Epoch 2/8\n",
            "60000/60000 [==============================] - 25s 419us/sample - loss: 0.1021 - acc: 0.9693\n",
            "Epoch 3/8\n",
            "60000/60000 [==============================] - 26s 435us/sample - loss: 0.1005 - acc: 0.9692\n",
            "Epoch 3/8\n",
            "60000/60000 [==============================] - 28s 463us/sample - loss: 0.1098 - acc: 0.9671\n",
            "Epoch 3/854080/60000 [==========================>...]\n",
            "60000/60000 [==============================] - 29s 485us/sample - loss: 0.1237 - acc: 0.9637\n",
            "18080/60000 [========>.....................] - ETA: 18s - loss: 0.0700 - acc: 0.9779Epoch 3/8\n",
            "60000/60000 [==============================] - 25s 418us/sample - loss: 0.0715 - acc: 0.9772\n",
            "30496/60000 [==============>...............] - ETA: 13s - loss: 0.0825 - acc: 0.9740Epoch 4/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "60000/60000 [==============================] - 26s 439us/sample - loss: 0.0723 - acc: 0.9771\n",
            "Epoch 4/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "60000/60000 [==============================] - 28s 465us/sample - loss: 0.0835 - acc: 0.9741\n",
            "33120/60000 [===============>..............] - ETA: 11s - loss: 0.0527 - acc: 0.9832Epoch 4/8\n",
            "60000/60000 [==============================] - 29s 481us/sample - loss: 0.0930 - acc: 0.9727\n",
            "23776/60000 [==========>...................] - ETA: 16s - loss: 0.0547 - acc: 0.9823Epoch 4/8\n",
            "60000/60000 [==============================] - 25s 415us/sample - loss: 0.0541 - acc: 0.9831\n",
            "24288/60000 [===========>..................] - ETA: 16s - loss: 0.0631 - acc: 0.9804Epoch 5/8\n",
            "60000/60000 [==============================] - 26s 435us/sample - loss: 0.0598 - acc: 0.9807\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 28s 459us/sample - loss: 0.0678 - acc: 0.9792\n",
            "18240/60000 [========>.....................] - ETA: 18s - loss: 0.0460 - acc: 0.9859Epoch 5/8\n",
            "11264/60000 [====>.........................]60000/60000 [==============================] - ETA: 22s - loss: 0.0532 - acc: 0.9837 - 29s 480us/sample - loss: 0.0786 - acc: 0.9771\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 25s 419us/sample - loss: 0.0434 - acc: 0.9857\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 27s 446us/sample - loss: 0.0512 - acc: 0.9842\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 28s 460us/sample - loss: 0.0594 - acc: 0.9818\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 29s 484us/sample - loss: 0.0688 - acc: 0.9795\n",
            "14272/60000 [======>.......................] - ETA: 21s - loss: 0.0422 - acc: 0.9870\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 6/8\n",
            "60000/60000 [==============================] - 26s 428us/sample - loss: 0.0354 - acc: 0.9887\n",
            "Epoch 7/8\n",
            "60000/60000 [==============================] - 27s 444us/sample - loss: 0.0441 - acc: 0.9862\n",
            "23136/60000 [==========>...................] - ETA: 17s - loss: 0.0584 - acc: 0.9835Epoch 7/8\n",
            "60000/60000 [==============================] - 27s 458us/sample - loss: 0.0501 - acc: 0.9847\n",
            "Epoch 7/8\n",
            "60000/60000 [==============================] - 25s 418us/sample - loss: 0.0291 - acc: 0.9901\n",
            " 9792/60000 [===>..........................] - ETA: 23s - loss: 0.0360 - acc: 0.9868Epoch 8/8\n",
            "60000/60000 [==============================] - 29s 480us/sample - loss: 0.0607 - acc: 0.9822\n",
            "Epoch 7/8\n",
            "60000/60000 [==============================] - 26s 436us/sample - loss: 0.0372 - acc: 0.9876\n",
            "17888/60000 [=======>......................] - ETA: 20s - loss: 0.0491 - acc: 0.9861Epoch 8/8\n",
            "40992/60000 [===================>..........]60000/60000 [==============================] - ETA: 9s - loss: 0.0525 - acc: 0.9843 - 28s 458us/sample - loss: 0.0460 - acc: 0.9851\n",
            "55168/60000 [==========================>...] - ETA: 2s - loss: 0.0252 - acc: 0.9915Epoch 8/8\n",
            "60000/60000 [==============================] - 25s 416us/sample - loss: 0.0255 - acc: 0.9914\n",
            "10000/10000 [==============================] - 4s 357us/sample - loss: 0.0689 - acc: 0.9810\n",
            "53280/60000 [=========================>....] - ETA: 3s - loss: 0.0540 - acc: 0.9842Model saved to disk\n",
            "18464/60000 [========>.....................] - ETA: 19s - loss: 0.0352 - acc: 0.9888Epoch 1/8\n",
            "60000/60000 [==============================] - 29s 475us/sample - loss: 0.0531 - acc: 0.9845\n",
            "Epoch 8/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "60000/60000 [==============================] - 26s 428us/sample - loss: 0.0316 - acc: 0.9900\n",
            "10000/10000 [==============================] - 4s 378us/sample - loss: 0.0821 - acc: 0.9771\n",
            "41056/60000 [===================>..........] - ETA: 8s - loss: 0.0379 - acc: 0.9877Model saved to disk\n",
            "32800/60000 [===============>..............] - ETA: 11s - loss: 0.3014 - acc: 0.9135Epoch 1/8\n",
            "60000/60000 [==============================] - 27s 450us/sample - loss: 0.0393 - acc: 0.9873\n",
            "10000/10000 [==============================]20096/60000 [=========>....................] - 4s 354us/sample - loss: 0.1359 - acc: 0.9697\n",
            "47840/60000 [======================>.......] - ETA: 5s - loss: 0.0466 - acc: 0.9859Model saved to disk\n",
            "60000/60000 [==============================] - 24s 403us/sample - loss: 0.2339 - acc: 0.9325\n",
            "Epoch 2/8\n",
            "30048/60000 [==============>...............] - ETA: 12s - loss: 0.2928 - acc: 0.9117Epoch 1/8\n",
            "60000/60000 [==============================] - 27s 455us/sample - loss: 0.0479 - acc: 0.9853\n",
            "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0765 - acc: 0.9795\n",
            "43808/60000 [====================>.........] - ETA: 6s - loss: 0.2515 - acc: 0.9236Model saved to disk\n",
            "22080/60000 [==========>...................] - ETA: 17s - loss: 0.3441 - acc: 0.8946Epoch 1/8\n",
            "60000/60000 [==============================] - 25s 416us/sample - loss: 0.2205 - acc: 0.9327\n",
            "37664/60000 [=================>............] - ETA: 8s - loss: 0.1071 - acc: 0.9688Epoch 2/8\n",
            "60000/60000 [==============================] - 24s 392us/sample - loss: 0.1023 - acc: 0.9699\n",
            "22912/60000 [==========>...................] - ETA: 18s - loss: 0.3744 - acc: 0.8853Epoch 3/8\n",
            "60000/60000 [==============================] - 27s 453us/sample - loss: 0.2291 - acc: 0.9299\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] - 29s 482us/sample - loss: 0.2523 - acc: 0.9250\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] - 27s 448us/sample - loss: 0.0977 - acc: 0.9696\n",
            "25760/60000 [===========>..................] - ETA: 15s - loss: 0.1156 - acc: 0.9645Epoch 3/8\n",
            "60000/60000 [==============================] - 25s 414us/sample - loss: 0.0708 - acc: 0.9781\n",
            "Epoch 4/8\n",
            "60000/60000 [==============================] - 28s 463us/sample - loss: 0.1139 - acc: 0.9657\n",
            "33440/60000 [===============>..............] - ETA: 12s - loss: 0.1237 - acc: 0.9644Epoch 3/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "60000/60000 [==============================] - 26s 437us/sample - loss: 0.0699 - acc: 0.9777\n",
            "23008/60000 [==========>...................] - ETA: 16s - loss: 0.0831 - acc: 0.9743Epoch 4/8\n",
            "60000/60000 [==============================] - 29s 479us/sample - loss: 0.1194 - acc: 0.9653\n",
            "Epoch 3/8\n",
            "60000/60000 [==============================] - 25s 418us/sample - loss: 0.0543 - acc: 0.9830\n",
            " 8032/60000 [===>..........................] - ETA: 24s - loss: 0.0825 - acc: 0.9747Epoch 5/8\n",
            "60000/60000 [==============================] - 28s 459us/sample - loss: 0.0853 - acc: 0.9740\n",
            "25888/60000 [===========>..................] - ETA: 14s - loss: 0.0406 - acc: 0.9876Epoch 4/8\n",
            "60000/60000 [==============================] - 27s 443us/sample - loss: 0.0588 - acc: 0.9809\n",
            "48448/60000 [=======================>......] - ETA: 4s - loss: 0.0409 - acc: 0.9872Epoch 5/8\n",
            "60000/60000 [==============================] - 29s 483us/sample - loss: 0.0938 - acc: 0.9725\n",
            "59872/60000 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9867Epoch 4/8\n",
            "60000/60000 [==============================] - 25s 421us/sample - loss: 0.0427 - acc: 0.9868\n",
            "   32/60000 [..............................] - ETA: 44s - loss: 0.0718 - acc: 0.9375Epoch 6/8\n",
            "60000/60000 [==============================] - 28s 464us/sample - loss: 0.0693 - acc: 0.9793\n",
            "29280/60000 [=============>................] - ETA: 14s - loss: 0.0787 - acc: 0.9760Epoch 5/8\n",
            "60000/60000 [==============================] - 27s 446us/sample - loss: 0.0467 - acc: 0.9851\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 25s 412us/sample - loss: 0.0354 - acc: 0.9883\n",
            "Epoch 7/8\n",
            "60000/60000 [==============================] - 28s 474us/sample - loss: 0.0791 - acc: 0.9762\n",
            "14560/60000 [======>.......................] - ETA: 19s - loss: 0.0361 - acc: 0.9876Epoch 5/8\n",
            "60000/60000 [==============================] - 28s 465us/sample - loss: 0.0605 - acc: 0.9809\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 26s 433us/sample - loss: 0.0426 - acc: 0.9862\n",
            "Epoch 7/8\n",
            "60000/60000 [==============================] - 24s 407us/sample - loss: 0.0294 - acc: 0.9901\n",
            "Epoch 8/8\n",
            "60000/60000 [==============================] - 29s 483us/sample - loss: 0.0676 - acc: 0.9805\n",
            "20800/60000 [=========>....................] - ETA: 17s - loss: 0.0353 - acc: 0.9887Epoch 6/8\n",
            "60000/60000 [==============================] - 28s 466us/sample - loss: 0.0511 - acc: 0.9841\n",
            "48320/60000 [=======================>......] - ETA: 4s - loss: 0.0232 - acc: 0.9922Epoch 7/8\n",
            "60000/60000 [==============================] - 25s 420us/sample - loss: 0.0236 - acc: 0.9921\n",
            "60000/60000 [==============================] - ETA: 11s - loss: 0.0595 - acc: 0.9814 - 27s 447us/sample - loss: 0.0373 - acc: 0.9877\n",
            "Epoch 8/8\n",
            "10000/10000 [==============================] - 4s 375us/sample - loss: 0.0729 - acc: 0.9801\n",
            "20000/60000 [=========>....................] - ETA: 18s - loss: 0.0454 - acc: 0.9864Model saved to disk\n",
            "53888/60000 [=========================>....] - ETA: 3s - loss: 0.0614 - acc: 0.9814Epoch 1/8\n",
            "60000/60000 [==============================] - 30s 493us/sample - loss: 0.0606 - acc: 0.9817\n",
            "Epoch 7/8\n",
            "60000/60000 [==============================] - 28s 464us/sample - loss: 0.0472 - acc: 0.9858\n",
            "31488/60000 [==============>...............] - ETA: 12s - loss: 0.3076 - acc: 0.9106Epoch 8/8\n",
            "60000/60000 [==============================] - 27s 458us/sample - loss: 0.0322 - acc: 0.9895\n",
            "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0723 - acc: 0.9808\n",
            "19488/60000 [========>.....................] - ETA: 19s - loss: 0.0432 - acc: 0.9868Model saved to disk\n",
            "60000/60000 [==============================]24224/60000 [===========>..................] - 25s 413us/sample - loss: 0.2396 - acc: 0.9302\n",
            " - ETA: 16s - loss: 0.0442 - acc: 0.9870Epoch 2/8\n",
            "48096/60000 [=======================>......] - ETA: 5s - loss: 0.0543 - acc: 0.9839Epoch 1/8\n",
            "60000/60000 [==============================] - 29s 484us/sample - loss: 0.0564 - acc: 0.9833\n",
            "Epoch 8/8\n",
            "60000/60000 [==============================] - 28s 472us/sample - loss: 0.0457 - acc: 0.9860\n",
            "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0767 - acc: 0.9823\n",
            "29312/60000 [=============>................] - ETA: 14s - loss: 0.0480 - acc: 0.9863Model saved to disk\n",
            "37056/60000 [=================>............] - ETA: 10s - loss: 0.0472 - acc: 0.9868Epoch 1/8\n",
            "60000/60000 [==============================] - 25s 417us/sample - loss: 0.1027 - acc: 0.9683\n",
            "52416/60000 [=========================>....]37376/60000 [=================>............] - ETA: 3s - loss: 0.2329 - acc: 0.9288 - ETA: 10s - loss: 0.0471 - acc: 0.9868Epoch 3/8\n",
            "60000/60000 [==============================] - 27s 447us/sample - loss: 0.2203 - acc: 0.9327\n",
            "44352/60000 [=====================>........] 7808/60000 [==>...........................]\n",
            "60000/60000 [==============================] - 29s 478us/sample - loss: 0.0504 - acc: 0.9856\n",
            "10000/10000 [==============================] - 4s 387us/sample - loss: 0.0945 - acc: 0.9764\n",
            "33888/60000 [===============>..............]37728/60000 [=================>............] - ETA: 12s - loss: 0.2900 - acc: 0.9103 - ETA: 9s - loss: 0.0751 - acc: 0.9768Model saved to disk\n",
            "51136/60000 [========================>.....] - ETA: 3s - loss: 0.0729 - acc: 0.9776Epoch 1/8\n",
            "60000/60000 [==============================] - 24s 394us/sample - loss: 0.0720 - acc: 0.9779\n",
            " 5888/60000 [=>............................] - ETA: 31s - loss: 0.6878 - acc: 0.7824\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 4/8\n",
            "60000/60000 [==============================] - 27s 448us/sample - loss: 0.2302 - acc: 0.9297\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] - 25s 418us/sample - loss: 0.0990 - acc: 0.9693\n",
            " 3328/60000 [>.............................] - ETA: 27s - loss: 0.0946 - acc: 0.9681Epoch 3/8\n",
            "60000/60000 [==============================] - 25s 418us/sample - loss: 0.0553 - acc: 0.9824\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 30s 495us/sample - loss: 0.2513 - acc: 0.9234\n",
            " 2752/60000 [>.............................] - ETA: 24s - loss: 0.0492 - acc: 0.9847Epoch 2/8\n",
            "60000/60000 [==============================] - 28s 467us/sample - loss: 0.1078 - acc: 0.9675\n",
            "58048/60000 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9771Epoch 3/8\n",
            "60000/60000 [==============================] - 27s 456us/sample - loss: 0.0727 - acc: 0.9768\n",
            " 1952/60000 [..............................] - ETA: 27s - loss: 0.0908 - acc: 0.9672Epoch 4/8\n",
            "60000/60000 [==============================] - 25s 425us/sample - loss: 0.0443 - acc: 0.9858\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 30s 500us/sample - loss: 0.1244 - acc: 0.9640\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 3/852608/60000 [=========================>....]\n",
            "60000/60000 [==============================] - 27s 457us/sample - loss: 0.0570 - acc: 0.9818\n",
            "Epoch 5/8\n",
            " 6848/60000 [==>...........................]60000/60000 [==============================] - ETA: 26s - loss: 0.0832 - acc: 0.9747 - 29s 477us/sample - loss: 0.0835 - acc: 0.9745\n",
            "Epoch 4/8\n",
            "60000/60000 [==============================] - 26s 431us/sample - loss: 0.0349 - acc: 0.9884\n",
            "34400/60000 [================>.............] - ETA: 12s - loss: 0.0653 - acc: 0.9801Epoch 7/8\n",
            "60000/60000 [==============================] - 31s 509us/sample - loss: 0.0957 - acc: 0.9722\n",
            "59456/60000 [============================>.]Epoch 4/8\n",
            "60000/60000 [==============================] - 28s 461us/sample - loss: 0.0496 - acc: 0.9834\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 29s 484us/sample - loss: 0.0686 - acc: 0.9792\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 26s 429us/sample - loss: 0.0294 - acc: 0.9903\n",
            "Epoch 8/8\n",
            "60000/60000 [==============================] - 27s 457us/sample - loss: 0.0410 - acc: 0.9865\n",
            "54688/60000 [==========================>...]Epoch 7/8\n",
            "60000/60000 [==============================] - 29s 487us/sample - loss: 0.0787 - acc: 0.9767\n",
            " 3616/60000 [>.............................] - ETA: 24s - loss: 0.0283 - acc: 0.9898Epoch 5/8\n",
            "60000/60000 [==============================] 5568/60000 [=>............................] - 28s 470us/sample - loss: 0.0604 - acc: 0.9813\n",
            " - ETA: 24s - loss: 0.0268 - acc: 0.9910Epoch 6/8\n",
            "60000/60000 [==============================] - 26s 431us/sample - loss: 0.0253 - acc: 0.9917\n",
            "10000/10000 [==============================] - 4s 375us/sample - loss: 0.0711 - acc: 0.9795\n",
            "33440/60000 [===============>..............] - ETA: 12s - loss: 0.0685 - acc: 0.9797Model saved to disk\n",
            "47136/60000 [======================>.......] - ETA: 5s - loss: 0.0341 - acc: 0.9886Epoch 1/8\n",
            "60000/60000 [==============================] - 26s 434us/sample - loss: 0.0346 - acc: 0.9884\n",
            "50752/60000 [========================>.....] - ETA: 4s - loss: 0.0686 - acc: 0.9797Epoch 8/8\n",
            "60000/60000 [==============================] - 28s 459us/sample - loss: 0.0509 - acc: 0.9839\n",
            "58688/60000 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9797Epoch 7/8\n",
            "60000/60000 [==============================] - 29s 486us/sample - loss: 0.0694 - acc: 0.9797\n",
            " 9984/60000 [===>..........................] - ETA: 23s - loss: 0.0244 - acc: 0.9918Epoch 6/8\n",
            "60000/60000 [==============================] - 26s 426us/sample - loss: 0.2490 - acc: 0.9269\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] - 28s 460us/sample - loss: 0.0333 - acc: 0.9890\n",
            "60000/60000 [==============================] - 28s 459us/sample - loss: 0.0455 - acc: 0.9859\n",
            "Epoch 8/8\n",
            "10000/10000 [==============================] - 4s 391us/sample - loss: 0.0731 - acc: 0.9803\n",
            "56704/60000 [===========================>..]Model saved to disk - ETA: 1s - loss: 0.0589 - acc: 0.9826\n",
            "60000/60000 [==============================] - 29s 481us/sample - loss: 0.0593 - acc: 0.9825\n",
            "Epoch 7/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            " 4864/60000 [=>............................] - ETA: 24s - loss: 0.0421 - acc: 0.9877Epoch 1/8\n",
            "60000/60000 [==============================] - 24s 404us/sample - loss: 0.1132 - acc: 0.9664\n",
            "Epoch 3/8\n",
            "60000/60000 [==============================] - 27s 458us/sample - loss: 0.0408 - acc: 0.9875\n",
            "60000/60000 [==============================] - 29s 476us/sample - loss: 0.0537 - acc: 0.9843\n",
            "43232/60000 [====================>.........] - ETA: 6s - loss: 0.0836 - acc: 0.9732Epoch 8/8\n",
            "60000/60000 [==============================] - 27s 445us/sample - loss: 0.2484 - acc: 0.9248\n",
            "Epoch 2/8\n",
            "10000/10000 [==============================] - 4s 371us/sample - loss: 0.0868 - acc: 0.9798\n",
            " 2656/60000 [>.............................] - ETA: 30s - loss: 0.0440 - acc: 0.9880Model saved to disk\n",
            "11552/60000 [====>.........................] - ETA: 20s - loss: 0.1213 - acc: 0.9619Epoch 1/8\n",
            "60000/60000 [==============================] - 24s 405us/sample - loss: 0.0831 - acc: 0.9736\n",
            "Epoch 4/8\n",
            "60000/60000 [==============================] - 27s 447us/sample - loss: 0.1159 - acc: 0.9639\n",
            "Epoch 3/846304/60000 [======================>.......] - ETA: 6s - loss: 0.2920 - acc: 0.9104\n",
            "60000/60000 [==============================] - 29s 484us/sample - loss: 0.0479 - acc: 0.9863\n",
            "60000/60000 [==============================] - 25s 418us/sample - loss: 0.0653 - acc: 0.9794\n",
            "Epoch 5/8\n",
            "10000/10000 [==============================] - 4s 363us/sample - loss: 0.0785 - acc: 0.9799\n",
            "60000/60000 [==============================] - 28s 468us/sample - loss: 0.2632 - acc: 0.9195\n",
            "14688/60000 [======>.......................] - ETA: 19s - loss: 0.0876 - acc: 0.9732Epoch 2/8\n",
            "15776/60000 [======>.......................] - ETA: 18s - loss: 0.0869 - acc: 0.9730Model saved to disk\n",
            "19200/60000 [========>.....................] - ETA: 14s - loss: 0.0521 - acc: 0.9834Epoch 1/8\n",
            "60000/60000 [==============================] - 25s 424us/sample - loss: 0.0923 - acc: 0.9713\n",
            "44096/60000 [=====================>........] - ETA: 6s - loss: 0.1359 - acc: 0.9593Epoch 4/8\n",
            "60000/60000 [==============================] 6624/60000 [==>...........................] - 24s 403us/sample - loss: 0.0542 - acc: 0.9826\n",
            " - ETA: 25s - loss: 0.0616 - acc: 0.9802Epoch 6/8\n",
            "60000/60000 [==============================] - 27s 449us/sample - loss: 0.1332 - acc: 0.9600\n",
            "10432/60000 [====>.........................] - ETA: 21s - loss: 0.0445 - acc: 0.9868Epoch 3/8\n",
            "17920/60000 [=======>......................]60000/60000 [==============================] - ETA: 19s - loss: 0.1048 - acc: 0.9670 - 30s 503us/sample - loss: 0.2882 - acc: 0.9119\n",
            "30560/60000 [==============>...............] - ETA: 12s - loss: 0.0452 - acc: 0.9858Epoch 2/8\n",
            "60000/60000 [==============================] - 27s 445us/sample - loss: 0.0739 - acc: 0.9762\n",
            "41408/60000 [===================>..........] - ETA: 8s - loss: 0.1045 - acc: 0.9689Epoch 5/8\n",
            "60000/60000 [==============================] - 25s 418us/sample - loss: 0.0454 - acc: 0.9850\n",
            "Epoch 7/8\n",
            "60000/60000 [==============================] - 28s 459us/sample - loss: 0.1046 - acc: 0.9689\n",
            "Epoch 4/8\n",
            "60000/60000 [==============================] - 29s 477us/sample - loss: 0.1479 - acc: 0.9578\n",
            "Epoch 3/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "18176/60000 [========>.....................]60000/60000 [==============================] - ETA: 20s - loss: 0.1150 - acc: 0.9663 - 27s 447us/sample - loss: 0.0637 - acc: 0.9798\n",
            "Epoch 6/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "60000/60000 [==============================] - 25s 422us/sample - loss: 0.0388 - acc: 0.9875\n",
            "Epoch 8/8\n",
            "51328/60000 [========================>.....] - ETA: 3s - loss: 0.0882 - acc: 0.9736Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2adCE0ogvdI-",
        "colab_type": "code",
        "outputId": "decb98c0-9de2-4664-e9e6-584657ec7bc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# q1Final.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        " tf.keras.layers.Flatten(),\n",
        " tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        " tf.keras.layers.Dropout(0.2),\n",
        " tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        " tf.keras.layers.Dropout(0.2),\n",
        " tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        " loss='sparse_categorical_crossentropy',\n",
        " metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=8)\n",
        "model.evaluate(x_test, y_test)\n",
        "model_json = model.to_json()\n",
        "json_file = open(\"model_MLP.json\", \"w\")\n",
        "json_file.write(model_json)\n",
        "json_file.close()\n",
        "model.save_weights(\"model_MLP.h5\")\n",
        "print(\"Model saved to disk\")\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "60000/60000 [==============================] - 8s 135us/sample - loss: 0.2130 - acc: 0.9347\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] - 8s 128us/sample - loss: 0.1075 - acc: 0.9674\n",
            "Epoch 3/8\n",
            "60000/60000 [==============================] - 8s 129us/sample - loss: 0.0810 - acc: 0.9745\n",
            "Epoch 4/8\n",
            "60000/60000 [==============================] - 8s 125us/sample - loss: 0.0658 - acc: 0.9791\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 8s 127us/sample - loss: 0.0563 - acc: 0.9820\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 8s 127us/sample - loss: 0.0505 - acc: 0.9839\n",
            "Epoch 7/8\n",
            "60000/60000 [==============================] - 8s 127us/sample - loss: 0.0472 - acc: 0.9856\n",
            "Epoch 8/8\n",
            "60000/60000 [==============================] - 8s 128us/sample - loss: 0.0431 - acc: 0.9876\n",
            "10000/10000 [==============================] - 1s 77us/sample - loss: 0.0721 - acc: 0.9828\n",
            "Model saved to disk\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/PODE APAGAR/EA072-EF1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoUFM86cvdJC",
        "colab_type": "text"
      },
      "source": [
        "## Questão 2\n",
        "\n",
        "Inicialmente, executou-se 5 vezes o código sugerido inicial a fim de verificar o desempenho da proposta. Seu desempenho foi de:\n",
        "\n",
        "* Loss: 0.0260; Acurácia: 0.9909.\n",
        "\n",
        "Utilizando-se o método de tentativa e erro, foi criado um script que verificava o desempenho da rede para diferentes parâmetros alterados como dropout (0.1 a 0.6), número de filtros (32 a 64), épocas de treinamento (2 a 6) e formato dos kernel utilizados (2x2 ou 3x3). O script executava esta mudança de parâmetros dentro de loops \"for\" para executar todas as combinações possíveis e tirava também a média das múltiplas execuções com mesmos parâmetros, a fim de se obter uma média de desempenho mais confiável. Os resultados desta varredura eram salvos ao final das execuções em um arquivo \"listas.txt\", permitindo ao usuários verificar qual a configuração obteve melhor desempenho.\n",
        "Foram utilizadas 4 threads - para varredura de redes de 1 a 4 camadas - durante o treinamento, a fim de promover paralelismo e diminuir o tempo requerido, que era ainda maior que o demandado para a questão 1. Verificou-se que com 6 épocas de treinamento o resultdo era levemente melhorado, mas não siginficativamente. A variação das demais grandezas fazia o desempenho diminuir nos testes. Então, após a varredura, foi realizado mais uma tentativa de treinamento com adição de uma camada convolucional e dropout de 0.3, o que elevou os resultados e nos trouxe à proposta final de código para esta questão.\n",
        "\n",
        "Para a proposta final deste modelo, os parâmetros alterados que resultaram no melhor desempenho durante a varredura foram:\n",
        "\n",
        "* Adição de uma camada convolucional (seguida de uma max pool) após a saída da primeira layer de max pool; 6 épocas de treinamento. Os demais parâmetros foram mantidos por não apresentar vantagem média significativa.\n",
        "\n",
        "O desempenho médio obtido foi de:\n",
        "\n",
        "* Loss: 0.0190; Acurácia: 0.9931.\n",
        "\n",
        "Ambas as soluções consumiram um tempo de execução da ordem de poucos minutos e a diferença de desempenho foi cerca de 0,22% em ganho.\n",
        "\n",
        "Os arquivos utilizados foram (no diretório q2):\n",
        "\n",
        "Proposta Inicial: q2Inicial.py\n",
        "Script de Varredura de parâmetros: q2.py\n",
        "Proposta Final: q2Final.py\n",
        "\n",
        "## Comparação entre ELM, MLP e CNN\n",
        "\n",
        "Desempenho:\n",
        "* ELM: 91,09%\n",
        "* MLP: 98,28%\n",
        "* CNN: 99,31%\n",
        "\n",
        "Nota-se claramente que a CNN apresenta o melhor desempenho dentre as 3 melhores técnicas utilizadas. Porém, o processo de treinamento para otimização desta toma dezenas de horas, enquanto que a ELM requer apenas alguns minutos para ser ajustada e ficar cerca de 8% abaixo em desempenho. Logo, se houver recursos computacionais, a CNN é a melhor escolha para este tipo de problema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PED7944vdJE",
        "colab_type": "code",
        "outputId": "ca057a82-a6d9-4fa8-a31d-39d5224fd433",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# q2Inicial.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "# reshape to be [samples][width][height][pixels]\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
        " activation='relu',\n",
        "input_shape=(28, 28, 1)))\n",
        "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        " loss='sparse_categorical_crossentropy',\n",
        " metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "model.evaluate(x_test, y_test)\n",
        "model_json = model.to_json()\n",
        "json_file = open(\"model_CNN.json\", \"w\")\n",
        "json_file.write(model_json)\n",
        "json_file.close()\n",
        "model.save_weights(\"model_CNN.h5\")\n",
        "print(\"Model saved to disk\")\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 16s 268us/sample - loss: 0.2004 - acc: 0.9403\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 15s 256us/sample - loss: 0.0851 - acc: 0.9741\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 15s 251us/sample - loss: 0.0628 - acc: 0.9809\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 15s 252us/sample - loss: 0.0532 - acc: 0.9837\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 15s 249us/sample - loss: 0.0461 - acc: 0.9857\n",
            "10000/10000 [==============================] - 1s 114us/sample - loss: 0.0294 - acc: 0.9906\n",
            "Model saved to disk\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/PODE APAGAR/EA072-EF1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY-q7n4UvdJH",
        "colab_type": "code",
        "outputId": "21f68464-4a4f-43f0-8707-9fbbf90c7416",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# q2.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import threading\n",
        "\n",
        "myMutex = threading.Lock()\n",
        "value = \"teste\"\n",
        "\n",
        "numeroDeNeuronios = []\n",
        "numeroDeEpocas = []\n",
        "numeroDeCamadas = []\n",
        "numeroDeDropout = []\n",
        "taxaDeAcertos = []\n",
        "\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread1Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 40, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [2, 6]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de filtros.\n",
        "\t\tfor filtros in [32, 64]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(filtros) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,3):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\t# reshape to be [samples][width][height][pixels]\n",
        "\t\t\t\t\tx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential()\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros, kernel_size=(3, 3),\n",
        "\t\t\t\t\t activation='relu',\n",
        "\t\t\t\t\tinput_shape=(28, 28, 1)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (3, 3), activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Flatten())\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_CNN1.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_CNN1.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(filtros)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread2Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 40, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [2, 6]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de filtros.\n",
        "\t\tfor filtros in [32, 64]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(filtros) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,3):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\t# reshape to be [samples][width][height][pixels]\n",
        "\t\t\t\t\tx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential()\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros, kernel_size=(2, 2),\n",
        "\t\t\t\t\t activation='relu',\n",
        "\t\t\t\t\tinput_shape=(28, 28, 1)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (3, 3), activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (3, 3), activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Flatten())\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_CNN2.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_CNN2.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(filtros)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread3Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 40, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [2, 6]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de filtros.\n",
        "\t\tfor filtros in [32, 64]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(filtros) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,3):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\t# reshape to be [samples][width][height][pixels]\n",
        "\t\t\t\t\tx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential()\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros, kernel_size=(3, 3),\n",
        "\t\t\t\t\t activation='relu',\n",
        "\t\t\t\t\tinput_shape=(28, 28, 1)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (2, 2), activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Flatten())\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_CNN3.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_CNN3.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(filtros)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread4Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 40, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [2, 6]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de filtros.\n",
        "\t\tfor filtros in [32, 64]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(filtros) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,3):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\t# reshape to be [samples][width][height][pixels]\n",
        "\t\t\t\t\tx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential()\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros, kernel_size=(3, 3),\n",
        "\t\t\t\t\t activation='relu',\n",
        "\t\t\t\t\tinput_shape=(28, 28, 1)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (3, 3), activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (3, 3), activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Flatten())\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_CNN4.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_CNN4.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(filtros)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\t\n",
        "\tcamadas1 = threading.Thread(target=thread1Camadas,args=(1,))\n",
        "\tcamadas2 = threading.Thread(target=thread2Camadas,args=(2,))\n",
        "\tcamadas3 = threading.Thread(target=thread3Camadas,args=(3,))\n",
        "\tcamadas4 = threading.Thread(target=thread4Camadas,args=(4,))\n",
        "\t\n",
        "\tcamadas1.start()\n",
        "\tcamadas2.start()\n",
        "\tcamadas3.start()\n",
        "\tcamadas4.start()\n",
        "\t\n",
        "\ttry:\n",
        "\t\tcamadas4.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\ttry:\n",
        "\t\tcamadas3.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\ttry:\n",
        "\t\tcamadas2.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\ttry:\n",
        "\t\tcamadas1.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\tlistasFile = open(\"listasCONV.txt\", \"w\")\n",
        "\tlistasFile.write(str(numeroDeNeuronios) + \"\\n\")\n",
        "\tlistasFile.write(str(numeroDeEpocas) + \"\\n\")\n",
        "\tlistasFile.write(str(numeroDeCamadas) + \"\\n\")\n",
        "\tlistasFile.write(str(numeroDeDropout) + \"\\n\")\n",
        "\tlistasFile.write(str(taxaDeAcertos) + \"\\n\")\n",
        "\tlistasFile.close()\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS1: 32\n",
            "\n",
            "\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS2: 32\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS3: 32\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS4: 32\n",
            "\n",
            "\n",
            "Epoch 1/2\n",
            "  288/60000 [..............................] - ETA: 1:33 - loss: 1.8135 - acc: 0.4167Epoch 1/2\n",
            "  992/60000 [..............................] - ETA: 1:21 - loss: 2.0883 - acc: 0.2984Epoch 1/2\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            " 1504/60000 [..............................] - ETA: 1:04 - loss: 1.7619 - acc: 0.4116Epoch 1/2\n",
            "60000/60000 [==============================] - 49s 822us/sample - loss: 0.1270 - acc: 0.9617\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 52s 870us/sample - loss: 0.1650 - acc: 0.9486\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 52s 860us/sample - loss: 0.1242 - acc: 0.9617\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 52s 866us/sample - loss: 0.1229 - acc: 0.9619\n",
            "  768/60000 [..............................] - ETA: 48s - loss: 0.0613 - acc: 0.9831Epoch 2/2\n",
            "60000/60000 [==============================] - 51s 851us/sample - loss: 0.0459 - acc: 0.9858\n",
            "60000/60000 [==============================] - 52s 859us/sample - loss: 0.0510 - acc: 0.9843\n",
            "60000/60000 [==============================] - 51s 847us/sample - loss: 0.0430 - acc: 0.9866\n",
            "60000/60000 [==============================] - 51s 856us/sample - loss: 0.0432 - acc: 0.9869\n",
            "10000/10000 [==============================] - 6s 559us/sample - loss: 0.0365 - acc: 0.9876\n",
            " 1376/10000 [===>..........................] 3904/10000 [==========>...................] - ETA: 4s - loss: 0.0401 - acc: 0.9855 - ETA: 2s - loss: 0.0512 - acc: 0.9834Model saved to disk\n",
            "10000/10000 [==============================] - 4s 381us/sample - loss: 0.0422 - acc: 0.9862\n",
            "10000/10000 [==============================] - 4s 403us/sample - loss: 0.0334 - acc: 0.9891\n",
            " 7872/10000 [======================>.......] - ETA: 0s - loss: 0.0334 - acc: 0.9892Model saved to disk\n",
            " 8672/10000 [=========================>....] - ETA: 0s - loss: 0.0328 - acc: 0.9895\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bModel saved to disk\n",
            "10000/10000 [==============================] - 3s 327us/sample - loss: 0.0323 - acc: 0.9896\n",
            "Model saved to disk\n",
            "Epoch 1/2\n",
            " 9504/60000 [===>..........................] - ETA: 20s - loss: 0.3375 - acc: 0.8932Epoch 1/2\n",
            "10592/60000 [====>.........................] - ETA: 19s - loss: 0.3201 - acc: 0.8991Epoch 1/2\n",
            "11136/60000 [====>.........................] - ETA: 19s - loss: 0.3094 - acc: 0.9027Epoch 1/2\n",
            "60000/60000 [==============================] - 46s 769us/sample - loss: 0.1238 - acc: 0.9611\n",
            "47488/60000 [======================>.......] - ETA: 11s - loss: 0.1915 - acc: 0.9385Epoch 2/2\n",
            "60000/60000 [==============================] - 52s 872us/sample - loss: 0.1215 - acc: 0.9631\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 53s 876us/sample - loss: 0.1654 - acc: 0.9469\n",
            "  768/60000 [..............................] - ETA: 51s - loss: 0.0354 - acc: 0.9909Epoch 2/2\n",
            " 1856/60000 [..............................]60000/60000 [==============================] - 53s 888us/sample - loss: 0.1258 - acc: 0.9614\n",
            " - ETA: 50s - loss: 0.0392 - acc: 0.9898 - ETA: 38s - loss: 0.0414 - acc: 0.9876Epoch 2/2\n",
            "44512/60000 [=====================>........] - ETA: 13s - loss: 0.0422 - acc: 0.987660000/60000 [==============================] - 50s 839us/sample - loss: 0.0434 - acc: 0.9865\n",
            "10000/10000 [==============================] - 7s 686us/sample - loss: 0.0344 - acc: 0.9885\n",
            "54240/60000 [==========================>...] - ETA: 4s - loss: 0.0418 - acc: 0.9875Model saved to disk\n",
            "60000/60000 [==============================] - 50s 840us/sample - loss: 0.0427 - acc: 0.9863\n",
            "60000/60000 [==============================] - 50s 839us/sample - loss: 0.0496 - acc: 0.9850\n",
            "59136/60000 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9875Epoch 1/2\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "60000/60000 [==============================] - 50s 839us/sample - loss: 0.0416 - acc: 0.9875\n",
            "10000/10000 [==============================] 5696/60000 [=>............................] - 4s 435us/sample - loss: 0.0402 - acc: 0.9862\n",
            " 6464/10000 [==================>...........] - ETA: 1s - loss: 0.0276 - acc: 0.9901\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bModel saved to disk\n",
            "10000/10000 [==============================] - 5s 499us/sample - loss: 0.0395 - acc: 0.9864\n",
            " 8480/60000 [===>..........................] - ETA: 32s - loss: 0.4509 - acc: 0.8598Model saved to disk\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "10000/10000 [==============================] 8864/60000 [===>..........................] - 5s 490us/sample - loss: 0.0226 - acc: 0.9920\n",
            "10176/60000 [====>.........................] - ETA: 30s - loss: 0.4048 - acc: 0.8755Model saved to disk\n",
            "16736/60000 [=======>......................] - ETA: 22s - loss: 0.3068 - acc: 0.9071Epoch 1/2\n",
            " 3808/60000 [>.............................] - ETA: 41s - loss: 0.6087 - acc: 0.8072Epoch 1/2\n",
            "23264/60000 [==========>...................] - ETA: 19s - loss: 0.2558 - acc: 0.9233Epoch 1/2\n",
            "60000/60000 [==============================] - 44s 727us/sample - loss: 0.1517 - acc: 0.9544\n",
            "42400/60000 [====================>.........] - ETA: 14s - loss: 0.1568 - acc: 0.9520Epoch 2/2\n",
            "60000/60000 [==============================]17280/60000 [=======>......................] - 50s 831us/sample - loss: 0.1301 - acc: 0.9600\n",
            " - ETA: 37s - loss: 0.0582 - acc: 0.9823Epoch 2/2\n",
            "60000/60000 [==============================] - 53s 879us/sample - loss: 0.1961 - acc: 0.9370\n",
            "24256/60000 [===========>..................] - ETA: 30s - loss: 0.0578 - acc: 0.9824\n",
            "60000/60000 [==============================] - 53s 886us/sample - loss: 0.1372 - acc: 0.9575\n",
            " 1120/60000 [..............................] - ETA: 53s - loss: 0.0643 - acc: 0.9812Epoch 2/2\n",
            "60000/60000 [==============================] - 51s 858us/sample - loss: 0.0561 - acc: 0.9828\n",
            "10000/10000 [==============================] - 7s 663us/sample - loss: 0.0444 - acc: 0.9855\n",
            "44224/60000 [=====================>........] - ETA: 13s - loss: 0.0500 - acc: 0.9847Model saved to disk\n",
            "51232/60000 [========================>.....] - ETA: 7s - loss: 0.0617 - acc: 0.9809Epoch 1/2\n",
            "60000/60000 [==============================] - 49s 811us/sample - loss: 0.0476 - acc: 0.9852\n",
            "60000/60000 [==============================] - 49s 812us/sample - loss: 0.0613 - acc: 0.9812\n",
            "10000/10000 [==============================] - 7s 687us/sample - loss: 0.0332 - acc: 0.9880\n",
            "60000/60000 [==============================] - 50s 826us/sample - loss: 0.0504 - acc: 0.9844\n",
            " - ETA: 36s - loss: 0.3671 - acc: 0.8889 2688/10000 [=======>......................] - ETA: 4s - loss: 0.0465 - acc: 0.9855Model saved to disk\n",
            "10000/10000 [==============================]16960/60000 [=======>......................] - 5s 456us/sample - loss: 0.0287 - acc: 0.9910\n",
            "18208/60000 [========>.....................] - ETA: 27s - loss: 0.2861 - acc: 0.9138Model saved to disk\n",
            " 9856/10000 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9915\n",
            "10000/10000 [==============================] - 4s 429us/sample - loss: 0.0251 - acc: 0.9916\n",
            "  992/60000 [..............................] - ETA: 54s - loss: 1.2646 - acc: 0.5978Model saved to disk\n",
            " 8960/60000 [===>..........................] - ETA: 30s - loss: 0.3878 - acc: 0.8802Epoch 1/2\n",
            "10528/60000 [====>.........................]30944/60000 [==============>...............] - ETA: 28s - loss: 0.3538 - acc: 0.8903 - ETA: 17s - loss: 0.2135 - acc: 0.9359Epoch 1/2\n",
            "60000/60000 [==============================] - 43s 716us/sample - loss: 0.1497 - acc: 0.9546\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 48s 803us/sample - loss: 0.1308 - acc: 0.9597\n",
            "22080/60000 [==========>...................] - ETA: 31s - loss: 0.0559 - acc: 0.9824Epoch 2/2\n",
            "60000/60000 [==============================] - 52s 870us/sample - loss: 0.1849 - acc: 0.9408\n",
            "59712/60000 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9552Epoch 2/2\n",
            "60000/60000 [==============================] - 52s 861us/sample - loss: 0.1431 - acc: 0.9552\n",
            "  384/60000 [..............................] - ETA: 43s - loss: 0.0399 - acc: 0.9870Epoch 2/2\n",
            "25632/60000 [===========>..................]60000/60000 [==============================] - ETA: 30s - loss: 0.0634 - acc: 0.9808 - 50s 833us/sample - loss: 0.0557 - acc: 0.9826\n",
            "10000/10000 [==============================] - 7s 692us/sample - loss: 0.0404 - acc: 0.9863\n",
            "46592/60000 [======================>.......]35456/60000 [================>.............] - ETA: 11s - loss: 0.0487 - acc: 0.9851 - ETA: 20s - loss: 0.0542 - acc: 0.9831Model saved to disk\n",
            "40320/60000 [===================>..........] - ETA: 16s - loss: 0.0610 - acc: 0.9809Epoch 1/2\n",
            "60000/60000 [==============================] - 51s 849us/sample - loss: 0.0487 - acc: 0.9850\n",
            "10000/10000 [==============================] - 7s 680us/sample - loss: 0.0326 - acc: 0.9892\n",
            "18240/60000 [========>.....................] - ETA: 34s - loss: 0.2834 - acc: 0.9126Model saved to disk\n",
            "60000/60000 [==============================] - 50s 833us/sample - loss: 0.0607 - acc: 0.9815\n",
            "60000/60000 [==============================] - 50s 835us/sample - loss: 0.0509 - acc: 0.9845\n",
            " 8288/10000 [=======================>......] - ETA: 0s - loss: 0.0270 - acc: 0.9906Epoch 1/2\n",
            "10000/10000 [==============================] - 5s 457us/sample - loss: 0.0252 - acc: 0.9914\n",
            "10000/10000 [==============================] - 5s 486us/sample - loss: 0.0356 - acc: 0.9888\n",
            "29696/60000 [=============>................]\n",
            "31072/60000 [==============>...............] - ETA: 20s - loss: 0.2140 - acc: 0.9350Model saved to disk\n",
            "41152/60000 [===================>..........] - ETA: 12s - loss: 0.1846 - acc: 0.9438Epoch 1/2\n",
            "  416/60000 [..............................] - ETA: 2:32 - loss: 2.2737 - acc: 0.1514\n",
            "60000/60000 [==============================] - 43s 712us/sample - loss: 0.1542 - acc: 0.9530\n",
            "30432/60000 [==============>...............] - ETA: 22s - loss: 0.2103 - acc: 0.9360Epoch 2/2\n",
            "60000/60000 [==============================] - 49s 809us/sample - loss: 0.1484 - acc: 0.9548\n",
            "45248/60000 [=====================>........] - ETA: 13s - loss: 0.2581 - acc: 0.9160\n",
            "60000/60000 [==============================] - 53s 890us/sample - loss: 0.2196 - acc: 0.9287\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 53s 886us/sample - loss: 0.1605 - acc: 0.9515\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 50s 834us/sample - loss: 0.0629 - acc: 0.9803\n",
            "10000/10000 [==============================] - 7s 700us/sample - loss: 0.0408 - acc: 0.9868\n",
            "39520/60000 [==================>...........]Model saved to disk - ETA: 16s - loss: 0.0585 - acc: 0.9827\n",
            "28416/60000 [=============>................] - ETA: 26s - loss: 0.0802 - acc: 0.9753Epoch 1/2\n",
            "60000/60000 [==============================] - 50s 827us/sample - loss: 0.0571 - acc: 0.9830\n",
            "10000/10000 [==============================] - 7s 707us/sample - loss: 0.0322 - acc: 0.9888\n",
            "53760/60000 [=========================>....] - ETA: 5s - loss: 0.0749 - acc: 0.9769\n",
            "58976/60000 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9823Epoch 1/2\n",
            "60000/60000 [==============================] - 50s 826us/sample - loss: 0.0742 - acc: 0.9773\n",
            "60000/60000 [==============================] - 49s 815us/sample - loss: 0.0595 - acc: 0.9824\n",
            "10000/10000 [==============================] - 6s 598us/sample - loss: 0.0306 - acc: 0.9902\n",
            "10000/10000 [==============================] - 6s 606us/sample - loss: 0.0297 - acc: 0.9896\n",
            "11744/60000 [====>.........................]44608/60000 [=====================>........] - ETA: 33s - loss: 0.3251 - acc: 0.8971 - ETA: 11s - loss: 0.1879 - acc: 0.9429Model saved to disk\n",
            "12544/60000 [=====>........................] - ETA: 32s - loss: 0.3172 - acc: 0.8996Model saved to disk\n",
            "22912/60000 [==========>...................] - ETA: 23s - loss: 0.2340 - acc: 0.9270Epoch 1/2\n",
            "57312/60000 [===========================>..] - ETA: 1s - loss: 0.1650 - acc: 0.9501Epoch 1/2\n",
            "60000/60000 [==============================] - 43s 709us/sample - loss: 0.1613 - acc: 0.9513\n",
            "Epoch 2/2\n",
            "34720/60000 [================>.............] - 46s 767us/sample - loss: 0.1442 - acc: 0.9555\n",
            " - ETA: 22s - loss: 0.3195 - acc: 0.8963Epoch 2/2\n",
            "60000/60000 [==============================] - 53s 891us/sample - loss: 0.2312 - acc: 0.9258\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 52s 860us/sample - loss: 0.0632 - acc: 0.9804\n",
            "60000/60000 [==============================] - 54s 895us/sample - loss: 0.1590 - acc: 0.9503\n",
            "Epoch 2/2\n",
            "10000/10000 [==============================] - 7s 702us/sample - loss: 0.0400 - acc: 0.9870\n",
            "10368/60000 [====>.........................] - ETA: 39s - loss: 0.0609 - acc: 0.9797Model saved to disk\n",
            "\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS3: 64\n",
            "\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "17536/60000 [=======>......................] - ETA: 32s - loss: 0.0863 - acc: 0.9731Epoch 1/2\n",
            "60000/60000 [==============================] - 52s 859us/sample - loss: 0.0584 - acc: 0.9825\n",
            "10000/10000 [==============================] - 8s 801us/sample - loss: 0.0332 - acc: 0.9883\n",
            "43712/60000 [====================>.........] - ETA: 14s - loss: 0.0614 - acc: 0.9814\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bModel saved to disk\n",
            "44832/60000 [=====================>........]\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS1: 64\n",
            "\n",
            "26208/60000 [============>.................] - ETA: 13s - loss: 0.0811 - acc: 0.9744\n",
            "49568/60000 [=======================>......] - ETA: 9s - loss: 0.0608 - acc: 0.9817Epoch 1/2\n",
            "60000/60000 [==============================] - 53s 877us/sample - loss: 0.0794 - acc: 0.9753\n",
            "60000/60000 [==============================] - 54s 892us/sample - loss: 0.0594 - acc: 0.9821\n",
            "10000/10000 [==============================] - 8s 786us/sample - loss: 0.0337 - acc: 0.9896\n",
            " 9888/10000 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9887Model saved to disk\n",
            "\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS4: 64\n",
            "\n",
            "\n",
            "10000/10000 [==============================] - 8s 778us/sample - loss: 0.0337 - acc: 0.9888\n",
            "53056/60000 [=========================>....] - ETA: 6s - loss: 0.1227 - acc: 0.9617Model saved to disk\n",
            "20256/60000 [=========>....................] - ETA: 39s - loss: 0.1842 - acc: 0.9417\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS2: 64\n",
            "\n",
            "\n",
            "27136/60000 [============>.................]60000/60000 [==============================] - ETA: 30s - loss: 0.1564 - acc: 0.9507 - 55s 917us/sample - loss: 0.1148 - acc: 0.9643\n",
            "Epoch 2/2\n",
            " 1024/60000 [..............................] - ETA: 41s - loss: 0.0322 - acc: 0.9922Epoch 1/2\n",
            "29760/60000 [=============>................] - ETA: 27s - loss: 0.1474 - acc: 0.9535Epoch 1/2\n",
            "60000/60000 [==============================] - 69s 1ms/sample - loss: 0.1064 - acc: 0.9671\n",
            "30304/60000 [==============>...............]33920/60000 [===============>..............] - ETA: 42s - loss: 0.2047 - acc: 0.9343 - ETA: 33s - loss: 0.0416 - acc: 0.9872Epoch 2/2\n",
            "60000/60000 [==============================] - 80s 1ms/sample - loss: 0.0412 - acc: 0.9872\n",
            "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.1353 - acc: 0.9567\n",
            " 4352/10000 [============>.................] - ETA: 6s - loss: 0.0638 - acc: 0.9789Epoch 2/2\n",
            "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.1124 - acc: 0.9646\n",
            "31616/60000 [==============>...............]\n",
            "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0398 - acc: 0.9861\n",
            " 5760/60000 [=>............................] - ETA: 1:06 - loss: 0.0546 - acc: 0.9828Model saved to disk\n",
            "10272/60000 [====>.........................] - ETA: 57s - loss: 0.0485 - acc: 0.9849Epoch 1/2\n",
            "60000/60000 [==============================] - 81s 1ms/sample - loss: 0.0393 - acc: 0.9875\n",
            "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0373 - acc: 0.9891\n",
            "Model saved to disk37952/60000 [=================>............]\n",
            "34912/60000 [================>.............] - ETA: 32s - loss: 0.1598 - acc: 0.9506Epoch 1/2\n",
            "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.0448 - acc: 0.9862\n",
            "60000/60000 [==============================] - 80s 1ms/sample - loss: 0.0415 - acc: 0.9869\n",
            "60000/60000 [==============================] - 75s 1ms/sample - loss: 0.1205 - acc: 0.9623\n",
            "24320/60000 [===========>..................] - ETA: 44s - loss: 0.1738 - acc: 0.9461Epoch 2/2\n",
            "10000/10000 [==============================] - 9s 869us/sample - loss: 0.0270 - acc: 0.9911\n",
            "26368/60000 [============>.................] - ETA: 41s - loss: 0.1664 - acc: 0.9483Model saved to disk\n",
            "10000/10000 [==============================] - 9s 881us/sample - loss: 0.0364 - acc: 0.9884\n",
            "30080/60000 [==============>...............] 5984/60000 [=>............................] - ETA: 35s - loss: 0.1552 - acc: 0.9515 - ETA: 45s - loss: 0.0450 - acc: 0.9881\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bModel saved to disk\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "10688/60000 [====>.........................] - ETA: 39s - loss: 0.0437 - acc: 0.9864Epoch 1/2\n",
            "37920/60000 [=================>............] - ETA: 24s - loss: 0.1374 - acc: 0.9573Epoch 1/2\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 0.1104 - acc: 0.9661\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 73s 1ms/sample - loss: 0.0420 - acc: 0.9869\n",
            "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0402 - acc: 0.9868\n",
            "55584/60000 [==========================>...]33376/60000 [===============>..............] - ETA: 6s - loss: 0.1401 - acc: 0.9562 - ETA: 35s - loss: 0.0365 - acc: 0.9887Model saved to disk\n",
            "57024/60000 [===========================>..] - ETA: 4s - loss: 0.1119 - acc: 0.9652Epoch 1/2\n",
            "60000/60000 [==============================] - 83s 1ms/sample - loss: 0.1340 - acc: 0.9581\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 84s 1ms/sample - loss: 0.1090 - acc: 0.9659\n",
            "Epoch 2/2\n",
            " 5856/60000 [=>............................] - ETA: 1:17 - loss: 0.0470 - acc: 0.9841\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bBuffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Melt9--4vdJK",
        "colab_type": "code",
        "outputId": "b777fe68-c310-4ad7-9181-6f9636dbe637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# q2Final.py\n",
        "import tensorflow as tf\n",
        "import os\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "# reshape to be [samples][width][height][pixels]\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
        " activation='relu',\n",
        "input_shape=(28, 28, 1)))\n",
        "model.add(tf.keras.layers.Conv2D(512, (3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Conv2D(512, (3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Conv2D(512, (3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        " loss='sparse_categorical_crossentropy',\n",
        " metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=10)\n",
        "model.evaluate(x_test, y_test)\n",
        "model_json = model.to_json()\n",
        "json_file = open(\"model_CNN.json\", \"w\")\n",
        "json_file.write(model_json)\n",
        "json_file.close()\n",
        "model.save_weights(\"model_CNN.h5\")\n",
        "print(\"Model saved to disk\")\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.2189 - acc: 0.9315\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0785 - acc: 0.9787\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0625 - acc: 0.9830\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0532 - acc: 0.9855\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0457 - acc: 0.9871\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0438 - acc: 0.9879\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0376 - acc: 0.9899\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0353 - acc: 0.9904\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0340 - acc: 0.9907\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0335 - acc: 0.9909\n",
            "10000/10000 [==============================] - 4s 428us/sample - loss: 0.0298 - acc: 0.9935\n",
            "Model saved to disk\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}