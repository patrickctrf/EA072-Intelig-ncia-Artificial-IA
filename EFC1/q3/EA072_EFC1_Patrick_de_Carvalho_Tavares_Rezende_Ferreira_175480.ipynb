{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kelx6PRGvdIt"
   },
   "source": [
    "# Patrick de Carvalho Tavares Rezende Ferreira - RA: 175480 - EFC1\n",
    "\n",
    "Repositório: https://github.com/patrickctrf/EA072-Inteligencia-Artificial-IA/tree/master/EFC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CxZXlOar-SvE",
    "outputId": "e67ebc89-4682-4ccc-f62f-c2cfd4242cb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/PODE APAGAR/EA072-EF1\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My\\ Drive/PODE\\ APAGAR/EA072-EF1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "odrxsaWtvdIx"
   },
   "source": [
    "## Questão 3\n",
    "\n",
    "Inicialmente, executou-se 5 vezes o código sugerido inicial a fim de verificar o desempenho da proposta. Seu desempenho foi de:\n",
    "\n",
    "* Loss: 0.0733; Acurácia: 0.9775.\n",
    "\n",
    "Utilizando-se o método de tentativa e erro, foi criado um script que verificava o desempenho da rede para diferentes parâmetros alterados como dropout (0.1 a 0.6), número de camadas (1 a 4 intermediárias), épocas de treinamento (4 a 8) e número de neurônios por camada (256 a 512). O script executava esta mudança de parâmetros dentro de loops \"for\" para executar todas as combinações possíveis e tirava também a média das múltiplas execuções com mesmos parâmetros, a fim de se obter uma média de desempenho mais confiável. Os resultados desta varredura eram salvos ao final das execuções em um arquivo \"listas.txt\", permitindo ao usuários verificar qual a configuração obteve melhor desempenho.\n",
    "Foram utilizadas 4 threads - para varredura de redes de 1 a 4 camadas - durante o treinamento, a fim de promover paralelismo e diminuir o tempo requerido, que chegava a dezenas de horas.\n",
    "\n",
    "Analisando as configurações que obtiveram o melhor desempenho, pode-se notar que as características que o maximizavam eram: 2 camadas, 512 neurônios, taxa de dropout próxima de 0.4 e 8 épocas de treinamento.\n",
    "\n",
    "Portanto, para a proposta final deste modelo, foi executada mais um treinamento com uso dos atributos analisados acima e os parâmetros que resultaram no melhor desempenho durante a varredura foram:\n",
    "\n",
    "* Camadas: 2; Neurônios por camada: 512; Dropout: 0.4; Épocas: 8.\n",
    "\n",
    "O desempenho médio obtido foi de:\n",
    "\n",
    "* Loss: 0.0663; Acurácia: 0.9823.\n",
    "\n",
    "Ambas as soluções consumiram um tempo de execução da ordem de poucos minutos e a diferença de desempenho foi cerca de 0,5% em ganho.\n",
    "\n",
    "Os arquivos utilizados foram (no diretório q3):\n",
    "\n",
    "Proposta Inicial: q3Inicial.py\n",
    "\n",
    "Script de Varredura de parâmetros: q3.py\n",
    "\n",
    "Proposta Final: q3Final.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "dV-GGfHzvdIz",
    "outputId": "c7d3dbc6-a843-4853-b4f5-97309c729510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.2698 - acc: 0.9197\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.1385 - acc: 0.9579\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.1080 - acc: 0.9668\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.0928 - acc: 0.9710\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.0821 - acc: 0.9736\n",
      "10000/10000 [==============================] - 1s 68us/sample - loss: 0.0640 - acc: 0.9797\n",
      "Model saved to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/content/drive/My Drive/PODE APAGAR/EA072-EF1'"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q3Inicial.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "model = tf.keras.models.Sequential([\n",
    " tf.keras.layers.Flatten(),\n",
    " tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    " tf.keras.layers.Dropout(0.5),\n",
    " tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    " loss='sparse_categorical_crossentropy',\n",
    " metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)\n",
    "model_json = model.to_json()\n",
    "json_file = open(\"model_MLP.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "model.save_weights(\"model_MLP.h5\")\n",
    "print(\"Model saved to disk\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "0SLFPiOhvdI4",
    "outputId": "b6c97708-08d7-41e4-8361-2e1901266ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epocas: 8\n",
      "CAMADAS1: 256\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epocas: 8\n",
      "CAMADAS2: 256\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epocas: 8\n",
      "CAMADAS3: 256\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epocas: 8\n",
      "CAMADAS4: 256\n",
      "\n",
      "\n",
      "Epoch 1/8\n",
      " 6112/60000 [==>...........................] - ETA: 12s - loss: 0.6049 - acc: 0.8253Epoch 1/8\n",
      "Epoch 1/8 1760/60000 [..............................] - ETA: 32s - loss: 1.0388 - acc: 0.7000\n",
      "12608/60000 [=====>........................] - ETA: 11s - loss: 0.4459 - acc: 0.8698\n",
      "60000/60000 [==============================] - 23s 379us/sample - loss: 0.2369 - acc: 0.9305\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 26s 434us/sample - loss: 0.2229 - acc: 0.9330\n",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 2/8\n",
      " 7616/60000 [==>...........................]60000/60000 [==============================] - ETA: 23s - loss: 0.1076 - acc: 0.9645 - 29s 476us/sample - loss: 0.2287 - acc: 0.9308\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] 3328/60000 [>.............................] - 30s 492us/sample - loss: 0.2444 - acc: 0.9270\n",
      " - ETA: 26s - loss: 0.1149 - acc: 0.9645Epoch 2/8\n",
      "60000/60000 [==============================] - 25s 419us/sample - loss: 0.1021 - acc: 0.9693\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 26s 435us/sample - loss: 0.1005 - acc: 0.9692\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 28s 463us/sample - loss: 0.1098 - acc: 0.9671\n",
      "Epoch 3/854080/60000 [==========================>...]\n",
      "60000/60000 [==============================] - 29s 485us/sample - loss: 0.1237 - acc: 0.9637\n",
      "18080/60000 [========>.....................] - ETA: 18s - loss: 0.0700 - acc: 0.9779Epoch 3/8\n",
      "60000/60000 [==============================] - 25s 418us/sample - loss: 0.0715 - acc: 0.9772\n",
      "30496/60000 [==============>...............] - ETA: 13s - loss: 0.0825 - acc: 0.9740Epoch 4/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "60000/60000 [==============================] - 26s 439us/sample - loss: 0.0723 - acc: 0.9771\n",
      "Epoch 4/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "60000/60000 [==============================] - 28s 465us/sample - loss: 0.0835 - acc: 0.9741\n",
      "33120/60000 [===============>..............] - ETA: 11s - loss: 0.0527 - acc: 0.9832Epoch 4/8\n",
      "60000/60000 [==============================] - 29s 481us/sample - loss: 0.0930 - acc: 0.9727\n",
      "23776/60000 [==========>...................] - ETA: 16s - loss: 0.0547 - acc: 0.9823Epoch 4/8\n",
      "60000/60000 [==============================] - 25s 415us/sample - loss: 0.0541 - acc: 0.9831\n",
      "24288/60000 [===========>..................] - ETA: 16s - loss: 0.0631 - acc: 0.9804Epoch 5/8\n",
      "60000/60000 [==============================] - 26s 435us/sample - loss: 0.0598 - acc: 0.9807\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 28s 459us/sample - loss: 0.0678 - acc: 0.9792\n",
      "18240/60000 [========>.....................] - ETA: 18s - loss: 0.0460 - acc: 0.9859Epoch 5/8\n",
      "11264/60000 [====>.........................]60000/60000 [==============================] - ETA: 22s - loss: 0.0532 - acc: 0.9837 - 29s 480us/sample - loss: 0.0786 - acc: 0.9771\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 25s 419us/sample - loss: 0.0434 - acc: 0.9857\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 27s 446us/sample - loss: 0.0512 - acc: 0.9842\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 28s 460us/sample - loss: 0.0594 - acc: 0.9818\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 29s 484us/sample - loss: 0.0688 - acc: 0.9795\n",
      "14272/60000 [======>.......................] - ETA: 21s - loss: 0.0422 - acc: 0.9870\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 6/8\n",
      "60000/60000 [==============================] - 26s 428us/sample - loss: 0.0354 - acc: 0.9887\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 27s 444us/sample - loss: 0.0441 - acc: 0.9862\n",
      "23136/60000 [==========>...................] - ETA: 17s - loss: 0.0584 - acc: 0.9835Epoch 7/8\n",
      "60000/60000 [==============================] - 27s 458us/sample - loss: 0.0501 - acc: 0.9847\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 25s 418us/sample - loss: 0.0291 - acc: 0.9901\n",
      " 9792/60000 [===>..........................] - ETA: 23s - loss: 0.0360 - acc: 0.9868Epoch 8/8\n",
      "60000/60000 [==============================] - 29s 480us/sample - loss: 0.0607 - acc: 0.9822\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 26s 436us/sample - loss: 0.0372 - acc: 0.9876\n",
      "17888/60000 [=======>......................] - ETA: 20s - loss: 0.0491 - acc: 0.9861Epoch 8/8\n",
      "40992/60000 [===================>..........]60000/60000 [==============================] - ETA: 9s - loss: 0.0525 - acc: 0.9843 - 28s 458us/sample - loss: 0.0460 - acc: 0.9851\n",
      "55168/60000 [==========================>...] - ETA: 2s - loss: 0.0252 - acc: 0.9915Epoch 8/8\n",
      "60000/60000 [==============================] - 25s 416us/sample - loss: 0.0255 - acc: 0.9914\n",
      "10000/10000 [==============================] - 4s 357us/sample - loss: 0.0689 - acc: 0.9810\n",
      "53280/60000 [=========================>....] - ETA: 3s - loss: 0.0540 - acc: 0.9842Model saved to disk\n",
      "18464/60000 [========>.....................] - ETA: 19s - loss: 0.0352 - acc: 0.9888Epoch 1/8\n",
      "60000/60000 [==============================] - 29s 475us/sample - loss: 0.0531 - acc: 0.9845\n",
      "Epoch 8/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "60000/60000 [==============================] - 26s 428us/sample - loss: 0.0316 - acc: 0.9900\n",
      "10000/10000 [==============================] - 4s 378us/sample - loss: 0.0821 - acc: 0.9771\n",
      "41056/60000 [===================>..........] - ETA: 8s - loss: 0.0379 - acc: 0.9877Model saved to disk\n",
      "32800/60000 [===============>..............] - ETA: 11s - loss: 0.3014 - acc: 0.9135Epoch 1/8\n",
      "60000/60000 [==============================] - 27s 450us/sample - loss: 0.0393 - acc: 0.9873\n",
      "10000/10000 [==============================]20096/60000 [=========>....................] - 4s 354us/sample - loss: 0.1359 - acc: 0.9697\n",
      "47840/60000 [======================>.......] - ETA: 5s - loss: 0.0466 - acc: 0.9859Model saved to disk\n",
      "60000/60000 [==============================] - 24s 403us/sample - loss: 0.2339 - acc: 0.9325\n",
      "Epoch 2/8\n",
      "30048/60000 [==============>...............] - ETA: 12s - loss: 0.2928 - acc: 0.9117Epoch 1/8\n",
      "60000/60000 [==============================] - 27s 455us/sample - loss: 0.0479 - acc: 0.9853\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0765 - acc: 0.9795\n",
      "43808/60000 [====================>.........] - ETA: 6s - loss: 0.2515 - acc: 0.9236Model saved to disk\n",
      "22080/60000 [==========>...................] - ETA: 17s - loss: 0.3441 - acc: 0.8946Epoch 1/8\n",
      "60000/60000 [==============================] - 25s 416us/sample - loss: 0.2205 - acc: 0.9327\n",
      "37664/60000 [=================>............] - ETA: 8s - loss: 0.1071 - acc: 0.9688Epoch 2/8\n",
      "60000/60000 [==============================] - 24s 392us/sample - loss: 0.1023 - acc: 0.9699\n",
      "22912/60000 [==========>...................] - ETA: 18s - loss: 0.3744 - acc: 0.8853Epoch 3/8\n",
      "60000/60000 [==============================] - 27s 453us/sample - loss: 0.2291 - acc: 0.9299\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 29s 482us/sample - loss: 0.2523 - acc: 0.9250\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 27s 448us/sample - loss: 0.0977 - acc: 0.9696\n",
      "25760/60000 [===========>..................] - ETA: 15s - loss: 0.1156 - acc: 0.9645Epoch 3/8\n",
      "60000/60000 [==============================] - 25s 414us/sample - loss: 0.0708 - acc: 0.9781\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 28s 463us/sample - loss: 0.1139 - acc: 0.9657\n",
      "33440/60000 [===============>..............] - ETA: 12s - loss: 0.1237 - acc: 0.9644Epoch 3/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "60000/60000 [==============================] - 26s 437us/sample - loss: 0.0699 - acc: 0.9777\n",
      "23008/60000 [==========>...................] - ETA: 16s - loss: 0.0831 - acc: 0.9743Epoch 4/8\n",
      "60000/60000 [==============================] - 29s 479us/sample - loss: 0.1194 - acc: 0.9653\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 25s 418us/sample - loss: 0.0543 - acc: 0.9830\n",
      " 8032/60000 [===>..........................] - ETA: 24s - loss: 0.0825 - acc: 0.9747Epoch 5/8\n",
      "60000/60000 [==============================] - 28s 459us/sample - loss: 0.0853 - acc: 0.9740\n",
      "25888/60000 [===========>..................] - ETA: 14s - loss: 0.0406 - acc: 0.9876Epoch 4/8\n",
      "60000/60000 [==============================] - 27s 443us/sample - loss: 0.0588 - acc: 0.9809\n",
      "48448/60000 [=======================>......] - ETA: 4s - loss: 0.0409 - acc: 0.9872Epoch 5/8\n",
      "60000/60000 [==============================] - 29s 483us/sample - loss: 0.0938 - acc: 0.9725\n",
      "59872/60000 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9867Epoch 4/8\n",
      "60000/60000 [==============================] - 25s 421us/sample - loss: 0.0427 - acc: 0.9868\n",
      "   32/60000 [..............................] - ETA: 44s - loss: 0.0718 - acc: 0.9375Epoch 6/8\n",
      "60000/60000 [==============================] - 28s 464us/sample - loss: 0.0693 - acc: 0.9793\n",
      "29280/60000 [=============>................] - ETA: 14s - loss: 0.0787 - acc: 0.9760Epoch 5/8\n",
      "60000/60000 [==============================] - 27s 446us/sample - loss: 0.0467 - acc: 0.9851\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 25s 412us/sample - loss: 0.0354 - acc: 0.9883\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 28s 474us/sample - loss: 0.0791 - acc: 0.9762\n",
      "14560/60000 [======>.......................] - ETA: 19s - loss: 0.0361 - acc: 0.9876Epoch 5/8\n",
      "60000/60000 [==============================] - 28s 465us/sample - loss: 0.0605 - acc: 0.9809\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 26s 433us/sample - loss: 0.0426 - acc: 0.9862\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 24s 407us/sample - loss: 0.0294 - acc: 0.9901\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 29s 483us/sample - loss: 0.0676 - acc: 0.9805\n",
      "20800/60000 [=========>....................] - ETA: 17s - loss: 0.0353 - acc: 0.9887Epoch 6/8\n",
      "60000/60000 [==============================] - 28s 466us/sample - loss: 0.0511 - acc: 0.9841\n",
      "48320/60000 [=======================>......] - ETA: 4s - loss: 0.0232 - acc: 0.9922Epoch 7/8\n",
      "60000/60000 [==============================] - 25s 420us/sample - loss: 0.0236 - acc: 0.9921\n",
      "60000/60000 [==============================] - ETA: 11s - loss: 0.0595 - acc: 0.9814 - 27s 447us/sample - loss: 0.0373 - acc: 0.9877\n",
      "Epoch 8/8\n",
      "10000/10000 [==============================] - 4s 375us/sample - loss: 0.0729 - acc: 0.9801\n",
      "20000/60000 [=========>....................] - ETA: 18s - loss: 0.0454 - acc: 0.9864Model saved to disk\n",
      "53888/60000 [=========================>....] - ETA: 3s - loss: 0.0614 - acc: 0.9814Epoch 1/8\n",
      "60000/60000 [==============================] - 30s 493us/sample - loss: 0.0606 - acc: 0.9817\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 28s 464us/sample - loss: 0.0472 - acc: 0.9858\n",
      "31488/60000 [==============>...............] - ETA: 12s - loss: 0.3076 - acc: 0.9106Epoch 8/8\n",
      "60000/60000 [==============================] - 27s 458us/sample - loss: 0.0322 - acc: 0.9895\n",
      "10000/10000 [==============================] - 4s 384us/sample - loss: 0.0723 - acc: 0.9808\n",
      "19488/60000 [========>.....................] - ETA: 19s - loss: 0.0432 - acc: 0.9868Model saved to disk\n",
      "60000/60000 [==============================]24224/60000 [===========>..................] - 25s 413us/sample - loss: 0.2396 - acc: 0.9302\n",
      " - ETA: 16s - loss: 0.0442 - acc: 0.9870Epoch 2/8\n",
      "48096/60000 [=======================>......] - ETA: 5s - loss: 0.0543 - acc: 0.9839Epoch 1/8\n",
      "60000/60000 [==============================] - 29s 484us/sample - loss: 0.0564 - acc: 0.9833\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 28s 472us/sample - loss: 0.0457 - acc: 0.9860\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 0.0767 - acc: 0.9823\n",
      "29312/60000 [=============>................] - ETA: 14s - loss: 0.0480 - acc: 0.9863Model saved to disk\n",
      "37056/60000 [=================>............] - ETA: 10s - loss: 0.0472 - acc: 0.9868Epoch 1/8\n",
      "60000/60000 [==============================] - 25s 417us/sample - loss: 0.1027 - acc: 0.9683\n",
      "52416/60000 [=========================>....]37376/60000 [=================>............] - ETA: 3s - loss: 0.2329 - acc: 0.9288 - ETA: 10s - loss: 0.0471 - acc: 0.9868Epoch 3/8\n",
      "60000/60000 [==============================] - 27s 447us/sample - loss: 0.2203 - acc: 0.9327\n",
      "44352/60000 [=====================>........] 7808/60000 [==>...........................]\n",
      "60000/60000 [==============================] - 29s 478us/sample - loss: 0.0504 - acc: 0.9856\n",
      "10000/10000 [==============================] - 4s 387us/sample - loss: 0.0945 - acc: 0.9764\n",
      "33888/60000 [===============>..............]37728/60000 [=================>............] - ETA: 12s - loss: 0.2900 - acc: 0.9103 - ETA: 9s - loss: 0.0751 - acc: 0.9768Model saved to disk\n",
      "51136/60000 [========================>.....] - ETA: 3s - loss: 0.0729 - acc: 0.9776Epoch 1/8\n",
      "60000/60000 [==============================] - 24s 394us/sample - loss: 0.0720 - acc: 0.9779\n",
      " 5888/60000 [=>............................] - ETA: 31s - loss: 0.6878 - acc: 0.7824\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 4/8\n",
      "60000/60000 [==============================] - 27s 448us/sample - loss: 0.2302 - acc: 0.9297\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 25s 418us/sample - loss: 0.0990 - acc: 0.9693\n",
      " 3328/60000 [>.............................] - ETA: 27s - loss: 0.0946 - acc: 0.9681Epoch 3/8\n",
      "60000/60000 [==============================] - 25s 418us/sample - loss: 0.0553 - acc: 0.9824\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 30s 495us/sample - loss: 0.2513 - acc: 0.9234\n",
      " 2752/60000 [>.............................] - ETA: 24s - loss: 0.0492 - acc: 0.9847Epoch 2/8\n",
      "60000/60000 [==============================] - 28s 467us/sample - loss: 0.1078 - acc: 0.9675\n",
      "58048/60000 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9771Epoch 3/8\n",
      "60000/60000 [==============================] - 27s 456us/sample - loss: 0.0727 - acc: 0.9768\n",
      " 1952/60000 [..............................] - ETA: 27s - loss: 0.0908 - acc: 0.9672Epoch 4/8\n",
      "60000/60000 [==============================] - 25s 425us/sample - loss: 0.0443 - acc: 0.9858\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 30s 500us/sample - loss: 0.1244 - acc: 0.9640\n",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 3/852608/60000 [=========================>....]\n",
      "60000/60000 [==============================] - 27s 457us/sample - loss: 0.0570 - acc: 0.9818\n",
      "Epoch 5/8\n",
      " 6848/60000 [==>...........................]60000/60000 [==============================] - ETA: 26s - loss: 0.0832 - acc: 0.9747 - 29s 477us/sample - loss: 0.0835 - acc: 0.9745\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 26s 431us/sample - loss: 0.0349 - acc: 0.9884\n",
      "34400/60000 [================>.............] - ETA: 12s - loss: 0.0653 - acc: 0.9801Epoch 7/8\n",
      "60000/60000 [==============================] - 31s 509us/sample - loss: 0.0957 - acc: 0.9722\n",
      "59456/60000 [============================>.]Epoch 4/8\n",
      "60000/60000 [==============================] - 28s 461us/sample - loss: 0.0496 - acc: 0.9834\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 29s 484us/sample - loss: 0.0686 - acc: 0.9792\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 26s 429us/sample - loss: 0.0294 - acc: 0.9903\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 27s 457us/sample - loss: 0.0410 - acc: 0.9865\n",
      "54688/60000 [==========================>...]Epoch 7/8\n",
      "60000/60000 [==============================] - 29s 487us/sample - loss: 0.0787 - acc: 0.9767\n",
      " 3616/60000 [>.............................] - ETA: 24s - loss: 0.0283 - acc: 0.9898Epoch 5/8\n",
      "60000/60000 [==============================] 5568/60000 [=>............................] - 28s 470us/sample - loss: 0.0604 - acc: 0.9813\n",
      " - ETA: 24s - loss: 0.0268 - acc: 0.9910Epoch 6/8\n",
      "60000/60000 [==============================] - 26s 431us/sample - loss: 0.0253 - acc: 0.9917\n",
      "10000/10000 [==============================] - 4s 375us/sample - loss: 0.0711 - acc: 0.9795\n",
      "33440/60000 [===============>..............] - ETA: 12s - loss: 0.0685 - acc: 0.9797Model saved to disk\n",
      "47136/60000 [======================>.......] - ETA: 5s - loss: 0.0341 - acc: 0.9886Epoch 1/8\n",
      "60000/60000 [==============================] - 26s 434us/sample - loss: 0.0346 - acc: 0.9884\n",
      "50752/60000 [========================>.....] - ETA: 4s - loss: 0.0686 - acc: 0.9797Epoch 8/8\n",
      "60000/60000 [==============================] - 28s 459us/sample - loss: 0.0509 - acc: 0.9839\n",
      "58688/60000 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9797Epoch 7/8\n",
      "60000/60000 [==============================] - 29s 486us/sample - loss: 0.0694 - acc: 0.9797\n",
      " 9984/60000 [===>..........................] - ETA: 23s - loss: 0.0244 - acc: 0.9918Epoch 6/8\n",
      "60000/60000 [==============================] - 26s 426us/sample - loss: 0.2490 - acc: 0.9269\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 28s 460us/sample - loss: 0.0333 - acc: 0.9890\n",
      "60000/60000 [==============================] - 28s 459us/sample - loss: 0.0455 - acc: 0.9859\n",
      "Epoch 8/8\n",
      "10000/10000 [==============================] - 4s 391us/sample - loss: 0.0731 - acc: 0.9803\n",
      "56704/60000 [===========================>..]Model saved to disk - ETA: 1s - loss: 0.0589 - acc: 0.9826\n",
      "60000/60000 [==============================] - 29s 481us/sample - loss: 0.0593 - acc: 0.9825\n",
      "Epoch 7/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      " 4864/60000 [=>............................] - ETA: 24s - loss: 0.0421 - acc: 0.9877Epoch 1/8\n",
      "60000/60000 [==============================] - 24s 404us/sample - loss: 0.1132 - acc: 0.9664\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 27s 458us/sample - loss: 0.0408 - acc: 0.9875\n",
      "60000/60000 [==============================] - 29s 476us/sample - loss: 0.0537 - acc: 0.9843\n",
      "43232/60000 [====================>.........] - ETA: 6s - loss: 0.0836 - acc: 0.9732Epoch 8/8\n",
      "60000/60000 [==============================] - 27s 445us/sample - loss: 0.2484 - acc: 0.9248\n",
      "Epoch 2/8\n",
      "10000/10000 [==============================] - 4s 371us/sample - loss: 0.0868 - acc: 0.9798\n",
      " 2656/60000 [>.............................] - ETA: 30s - loss: 0.0440 - acc: 0.9880Model saved to disk\n",
      "11552/60000 [====>.........................] - ETA: 20s - loss: 0.1213 - acc: 0.9619Epoch 1/8\n",
      "60000/60000 [==============================] - 24s 405us/sample - loss: 0.0831 - acc: 0.9736\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 27s 447us/sample - loss: 0.1159 - acc: 0.9639\n",
      "Epoch 3/846304/60000 [======================>.......] - ETA: 6s - loss: 0.2920 - acc: 0.9104\n",
      "60000/60000 [==============================] - 29s 484us/sample - loss: 0.0479 - acc: 0.9863\n",
      "60000/60000 [==============================] - 25s 418us/sample - loss: 0.0653 - acc: 0.9794\n",
      "Epoch 5/8\n",
      "10000/10000 [==============================] - 4s 363us/sample - loss: 0.0785 - acc: 0.9799\n",
      "60000/60000 [==============================] - 28s 468us/sample - loss: 0.2632 - acc: 0.9195\n",
      "14688/60000 [======>.......................] - ETA: 19s - loss: 0.0876 - acc: 0.9732Epoch 2/8\n",
      "15776/60000 [======>.......................] - ETA: 18s - loss: 0.0869 - acc: 0.9730Model saved to disk\n",
      "19200/60000 [========>.....................] - ETA: 14s - loss: 0.0521 - acc: 0.9834Epoch 1/8\n",
      "60000/60000 [==============================] - 25s 424us/sample - loss: 0.0923 - acc: 0.9713\n",
      "44096/60000 [=====================>........] - ETA: 6s - loss: 0.1359 - acc: 0.9593Epoch 4/8\n",
      "60000/60000 [==============================] 6624/60000 [==>...........................] - 24s 403us/sample - loss: 0.0542 - acc: 0.9826\n",
      " - ETA: 25s - loss: 0.0616 - acc: 0.9802Epoch 6/8\n",
      "60000/60000 [==============================] - 27s 449us/sample - loss: 0.1332 - acc: 0.9600\n",
      "10432/60000 [====>.........................] - ETA: 21s - loss: 0.0445 - acc: 0.9868Epoch 3/8\n",
      "17920/60000 [=======>......................]60000/60000 [==============================] - ETA: 19s - loss: 0.1048 - acc: 0.9670 - 30s 503us/sample - loss: 0.2882 - acc: 0.9119\n",
      "30560/60000 [==============>...............] - ETA: 12s - loss: 0.0452 - acc: 0.9858Epoch 2/8\n",
      "60000/60000 [==============================] - 27s 445us/sample - loss: 0.0739 - acc: 0.9762\n",
      "41408/60000 [===================>..........] - ETA: 8s - loss: 0.1045 - acc: 0.9689Epoch 5/8\n",
      "60000/60000 [==============================] - 25s 418us/sample - loss: 0.0454 - acc: 0.9850\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 28s 459us/sample - loss: 0.1046 - acc: 0.9689\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 29s 477us/sample - loss: 0.1479 - acc: 0.9578\n",
      "Epoch 3/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "18176/60000 [========>.....................]60000/60000 [==============================] - ETA: 20s - loss: 0.1150 - acc: 0.9663 - 27s 447us/sample - loss: 0.0637 - acc: 0.9798\n",
      "Epoch 6/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "60000/60000 [==============================] - 25s 422us/sample - loss: 0.0388 - acc: 0.9875\n",
      "Epoch 8/8\n",
      "51328/60000 [========================>.....] - ETA: 3s - loss: 0.0882 - acc: 0.9736Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "#q3.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import threading\n",
    "\n",
    "myMutex = threading.Lock()\n",
    "value = \"teste\"\n",
    "\n",
    "numeroDeNeuronios = []\n",
    "numeroDeEpocas = []\n",
    "numeroDeCamadas = []\n",
    "numeroDeDropout = []\n",
    "taxaDeAcertos = []\n",
    "\n",
    "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
    "def thread1Camadas(camadas):\n",
    "\t\n",
    "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
    "\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\n",
    "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
    "\tvaloresDropout = range(10, 60, 10)# Variaremos de 10% em 10%.\n",
    "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
    "\t\n",
    "\t# Testando resultados com diferentes quantidades de epocas.\n",
    "\tfor epocas in [8, 4]:\n",
    "\t\n",
    "\t\t# Testando resultados com diferentes quantidades de neuronios.\n",
    "\t\tfor neuronios in [256, 512]:\n",
    "\t\t\n",
    "\t\t\t# So para indicar em que passo da execucao estamos.\n",
    "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(neuronios) + \"\\n\\n\")\n",
    "\t\n",
    "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
    "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
    "\t\t\tfor taxaDropout in valoresDropout:\n",
    "\t\t\t\n",
    "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
    "\t\t\t\tfor iteracaoMedia in range(1,4):\n",
    "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
    "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\t\t\t\t\tmodel = tf.keras.models.Sequential([\n",
    "\t\t\t\t\t tf.keras.layers.Flatten(),\n",
    "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
    "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
    "\t\t\t\t\t tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "\t\t\t\t\t])\n",
    "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
    "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
    "\t\t\t\t\t metrics=['accuracy'])\n",
    "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
    "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
    "\t\t\t\t\tmodel_json = model.to_json()\n",
    "\t\t\t\t\tjson_file = open(\"model_MLP1.json\", \"w\")\n",
    "\t\t\t\t\tjson_file.write(model_json)\n",
    "\t\t\t\t\tjson_file.close()\n",
    "\t\t\t\t\tmodel.save_weights(\"model_MLP1.h5\")\n",
    "\t\t\t\t\tprint(\"Model saved to disk\")\n",
    "\t\t\t\t\tos.getcwd()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\tmyMutex.acquire()\n",
    "\t\t\t\tnumeroDeNeuronios.append(neuronios)\n",
    "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
    "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
    "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
    "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
    "\t\t\t\tmyMutex.release()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Reiniciamos a soma.\n",
    "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\t\t\t\n",
    "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
    "def thread2Camadas(camadas):\n",
    "\t\n",
    "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
    "\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\n",
    "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
    "\tvaloresDropout = range(10, 60, 10)# Variaremos de 10% em 10%.\n",
    "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
    "\t\n",
    "\t# Testando resultados com diferentes quantidades de epocas.\n",
    "\tfor epocas in [8, 4]:\n",
    "\t\n",
    "\t\t# Testando resultados com diferentes quantidades de neuronios.\n",
    "\t\tfor neuronios in [256, 512]:\n",
    "\t\t\n",
    "\t\t\t# So para indicar em que passo da execucao estamos.\n",
    "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(neuronios) + \"\\n\\n\")\n",
    "\t\n",
    "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
    "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
    "\t\t\tfor taxaDropout in valoresDropout:\n",
    "\t\t\t\n",
    "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
    "\t\t\t\tfor iteracaoMedia in range(1,4):\n",
    "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
    "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\t\t\t\t\tmodel = tf.keras.models.Sequential([\n",
    "\t\t\t\t\t tf.keras.layers.Flatten(),\n",
    "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
    "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
    "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
    "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
    "\t\t\t\t\t tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "\t\t\t\t\t])\n",
    "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
    "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
    "\t\t\t\t\t metrics=['accuracy'])\n",
    "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
    "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
    "\t\t\t\t\tmodel_json = model.to_json()\n",
    "\t\t\t\t\tjson_file = open(\"model_MLP2.json\", \"w\")\n",
    "\t\t\t\t\tjson_file.write(model_json)\n",
    "\t\t\t\t\tjson_file.close()\n",
    "\t\t\t\t\tmodel.save_weights(\"model_MLP2.h5\")\n",
    "\t\t\t\t\tprint(\"Model saved to disk\")\n",
    "\t\t\t\t\tos.getcwd()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\tmyMutex.acquire()\n",
    "\t\t\t\tnumeroDeNeuronios.append(neuronios)\n",
    "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
    "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
    "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
    "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
    "\t\t\t\tmyMutex.release()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Reiniciamos a soma.\n",
    "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\t\t\t\n",
    "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
    "def thread3Camadas(camadas):\n",
    "\t\n",
    "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
    "\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\n",
    "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
    "\tvaloresDropout = range(10, 60, 10)# Variaremos de 10% em 10%.\n",
    "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
    "\t\n",
    "\t# Testando resultados com diferentes quantidades de epocas.\n",
    "\tfor epocas in [8, 4]:\n",
    "\t\n",
    "\t\t# Testando resultados com diferentes quantidades de neuronios.\n",
    "\t\tfor neuronios in [256, 512]:\n",
    "\t\t\n",
    "\t\t\t# So para indicar em que passo da execucao estamos.\n",
    "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(neuronios) + \"\\n\\n\")\n",
    "\t\n",
    "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
    "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
    "\t\t\tfor taxaDropout in valoresDropout:\n",
    "\t\t\t\n",
    "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
    "\t\t\t\tfor iteracaoMedia in range(1,4):\n",
    "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
    "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\t\t\t\t\tmodel = tf.keras.models.Sequential([\n",
    "\t\t\t\t\t tf.keras.layers.Flatten(),\n",
    "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
    "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
    "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
    "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
    "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
    "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
    "\t\t\t\t\t tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "\t\t\t\t\t])\n",
    "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
    "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
    "\t\t\t\t\t metrics=['accuracy'])\n",
    "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
    "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
    "\t\t\t\t\tmodel_json = model.to_json()\n",
    "\t\t\t\t\tjson_file = open(\"model_MLP3.json\", \"w\")\n",
    "\t\t\t\t\tjson_file.write(model_json)\n",
    "\t\t\t\t\tjson_file.close()\n",
    "\t\t\t\t\tmodel.save_weights(\"model_MLP3.h5\")\n",
    "\t\t\t\t\tprint(\"Model saved to disk\")\n",
    "\t\t\t\t\tos.getcwd()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\tmyMutex.acquire()\n",
    "\t\t\t\tnumeroDeNeuronios.append(neuronios)\n",
    "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
    "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
    "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
    "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
    "\t\t\t\tmyMutex.release()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Reiniciamos a soma.\n",
    "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\t\t\t\n",
    "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
    "def thread4Camadas(camadas):\n",
    "\t\n",
    "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
    "\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\n",
    "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
    "\tvaloresDropout = range(10, 60, 10)# Variaremos de 10% em 10%.\n",
    "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
    "\t\n",
    "\t# Testando resultados com diferentes quantidades de epocas.\n",
    "\tfor epocas in [8, 4]:\n",
    "\t\n",
    "\t\t# Testando resultados com diferentes quantidades de neuronios.\n",
    "\t\tfor neuronios in [256, 512]:\n",
    "\t\t\n",
    "\t\t\t# So para indicar em que passo da execucao estamos.\n",
    "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(neuronios) + \"\\n\\n\")\n",
    "\t\n",
    "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
    "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
    "\t\t\tfor taxaDropout in valoresDropout:\n",
    "\t\t\t\n",
    "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
    "\t\t\t\tfor iteracaoMedia in range(1,4):\n",
    "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
    "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\t\t\t\t\tmodel = tf.keras.models.Sequential([\n",
    "\t\t\t\t\t tf.keras.layers.Flatten(),\n",
    "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
    "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
    "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
    "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
    "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
    "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
    "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
    "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
    "\t\t\t\t\t tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "\t\t\t\t\t])\n",
    "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
    "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
    "\t\t\t\t\t metrics=['accuracy'])\n",
    "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
    "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
    "\t\t\t\t\tmodel_json = model.to_json()\n",
    "\t\t\t\t\tjson_file = open(\"model_MLP4.json\", \"w\")\n",
    "\t\t\t\t\tjson_file.write(model_json)\n",
    "\t\t\t\t\tjson_file.close()\n",
    "\t\t\t\t\tmodel.save_weights(\"model_MLP4.h5\")\n",
    "\t\t\t\t\tprint(\"Model saved to disk\")\n",
    "\t\t\t\t\tos.getcwd()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\tmyMutex.acquire()\n",
    "\t\t\t\tnumeroDeNeuronios.append(neuronios)\n",
    "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
    "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
    "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
    "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
    "\t\t\t\tmyMutex.release()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Reiniciamos a soma.\n",
    "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\t\n",
    "\tcamadas1 = threading.Thread(target=thread1Camadas,args=(1,))\n",
    "\tcamadas2 = threading.Thread(target=thread2Camadas,args=(2,))\n",
    "\tcamadas3 = threading.Thread(target=thread3Camadas,args=(3,))\n",
    "\tcamadas4 = threading.Thread(target=thread4Camadas,args=(4,))\n",
    "\t\n",
    "\tcamadas1.start()\n",
    "\tcamadas2.start()\n",
    "\tcamadas3.start()\n",
    "\tcamadas4.start()\n",
    "\t\n",
    "\ttry:\n",
    "\t\tcamadas4.join(); \n",
    "\texcept:\n",
    "\t\tpass;\n",
    "\t\t\n",
    "\ttry:\n",
    "\t\tcamadas3.join(); \n",
    "\texcept:\n",
    "\t\tpass;\n",
    "\t\t\n",
    "\ttry:\n",
    "\t\tcamadas2.join(); \n",
    "\texcept:\n",
    "\t\tpass;\n",
    "\t\t\n",
    "\ttry:\n",
    "\t\tcamadas1.join(); \n",
    "\texcept:\n",
    "\t\tpass;\n",
    "\t\t\n",
    "\tlistasFile = open(\"listasFFULLYCONNECTED.txt\", \"w\")\n",
    "\tlistasFile.write(str(numeroDeNeuronios) + \"\\n\")\n",
    "\tlistasFile.write(str(numeroDeEpocas) + \"\\n\")\n",
    "\tlistasFile.write(str(numeroDeCamadas) + \"\\n\")\n",
    "\tlistasFile.write(str(numeroDeDropout) + \"\\n\")\n",
    "\tlistasFile.write(str(taxaDeAcertos) + \"\\n\")\n",
    "\tlistasFile.close()\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "2adCE0ogvdI-",
    "outputId": "85a4b3f6-eb1f-4c98-e26e-f777f875c331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.2446 - acc: 0.9246\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.1283 - acc: 0.9617\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.1008 - acc: 0.9691\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0869 - acc: 0.9737\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.0779 - acc: 0.9765\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0712 - acc: 0.9787\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.0653 - acc: 0.9803\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.0618 - acc: 0.9819\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0669 - acc: 0.9823\n",
      "Model saved to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q3Final.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "model = tf.keras.models.Sequential([\n",
    " tf.keras.layers.Flatten(),\n",
    " tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    " tf.keras.layers.Dropout(0.3),\n",
    " tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    " tf.keras.layers.Dropout(0.4),\n",
    " tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    " loss='sparse_categorical_crossentropy',\n",
    " metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=8)\n",
    "model.evaluate(x_test, y_test)\n",
    "model_json = model.to_json()\n",
    "json_file = open(\"model_MLP.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "model.save_weights(\"model_MLP.h5\")\n",
    "print(\"Model saved to disk\")\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "4373987a9a754fee8a260ba619c4e8b3",
   "lastKernelId": "374183ba-9524-411d-89a2-c62ca5287fa9"
  },
  "accelerator": "GPU",
  "celltoolbar": "Attachments",
  "colab": {
   "collapsed_sections": [],
   "name": "EA072_EFC1_Patrick_de_Carvalho_Tavares_Rezende_Ferreira_175480.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
