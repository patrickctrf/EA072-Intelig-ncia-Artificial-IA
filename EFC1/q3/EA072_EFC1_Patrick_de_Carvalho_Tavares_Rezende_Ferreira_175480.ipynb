{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "@webio": {
      "lastCommId": "4373987a9a754fee8a260ba619c4e8b3",
      "lastKernelId": "374183ba-9524-411d-89a2-c62ca5287fa9"
    },
    "celltoolbar": "Attachments",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "EA072_EFC1_Patrick_de_Carvalho_Tavares_Rezende_Ferreira_175480.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kelx6PRGvdIt",
        "colab_type": "text"
      },
      "source": [
        "# Patrick de Carvalho Tavares Rezende Ferreira - RA: 175480 - EFC2\n",
        "\n",
        "Repositório: https://github.com/patrickctrf/EA072-Inteligencia-Artificial-IA/tree/master/EFC1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxZXlOar-SvE",
        "colab_type": "code",
        "outputId": "c9cebfdf-0d8a-4627-90a6-ec78f2f2ff49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/PODE\\ APAGAR/EA072-EF1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/PODE APAGAR/EA072-EF1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odrxsaWtvdIx",
        "colab_type": "text"
      },
      "source": [
        "## Questão 1\n",
        "\n",
        "Inicialmente, executou-se 5 vezes o código sugerido inicial a fim de verificar o desempenho da proposta. Seu desempenho foi de:\n",
        "\n",
        "* Loss: 0.0733; Acurácia: 0.9775.\n",
        "\n",
        "Utilizando-se o método de tentativa e erro, foi criado um script que verificava o desempenho da rede para diferentes parâmetros alterados como dropout (0.1 a 0.6), número de camadas (1 a 4 intermediárias), épocas de treinamento (4 a 8) e número de neurônios por camada (256 a 512). O script executava esta mudança de parâmetros dentro de loops \"for\" para executar todas as combinações possíveis e tirava também a média das múltiplas execuções com mesmos parâmetros, a fim de se obter uma média de desempenho mais confiável. Os resultados desta varredura eram salvos ao final das execuções em um arquivo \"listas.txt\", permitindo ao usuários verificar qual a configuração obteve melhor desempenho.\n",
        "Foram utilizadas 4 threads - para varredura de redes de 1 a 4 camadas - durante o treinamento, a fim de promover paralelismo e diminuir o tempo requerido, que chegava a dezenas de horas.\n",
        "\n",
        "Para a proposta final deste modelo, os parâmetros que resultaram no melhor desempenho durante a varredura foram:\n",
        "\n",
        "* Camadas: 2; Neurônios por camada: 512; Dropout: 0.2; Épocas: 8.\n",
        "\n",
        "O desempenho médio obtido foi de:\n",
        "\n",
        "* Loss: 0.0693; Acurácia: 0.9828.\n",
        "\n",
        "Ambas as soluções consumiram um tempo de execução da ordem de poucos minutos e a diferença de desempenho foi cerca de 0,5% em ganho.\n",
        "\n",
        "Os arquivos utilizados foram (no diretório q1):\n",
        "\n",
        "Proposta Inicial: q1Inicial.py\n",
        "Script de Varredura de parâmetros: q1.py\n",
        "Proposta Final: q1Final.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV-GGfHzvdIz",
        "colab_type": "code",
        "outputId": "2781030b-0371-4f87-d90a-f16711e8e8b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "# q1Inicial.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        " tf.keras.layers.Flatten(),\n",
        " tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        " tf.keras.layers.Dropout(0.5),\n",
        " tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        " loss='sparse_categorical_crossentropy',\n",
        " metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "model.evaluate(x_test, y_test)\n",
        "model_json = model.to_json()\n",
        "json_file = open(\"model_MLP.json\", \"w\")\n",
        "json_file.write(model_json)\n",
        "json_file.close()\n",
        "model.save_weights(\"model_MLP.h5\")\n",
        "print(\"Model saved to disk\")\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.2693 - acc: 0.9196\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.1367 - acc: 0.9583\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 6s 105us/sample - loss: 0.1079 - acc: 0.9664\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.0947 - acc: 0.9703\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.0808 - acc: 0.9737\n",
            "10000/10000 [==============================] - 1s 70us/sample - loss: 0.0684 - acc: 0.9791\n",
            "Model saved to disk\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/PODE APAGAR/EA072-EF1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SLFPiOhvdI4",
        "colab_type": "code",
        "outputId": "de134c71-c972-48ba-d980-ac5959b21028",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import threading\n",
        "\n",
        "myMutex = threading.Lock()\n",
        "value = \"teste\"\n",
        "\n",
        "numeroDeNeuronios = []\n",
        "numeroDeEpocas = []\n",
        "numeroDeCamadas = []\n",
        "numeroDeDropout = []\n",
        "taxaDeAcertos = []\n",
        "\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread1Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 60, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [8, 4]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de neuronios.\n",
        "\t\tfor neuronios in [256, 512]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(neuronios) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,4):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential([\n",
        "\t\t\t\t\t tf.keras.layers.Flatten(),\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\t\t\t\t\t])\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_MLP1.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_MLP1.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(neuronios)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread2Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 60, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [8, 4]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de neuronios.\n",
        "\t\tfor neuronios in [256, 512]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(neuronios) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,4):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential([\n",
        "\t\t\t\t\t tf.keras.layers.Flatten(),\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\t\t\t\t\t])\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_MLP2.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_MLP2.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(neuronios)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread3Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 60, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [8, 4]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de neuronios.\n",
        "\t\tfor neuronios in [256, 512]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(neuronios) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,4):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential([\n",
        "\t\t\t\t\t tf.keras.layers.Flatten(),\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\t\t\t\t\t])\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_MLP3.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_MLP3.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(neuronios)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread4Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 60, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [8, 4]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de neuronios.\n",
        "\t\tfor neuronios in [256, 512]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(neuronios) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,4):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential([\n",
        "\t\t\t\t\t tf.keras.layers.Flatten(),\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(neuronios, activation=tf.nn.relu),\n",
        "\t\t\t\t\t tf.keras.layers.Dropout(taxaDropout),# Diferentes valores de dropout.\n",
        "\t\t\t\t\t tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\t\t\t\t\t])\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_MLP4.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_MLP4.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(neuronios)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\t\n",
        "\tcamadas1 = threading.Thread(target=thread1Camadas,args=(1,))\n",
        "\tcamadas2 = threading.Thread(target=thread2Camadas,args=(2,))\n",
        "\tcamadas3 = threading.Thread(target=thread3Camadas,args=(3,))\n",
        "\tcamadas4 = threading.Thread(target=thread4Camadas,args=(4,))\n",
        "\t\n",
        "\tcamadas1.start()\n",
        "\tcamadas2.start()\n",
        "\tcamadas3.start()\n",
        "\tcamadas4.start()\n",
        "\t\n",
        "\ttry:\n",
        "\t\tcamadas4.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\ttry:\n",
        "\t\tcamadas3.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\ttry:\n",
        "\t\tcamadas2.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\ttry:\n",
        "\t\tcamadas1.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\tlistasFile = open(\"listasFFULLYCONNECTED.txt\", \"w\")\n",
        "\tlistasFile.write(str(numeroDeNeuronios) + \"\\n\")\n",
        "\tlistasFile.write(str(numeroDeEpocas) + \"\\n\")\n",
        "\tlistasFile.write(str(numeroDeCamadas) + \"\\n\")\n",
        "\tlistasFile.write(str(numeroDeDropout) + \"\\n\")\n",
        "\tlistasFile.write(str(taxaDeAcertos) + \"\\n\")\n",
        "\tlistasFile.close()\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "epocas: 8\n",
            "CAMADAS2: 256\n",
            "\n",
            "\n",
            "\n",
            "epocas: 8\n",
            "CAMADAS1: 256\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epocas: 8\n",
            "CAMADAS3: 256\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epocas: 8\n",
            "CAMADAS4: 256\n",
            "\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Epoch 1/8Epoch 1/8\n",
            "\n",
            "Epoch 1/8\n",
            "  384/60000 [..............................] - ETA: 1:51 - loss: 1.8786 - acc: 0.4531Epoch 1/8\n",
            "60000/60000 [==============================] - 25s 420us/sample - loss: 0.2316 - acc: 0.9337\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] - 27s 445us/sample - loss: 0.2174 - acc: 0.9337\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] - 28s 463us/sample - loss: 0.2310 - acc: 0.9294\n",
            " 6368/60000 [==>...........................] - ETA: 22s - loss: 0.1057 - acc: 0.9678Epoch 2/8\n",
            "60000/60000 [==============================] - 29s 489us/sample - loss: 0.2515 - acc: 0.9246\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] - 25s 425us/sample - loss: 0.1020 - acc: 0.9696\n",
            "Epoch 3/8\n",
            " - ETA: 2s - loss: 0.1098 - acc: 0.966760000/60000 [==============================] - 27s 443us/sample - loss: 0.0996 - acc: 0.9697\n",
            "Epoch 3/8\n",
            "60000/60000 [==============================] - 27s 454us/sample - loss: 0.1095 - acc: 0.9671\n",
            "Epoch 3/8\n",
            "60000/60000 [==============================] - 29s 478us/sample - loss: 0.1243 - acc: 0.9645\n",
            "20096/60000 [=========>....................] - ETA: 15s - loss: 0.0740 - acc: 0.9762Epoch 3/8\n",
            "60000/60000 [==============================] - 24s 408us/sample - loss: 0.0727 - acc: 0.9769\n",
            "44512/60000 [=====================>........] - ETA: 6s - loss: 0.0857 - acc: 0.9740Epoch 4/8\n",
            "60000/60000 [==============================] - 26s 433us/sample - loss: 0.0747 - acc: 0.9759\n",
            " 9920/60000 [===>..........................] - ETA: 20s - loss: 0.0458 - acc: 0.9853Epoch 4/8\n",
            "60000/60000 [==============================] - 27s 453us/sample - loss: 0.0861 - acc: 0.9739\n",
            " 7072/60000 [==>...........................] - ETA: 22s - loss: 0.0557 - acc: 0.9832\n",
            "60000/60000 [==============================] - 28s 475us/sample - loss: 0.0923 - acc: 0.9734\n",
            "Epoch 4/8\n",
            "60000/60000 [==============================] - 25s 414us/sample - loss: 0.0540 - acc: 0.9834\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 26s 431us/sample - loss: 0.0572 - acc: 0.9819\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 27s 455us/sample - loss: 0.0684 - acc: 0.9789\n",
            "\n",
            "60000/60000 [==============================] - 28s 466us/sample - loss: 0.0799 - acc: 0.9771\n",
            "12800/60000 [=====>........................] - ETA: 20s - loss: 0.0579 - acc: 0.9808Epoch 5/8\n",
            "60000/60000 [==============================] - 24s 403us/sample - loss: 0.0419 - acc: 0.9866\n",
            "Epoch 6/8\n",
            " - ETA: 6s - loss: 0.0554 - acc: 0.982460000/60000 [==============================] - 26s 428us/sample - loss: 0.0494 - acc: 0.9843\n",
            "33760/60000 [===============>..............] - ETA: 12s - loss: 0.0634 - acc: 0.9814Epoch 6/8\n",
            "60000/60000 [==============================] - 27s 449us/sample - loss: 0.0586 - acc: 0.9813\n",
            "31008/60000 [==============>...............] - ETA: 11s - loss: 0.0339 - acc: 0.9897Epoch 6/8\n",
            "60000/60000 [==============================] - 28s 458us/sample - loss: 0.0667 - acc: 0.9809\n",
            "13728/60000 [=====>........................] - ETA: 20s - loss: 0.0526 - acc: 0.9838Epoch 6/8\n",
            " - ETA: 8s - loss: 0.0403 - acc: 0.987160000/60000 [==============================] - 24s 405us/sample - loss: 0.0343 - acc: 0.9890\n",
            "Epoch 7/8\n",
            "60000/60000 [==============================] - 26s 431us/sample - loss: 0.0434 - acc: 0.9858\n",
            "44352/60000 [=====================>........] - ETA: 7s - loss: 0.0517 - acc: 0.9839Epoch 7/8\n",
            "60000/60000 [==============================] - 27s 452us/sample - loss: 0.0531 - acc: 0.9838\n",
            "37568/60000 [=================>............] - ETA: 9s - loss: 0.0297 - acc: 0.9904Epoch 7/8\n",
            "60000/60000 [==============================] - 28s 473us/sample - loss: 0.0624 - acc: 0.9817\n",
            "16704/60000 [=======>......................] - ETA: 19s - loss: 0.0393 - acc: 0.9880Epoch 7/8\n",
            "60000/60000 [==============================] - 25s 413us/sample - loss: 0.0300 - acc: 0.9901\n",
            "Epoch 8/8\n",
            "60000/60000 [==============================] - 26s 432us/sample - loss: 0.0370 - acc: 0.9879\n",
            "Epoch 8/8\n",
            "60000/60000 [==============================] - 27s 453us/sample - loss: 0.0444 - acc: 0.9866\n",
            "41984/60000 [===================>..........] - ETA: 8s - loss: 0.0530 - acc: 0.9836Epoch 8/8\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.0244 - acc: 0.9920\n",
            "60000/60000 [==============================] - 28s 462us/sample - loss: 0.0525 - acc: 0.9838\n",
            "Epoch 8/8\n",
            "10000/10000 [==============================] - 4s 370us/sample - loss: 0.0703 - acc: 0.9819\n",
            "24608/60000 [===========>..................] - ETA: 15s - loss: 0.0371 - acc: 0.9883Model saved to disk\n",
            "30208/60000 [==============>...............] - ETA: 13s - loss: 0.0370 - acc: 0.9884Epoch 1/8\n",
            "60000/60000 [==============================] - 25s 422us/sample - loss: 0.0331 - acc: 0.9889\n",
            "10000/10000 [==============================] - 4s 368us/sample - loss: 0.0816 - acc: 0.9791\n",
            "19584/60000 [========>.....................] - ETA: 15s - loss: 0.3817 - acc: 0.8928Model saved to disk\n",
            "28000/60000 [=============>................] - ETA: 12s - loss: 0.3282 - acc: 0.9069\n",
            "60000/60000 [==============================] - 26s 429us/sample - loss: 0.0401 - acc: 0.9877\n",
            "10000/10000 [==============================] - 4s 357us/sample - loss: 0.0852 - acc: 0.9789\n",
            "48608/60000 [=======================>......] - ETA: 5s - loss: 0.0488 - acc: 0.9857Model saved to disk\n",
            "52224/60000 [=========================>....] - ETA: 2s - loss: 0.2532 - acc: 0.9281Epoch 1/8\n",
            "60000/60000 [==============================] - 26s 437us/sample - loss: 0.0499 - acc: 0.9855\n",
            "60000/60000 [==============================] - 22s 372us/sample - loss: 0.2387 - acc: 0.9318\n",
            "Epoch 2/8\n",
            "10000/10000 [==============================] - 4s 356us/sample - loss: 0.1086 - acc: 0.9731\n",
            "12576/60000 [=====>........................] - ETA: 20s - loss: 0.4534 - acc: 0.8581Model saved to disk\n",
            "20032/60000 [=========>....................] - ETA: 14s - loss: 0.1055 - acc: 0.9668\n",
            "60000/60000 [==============================] - 24s 400us/sample - loss: 0.2174 - acc: 0.9344\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 2/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "60000/60000 [==============================] - 26s 431us/sample - loss: 0.2326 - acc: 0.9275\n",
            "Epoch 2/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24064/60000 [===========>..................]\n",
            "60000/60000 [==============================] - 23s 390us/sample - loss: 0.1037 - acc: 0.9680\n",
            "Epoch 3/8\n",
            "60000/60000 [==============================] - 29s 482us/sample - loss: 0.2506 - acc: 0.9245\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 2/8\n",
            "60000/60000 [==============================] - 26s 437us/sample - loss: 0.1007 - acc: 0.9687\n",
            "Epoch 3/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "60000/60000 [==============================] - 25s 412us/sample - loss: 0.0721 - acc: 0.9783\n",
            "Epoch 4/8\n",
            "60000/60000 [==============================] - 27s 449us/sample - loss: 0.1110 - acc: 0.9671\n",
            " 4192/60000 [=>............................] - ETA: 24s - loss: 0.0548 - acc: 0.9828Epoch 3/8\n",
            "60000/60000 [==============================] - 26s 433us/sample - loss: 0.0713 - acc: 0.9773\n",
            "Epoch 4/839200/60000 [==================>...........]\n",
            "60000/60000 [==============================]39424/60000 [==================>...........] - 29s 479us/sample - loss: 0.1230 - acc: 0.9645\n",
            " - ETA: 8s - loss: 0.0523 - acc: 0.9836Epoch 3/8\n",
            "60000/60000 [==============================] - 25s 420us/sample - loss: 0.0549 - acc: 0.9830\n",
            "51584/60000 [========================>.....] - ETA: 3s - loss: 0.0825 - acc: 0.9746Epoch 5/8\n",
            "60000/60000 [==============================] - 27s 452us/sample - loss: 0.0835 - acc: 0.9744\n",
            "Epoch 4/8\n",
            "60000/60000 [==============================] - 26s 432us/sample - loss: 0.0581 - acc: 0.9812\n",
            "Epoch 5/830144/60000 [==============>...............]\n",
            "60000/60000 [==============================] - 28s 472us/sample - loss: 0.0942 - acc: 0.9731\n",
            "Epoch 4/8\n",
            "60000/60000 [==============================] - 25s 412us/sample - loss: 0.0417 - acc: 0.9865\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 27s 451us/sample - loss: 0.0670 - acc: 0.9785\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 26s 430us/sample - loss: 0.0457 - acc: 0.9851\n",
            "27584/60000 [============>.................] - ETA: 14s - loss: 0.0609 - acc: 0.9814Epoch 6/8\n",
            "60000/60000 [==============================] - 28s 469us/sample - loss: 0.0785 - acc: 0.9768\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 25s 412us/sample - loss: 0.0343 - acc: 0.9886\n",
            "Epoch 7/8\n",
            "60000/60000 [==============================] - 27s 448us/sample - loss: 0.0614 - acc: 0.9809\n",
            "19552/60000 [========>.....................]\n",
            "60000/60000 [==============================] - 26s 438us/sample - loss: 0.0395 - acc: 0.9869\n",
            "Epoch 7/8\n",
            "60000/60000 [==============================] - 25s 419us/sample - loss: 0.0301 - acc: 0.9901\n",
            "Epoch 8/8\n",
            "60000/60000 [==============================] - 29s 475us/sample - loss: 0.0687 - acc: 0.9806\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 27s 447us/sample - loss: 0.0528 - acc: 0.9839\n",
            "Epoch 7/8\n",
            "60000/60000 [==============================] - 26s 431us/sample - loss: 0.0381 - acc: 0.9880\n",
            "39840/60000 [==================>...........] - ETA: 9s - loss: 0.0610 - acc: 0.9825Epoch 8/8\n",
            "60000/60000 [==============================] - 25s 417us/sample - loss: 0.0244 - acc: 0.9919\n",
            "10000/10000 [==============================] - 4s 371us/sample - loss: 0.0703 - acc: 0.9815\n",
            "41728/60000 [===================>..........] - ETA: 8s - loss: 0.0463 - acc: 0.9858Model saved to disk\n",
            "60000/60000 [==============================] - 28s 463us/sample - loss: 0.0620 - acc: 0.9821\n",
            "22336/60000 [==========>...................]43616/60000 [====================>.........] - ETA: 15s - loss: 0.0285 - acc: 0.9906 - ETA: 7s - loss: 0.0463 - acc: 0.9858Epoch 7/8\n",
            "48064/60000 [=======================>......] - ETA: 5s - loss: 0.0461 - acc: 0.9858Epoch 1/8\n",
            "60000/60000 [==============================] - 27s 447us/sample - loss: 0.0474 - acc: 0.9857\n",
            "Epoch 8/8\n",
            "60000/60000 [==============================] - 25s 417us/sample - loss: 0.0319 - acc: 0.9897\n",
            "10000/10000 [==============================] - 4s 393us/sample - loss: 0.0675 - acc: 0.9804\n",
            "43968/60000 [====================>.........] - ETA: 7s - loss: 0.0561 - acc: 0.9829\n",
            "53376/60000 [=========================>....] - ETA: 2s - loss: 0.2528 - acc: 0.9269Epoch 1/8\n",
            "60000/60000 [==============================] - 24s 403us/sample - loss: 0.2400 - acc: 0.9305\n",
            "Epoch 2/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "60000/60000 [==============================] - 27s 456us/sample - loss: 0.0554 - acc: 0.9834\n",
            "Epoch 8/8\n",
            "60000/60000 [==============================] - 26s 436us/sample - loss: 0.0421 - acc: 0.9876\n",
            "10000/10000 [==============================] - 4s 359us/sample - loss: 0.0757 - acc: 0.9789\n",
            "22112/60000 [==========>...................] - ETA: 16s - loss: 0.0453 - acc: 0.9861Model saved to disk\n",
            "43904/60000 [====================>.........] - ETA: 6s - loss: 0.2516 - acc: 0.9231Epoch 1/8\n",
            "60000/60000 [==============================] - 25s 414us/sample - loss: 0.2198 - acc: 0.9330\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] - 23s 392us/sample - loss: 0.1034 - acc: 0.9687\n",
            "Epoch 3/8\n",
            "60000/60000 [==============================] - 27s 453us/sample - loss: 0.0498 - acc: 0.9853\n",
            "10000/10000 [==============================] - 4s 354us/sample - loss: 0.0825 - acc: 0.9791\n",
            "38368/60000 [==================>...........] - ETA: 9s - loss: 0.2720 - acc: 0.9181Model saved to disk\n",
            "49024/60000 [=======================>......] - ETA: 4s - loss: 0.2462 - acc: 0.9254Epoch 1/8\n",
            "60000/60000 [==============================] - 26s 438us/sample - loss: 0.2285 - acc: 0.9310\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] - 24s 405us/sample - loss: 0.0976 - acc: 0.9700\n",
            "Epoch 3/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "60000/60000 [==============================] - 23s 384us/sample - loss: 0.0723 - acc: 0.9775\n",
            "Epoch 4/8\n",
            "60000/60000 [==============================] - 29s 482us/sample - loss: 0.2495 - acc: 0.9254\n",
            "53120/60000 [=========================>....] - ETA: 3s - loss: 0.1113 - acc: 0.9660Epoch 2/8\n",
            "60000/60000 [==============================] - 27s 453us/sample - loss: 0.1100 - acc: 0.9666\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 3/8\n",
            "60000/60000 [==============================] - 25s 417us/sample - loss: 0.0549 - acc: 0.9827\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 26s 429us/sample - loss: 0.0723 - acc: 0.9771\n",
            "Epoch 4/8\n",
            "60000/60000 [==============================] - 28s 473us/sample - loss: 0.1224 - acc: 0.9633\n",
            "53696/60000 [=========================>....] - ETA: 2s - loss: 0.0412 - acc: 0.9871Epoch 3/8\n",
            "60000/60000 [==============================] - 28s 460us/sample - loss: 0.0841 - acc: 0.9745\n",
            "59584/60000 [============================>.] 5152/60000 [=>............................] - ETA: 0s - loss: 0.0417 - acc: 0.9870 - ETA: 24s - loss: 0.0869 - acc: 0.9726Epoch 4/8\n",
            "60000/60000 [==============================] - 25s 412us/sample - loss: 0.0417 - acc: 0.9869\n",
            "55840/60000 [==========================>...] - ETA: 1s - loss: 0.0570 - acc: 0.9815Epoch 6/8\n",
            "60000/60000 [==============================] - 26s 431us/sample - loss: 0.0571 - acc: 0.9815\n",
            " 4608/60000 [=>............................] 4480/60000 [=>............................] - ETA: 24s - loss: 0.0684 - acc: 0.9803 - ETA: 22s - loss: 0.0270 - acc: 0.9911Epoch 5/8\n",
            "60000/60000 [==============================] - 25s 417us/sample - loss: 0.0348 - acc: 0.9889\n",
            "58080/60000 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9725Epoch 7/8\n",
            "60000/60000 [==============================] - 28s 473us/sample - loss: 0.0951 - acc: 0.9725\n",
            "57504/60000 [===========================>..] - ETA: 1s - loss: 0.0681 - acc: 0.9791Epoch 4/8\n",
            "60000/60000 [==============================] - 27s 453us/sample - loss: 0.0683 - acc: 0.9789\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 26s 432us/sample - loss: 0.0497 - acc: 0.9843\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 25s 410us/sample - loss: 0.0302 - acc: 0.9900\n",
            "Epoch 8/8\n",
            "60000/60000 [==============================] - 28s 465us/sample - loss: 0.0785 - acc: 0.9774\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 26s 434us/sample - loss: 0.0419 - acc: 0.9862\n",
            "59232/60000 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9819Epoch 7/8\n",
            "60000/60000 [==============================] - 27s 453us/sample - loss: 0.0589 - acc: 0.9818\n",
            "  736/60000 [..............................] - ETA: 30s - loss: 0.0789 - acc: 0.9796Epoch 6/8\n",
            "60000/60000 [==============================] - 25s 410us/sample - loss: 0.0247 - acc: 0.9919\n",
            "10000/10000 [==============================]57056/60000 [===========================>..] - 4s 377us/sample - loss: 0.0673 - acc: 0.9824\n",
            "53824/60000 [=========================>....] - ETA: 2s - loss: 0.0502 - acc: 0.9846Model saved to disk\n",
            "60000/60000 [==============================] - 25s 423us/sample - loss: 0.0355 - acc: 0.9885\n",
            "55168/60000 [==========================>...] - ETA: 2s - loss: 0.0680 - acc: 0.9799Epoch 8/8\n",
            "60000/60000 [==============================] - 27s 444us/sample - loss: 0.0501 - acc: 0.9843\n",
            " 3616/60000 [>.............................] - ETA: 25s - loss: 0.0394 - acc: 0.9862Epoch 7/8\n",
            " 4128/60000 [=>............................] - ETA: 25s - loss: 0.0403 - acc: 0.9864Epoch 1/8\n",
            "60000/60000 [==============================] - 28s 461us/sample - loss: 0.0683 - acc: 0.9797\n",
            "  256/60000 [..............................] - ETA: 1:33 - loss: 2.0741 - acc: 0.3672 5024/60000 [=>............................] - ETA: 24s - loss: 0.0395 - acc: 0.9867Epoch 6/8\n",
            "60000/60000 [==============================] - 26s 429us/sample - loss: 0.0336 - acc: 0.9888\n",
            "60000/60000 [==============================] - 25s 416us/sample - loss: 0.2489 - acc: 0.9274\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] - 27s 447us/sample - loss: 0.0453 - acc: 0.9855\n",
            "56448/60000 [===========================>..] - ETA: 1s - loss: 0.0617 - acc: 0.9812Epoch 8/8\n",
            "10000/10000 [==============================] - 4s 360us/sample - loss: 0.0823 - acc: 0.9789\n",
            "59264/60000 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9812Model saved to disk\n",
            "60000/60000 [==============================] - 28s 461us/sample - loss: 0.0620 - acc: 0.9812\n",
            "Epoch 7/8\n",
            " 6464/60000 [==>...........................] - ETA: 24s - loss: 0.0498 - acc: 0.9862Epoch 1/8\n",
            "51264/60000 [========================>.....]60000/60000 [==============================] - ETA: 3s - loss: 0.0398 - acc: 0.9877 - 25s 411us/sample - loss: 0.1131 - acc: 0.9653\n",
            "Epoch 3/8\n",
            "60000/60000 [==============================] - 27s 451us/sample - loss: 0.0411 - acc: 0.9873\n",
            "60000/60000 [==============================] - 28s 463us/sample - loss: 0.0546 - acc: 0.9840\n",
            " 5344/10000 [===============>..............] - ETA: 1s - loss: 0.1098 - acc: 0.9736Epoch 8/8\n",
            "60000/60000 [==============================] - 26s 437us/sample - loss: 0.2425 - acc: 0.9268\n",
            " 3328/60000 [>.............................] - ETA: 23s - loss: 0.0358 - acc: 0.9883Epoch 2/8\n",
            "10000/10000 [==============================] - 4s 366us/sample - loss: 0.0834 - acc: 0.9801\n",
            " 4640/60000 [=>............................] - ETA: 23s - loss: 0.0422 - acc: 0.9873Model saved to disk\n",
            "10848/60000 [====>.........................] - ETA: 19s - loss: 0.1183 - acc: 0.9636Epoch 1/8\n",
            "60000/60000 [==============================] - 24s 393us/sample - loss: 0.0813 - acc: 0.9746\n",
            "25248/60000 [===========>..................] - ETA: 16s - loss: 0.3750 - acc: 0.8814Epoch 4/8\n",
            "60000/60000 [==============================] - 25s 423us/sample - loss: 0.1149 - acc: 0.9647\n",
            "Epoch 3/8\n",
            "60000/60000 [==============================] - 27s 456us/sample - loss: 0.0486 - acc: 0.9857\n",
            "10000/10000 [==============================] - 4s 370us/sample - loss: 0.0792 - acc: 0.9788\n",
            "12992/60000 [=====>........................]36736/60000 [=================>............] - ETA: 17s - loss: 0.0855 - acc: 0.9738 - ETA: 9s - loss: 0.0649 - acc: 0.9797Model saved to disk\n",
            "14144/60000 [======>.......................]60000/60000 [==============================] - ETA: 17s - loss: 0.0861 - acc: 0.9736 - 26s 441us/sample - loss: 0.2632 - acc: 0.9188\n",
            "37984/60000 [=================>............] - ETA: 8s - loss: 0.0643 - acc: 0.9798Epoch 2/8\n",
            "Epoch 1/811264/60000 [====>.........................] - ETA: 19s - loss: 0.1356 - acc: 0.9578\n",
            "60000/60000 [==============================] - 23s 380us/sample - loss: 0.0643 - acc: 0.9802\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 24s 397us/sample - loss: 0.0885 - acc: 0.9726\n",
            "Epoch 4/8\n",
            "60000/60000 [==============================] - 26s 427us/sample - loss: 0.1294 - acc: 0.9608\n",
            "44224/60000 [=====================>........] - ETA: 7s - loss: 0.3343 - acc: 0.8983Epoch 3/8\n",
            "60000/60000 [==============================] - 24s 404us/sample - loss: 0.0526 - acc: 0.9831\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 29s 479us/sample - loss: 0.2973 - acc: 0.9104\n",
            " 2144/60000 [>.............................] - ETA: 23s - loss: 0.0353 - acc: 0.9869Epoch 2/8\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "60000/60000 [==============================] - 26s 428us/sample - loss: 0.0734 - acc: 0.9779\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 27s 448us/sample - loss: 0.1055 - acc: 0.9688\n",
            "42144/60000 [====================>.........] - ETA: 8s - loss: 0.1560 - acc: 0.9563Epoch 4/8\n",
            "60000/60000 [==============================] - 24s 406us/sample - loss: 0.0452 - acc: 0.9852\n",
            "28608/60000 [=============>................] - ETA: 13s - loss: 0.0585 - acc: 0.9816Epoch 7/8\n",
            "60000/60000 [==============================] - 28s 462us/sample - loss: 0.1517 - acc: 0.9569\n",
            "17568/60000 [=======>......................] - ETA: 20s - loss: 0.0813 - acc: 0.9759Epoch 3/8\n",
            "60000/60000 [==============================] - 26s 435us/sample - loss: 0.0648 - acc: 0.9798\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 27s 456us/sample - loss: 0.0893 - acc: 0.9733\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 24s 404us/sample - loss: 0.0378 - acc: 0.9874\n",
            "Epoch 8/8\n",
            "60000/60000 [==============================] - 28s 470us/sample - loss: 0.1192 - acc: 0.9668\n",
            "Epoch 4/8\n",
            "33728/60000 [===============>..............] - ETA: 11s - loss: 0.0779 - acc: 0.9773Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2adCE0ogvdI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# q1Final.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        " tf.keras.layers.Flatten(),\n",
        " tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        " tf.keras.layers.Dropout(0.2),\n",
        " tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        " tf.keras.layers.Dropout(0.2),\n",
        " tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        " loss='sparse_categorical_crossentropy',\n",
        " metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=8)\n",
        "model.evaluate(x_test, y_test)\n",
        "model_json = model.to_json()\n",
        "json_file = open(\"model_MLP.json\", \"w\")\n",
        "json_file.write(model_json)\n",
        "json_file.close()\n",
        "model.save_weights(\"model_MLP.h5\")\n",
        "print(\"Model saved to disk\")\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoUFM86cvdJC",
        "colab_type": "text"
      },
      "source": [
        "## Questão 2\n",
        "\n",
        "Inicialmente, executou-se 5 vezes o código sugerido inicial a fim de verificar o desempenho da proposta. Seu desempenho foi de:\n",
        "\n",
        "* Loss: 0.0260; Acurácia: 0.9909.\n",
        "\n",
        "Utilizando-se o método de tentativa e erro, foi criado um script que verificava o desempenho da rede para diferentes parâmetros alterados como dropout (0.1 a 0.6), número de filtros (32 a 64), épocas de treinamento (2 a 6) e formato dos kernel utilizados (2x2 ou 3x3). O script executava esta mudança de parâmetros dentro de loops \"for\" para executar todas as combinações possíveis e tirava também a média das múltiplas execuções com mesmos parâmetros, a fim de se obter uma média de desempenho mais confiável. Os resultados desta varredura eram salvos ao final das execuções em um arquivo \"listas.txt\", permitindo ao usuários verificar qual a configuração obteve melhor desempenho.\n",
        "Foram utilizadas 4 threads - para varredura de redes de 1 a 4 camadas - durante o treinamento, a fim de promover paralelismo e diminuir o tempo requerido, que era ainda maior que o demandado para a questão 1. Verificou-se que com 6 épocas de treinamento o resultdo era levemente melhorado, mas não siginficativamente. A variação das demais grandezas fazia o desempenho diminuir nos testes. Então, após a varredura, foi realizado mais uma tentativa de treinamento com adição de uma camada convolucional e dropout de 0.3, o que elevou os resultados e nos trouxe à proposta final de código para esta questão.\n",
        "\n",
        "Para a proposta final deste modelo, os parâmetros alterados que resultaram no melhor desempenho durante a varredura foram:\n",
        "\n",
        "* Adição de uma camada convolucional (seguida de uma max pool) após a saída da primeira layer de max pool; 6 épocas de treinamento. Os demais parâmetros foram mantidos por não apresentar vantagem média significativa.\n",
        "\n",
        "O desempenho médio obtido foi de:\n",
        "\n",
        "* Loss: 0.0190; Acurácia: 0.9931.\n",
        "\n",
        "Ambas as soluções consumiram um tempo de execução da ordem de poucos minutos e a diferença de desempenho foi cerca de 0,22% em ganho.\n",
        "\n",
        "Os arquivos utilizados foram (no diretório q2):\n",
        "\n",
        "Proposta Inicial: q2Inicial.py\n",
        "Script de Varredura de parâmetros: q2.py\n",
        "Proposta Final: q2Final.py\n",
        "\n",
        "## Comparação entre ELM, MLP e CNN\n",
        "\n",
        "Desempenho:\n",
        "* ELM: 91,09%\n",
        "* MLP: 98,28%\n",
        "* CNN: 99,31%\n",
        "\n",
        "Nota-se claramente que a CNN apresenta o melhor desempenho dentre as 3 melhores técnicas utilizadas. Porém, o processo de treinamento para otimização desta toma dezenas de horas, enquanto que a ELM requer apenas alguns minutos para ser ajustada e ficar cerca de 8% abaixo em desempenho. Logo, se houver recursos computacionais, a CNN é a melhor escolha para este tipo de problema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PED7944vdJE",
        "colab_type": "code",
        "outputId": "c7f3726b-a533-4fe6-98c6-dbed542820e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# q2Inicial.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "# reshape to be [samples][width][height][pixels]\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
        " activation='relu',\n",
        "input_shape=(28, 28, 1)))\n",
        "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        " loss='sparse_categorical_crossentropy',\n",
        " metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "model.evaluate(x_test, y_test)\n",
        "model_json = model.to_json()\n",
        "json_file = open(\"model_CNN.json\", \"w\")\n",
        "json_file.write(model_json)\n",
        "json_file.close()\n",
        "model.save_weights(\"model_CNN.h5\")\n",
        "print(\"Model saved to disk\")\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 18s 303us/sample - loss: 0.1870 - acc: 0.9441\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 15s 254us/sample - loss: 0.0799 - acc: 0.9761\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 15s 246us/sample - loss: 0.0602 - acc: 0.9819\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 14s 240us/sample - loss: 0.0478 - acc: 0.9852\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 14s 241us/sample - loss: 0.0400 - acc: 0.9875\n",
            "10000/10000 [==============================] - 1s 112us/sample - loss: 0.0331 - acc: 0.9892\n",
            "Model saved to disk\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/PODE APAGAR/EA072-EF1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY-q7n4UvdJH",
        "colab_type": "code",
        "outputId": "f87447bb-819b-44b7-8ec8-67b76dcc991f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# q2.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import threading\n",
        "\n",
        "myMutex = threading.Lock()\n",
        "value = \"teste\"\n",
        "\n",
        "numeroDeNeuronios = []\n",
        "numeroDeEpocas = []\n",
        "numeroDeCamadas = []\n",
        "numeroDeDropout = []\n",
        "taxaDeAcertos = []\n",
        "\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread1Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 40, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [2, 6]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de filtros.\n",
        "\t\tfor filtros in [32, 64]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(filtros) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,3):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\t# reshape to be [samples][width][height][pixels]\n",
        "\t\t\t\t\tx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential()\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros, kernel_size=(3, 3),\n",
        "\t\t\t\t\t activation='relu',\n",
        "\t\t\t\t\tinput_shape=(28, 28, 1)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (3, 3), activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Flatten())\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_CNN1.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_CNN1.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(filtros)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread2Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 40, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [2, 6]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de filtros.\n",
        "\t\tfor filtros in [32, 64]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(filtros) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,3):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\t# reshape to be [samples][width][height][pixels]\n",
        "\t\t\t\t\tx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential()\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros, kernel_size=(2, 2),\n",
        "\t\t\t\t\t activation='relu',\n",
        "\t\t\t\t\tinput_shape=(28, 28, 1)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (3, 3), activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (3, 3), activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Flatten())\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_CNN2.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_CNN2.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(filtros)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread3Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 40, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [2, 6]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de filtros.\n",
        "\t\tfor filtros in [32, 64]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(filtros) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,3):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\t# reshape to be [samples][width][height][pixels]\n",
        "\t\t\t\t\tx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential()\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros, kernel_size=(3, 3),\n",
        "\t\t\t\t\t activation='relu',\n",
        "\t\t\t\t\tinput_shape=(28, 28, 1)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (2, 2), activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Flatten())\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_CNN3.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_CNN3.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(filtros)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
        "def thread4Camadas(camadas):\n",
        "\t\n",
        "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
        "\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\n",
        "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
        "\tvaloresDropout = range(10, 40, 10)# Variaremos de 10% em 10%.\n",
        "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
        "\t\n",
        "\t# Testando resultados com diferentes quantidades de epocas.\n",
        "\tfor epocas in [2, 6]:\n",
        "\t\n",
        "\t\t# Testando resultados com diferentes quantidades de filtros.\n",
        "\t\tfor filtros in [32, 64]:\n",
        "\t\t\n",
        "\t\t\t# So para indicar em que passo da execucao estamos.\n",
        "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(filtros) + \"\\n\\n\")\n",
        "\t\n",
        "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
        "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
        "\t\t\tfor taxaDropout in valoresDropout:\n",
        "\t\t\t\n",
        "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
        "\t\t\t\tfor iteracaoMedia in range(1,3):\n",
        "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
        "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\t\t\t\t\t# reshape to be [samples][width][height][pixels]\n",
        "\t\t\t\t\tx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\t\t\t\t\tmodel = tf.keras.models.Sequential()\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros, kernel_size=(3, 3),\n",
        "\t\t\t\t\t activation='relu',\n",
        "\t\t\t\t\tinput_shape=(28, 28, 1)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (3, 3), activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (3, 3), activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3)))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Flatten())\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
        "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
        "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
        "\t\t\t\t\t metrics=['accuracy'])\n",
        "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
        "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
        "\t\t\t\t\tmodel_json = model.to_json()\n",
        "\t\t\t\t\tjson_file = open(\"model_CNN4.json\", \"w\")\n",
        "\t\t\t\t\tjson_file.write(model_json)\n",
        "\t\t\t\t\tjson_file.close()\n",
        "\t\t\t\t\tmodel.save_weights(\"model_CNN4.h5\")\n",
        "\t\t\t\t\tprint(\"Model saved to disk\")\n",
        "\t\t\t\t\tos.getcwd()\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\tmyMutex.acquire()\n",
        "\t\t\t\tnumeroDeNeuronios.append(filtros)\n",
        "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
        "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
        "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
        "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
        "\t\t\t\tmyMutex.release()\n",
        "\t\t\t\t\n",
        "\t\t\t\t# Reiniciamos a soma.\n",
        "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\t\n",
        "\tcamadas1 = threading.Thread(target=thread1Camadas,args=(1,))\n",
        "\tcamadas2 = threading.Thread(target=thread2Camadas,args=(2,))\n",
        "\tcamadas3 = threading.Thread(target=thread3Camadas,args=(3,))\n",
        "\tcamadas4 = threading.Thread(target=thread4Camadas,args=(4,))\n",
        "\t\n",
        "\tcamadas1.start()\n",
        "\tcamadas2.start()\n",
        "\tcamadas3.start()\n",
        "\tcamadas4.start()\n",
        "\t\n",
        "\ttry:\n",
        "\t\tcamadas4.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\ttry:\n",
        "\t\tcamadas3.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\ttry:\n",
        "\t\tcamadas2.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\ttry:\n",
        "\t\tcamadas1.join(); \n",
        "\texcept:\n",
        "\t\tpass;\n",
        "\t\t\n",
        "\tlistasFile = open(\"listasCONV.txt\", \"w\")\n",
        "\tlistasFile.write(str(numeroDeNeuronios) + \"\\n\")\n",
        "\tlistasFile.write(str(numeroDeEpocas) + \"\\n\")\n",
        "\tlistasFile.write(str(numeroDeCamadas) + \"\\n\")\n",
        "\tlistasFile.write(str(numeroDeDropout) + \"\\n\")\n",
        "\tlistasFile.write(str(taxaDeAcertos) + \"\\n\")\n",
        "\tlistasFile.close()\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS1: 32\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS2: 32\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS3: 32\n",
            "\n",
            "\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS4: 32\n",
            "\n",
            "\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Epoch 1/2\n",
            "Epoch 1/2\n",
            "Epoch 1/2\n",
            "Epoch 1/2\n",
            "60000/60000 [==============================] - 50s 826us/sample - loss: 0.1274 - acc: 0.9620\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 51s 843us/sample - loss: 0.1204 - acc: 0.9633\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 51s 844us/sample - loss: 0.1283 - acc: 0.9597\n",
            "59488/60000 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9471Epoch 2/2\n",
            "60000/60000 [==============================] - 51s 852us/sample - loss: 0.1630 - acc: 0.9474\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 48s 799us/sample - loss: 0.0443 - acc: 0.9860\n",
            "60000/60000 [==============================] - 49s 814us/sample - loss: 0.0411 - acc: 0.9872\n",
            "60000/60000 [==============================] - 48s 808us/sample - loss: 0.0436 - acc: 0.9866\n",
            "60000/60000 [==============================] - 48s 807us/sample - loss: 0.0505 - acc: 0.9848\n",
            "10000/10000 [==============================] - 4s 432us/sample - loss: 0.0383 - acc: 0.9875\n",
            " 6464/10000 [==================>...........] - ETA: 1s - loss: 0.0485 - acc: 0.9845Model saved to disk\n",
            "10000/10000 [==============================] - 4s 361us/sample - loss: 0.0351 - acc: 0.9873\n",
            " 9280/10000 [==========================>...] - ETA: 0s - loss: 0.0361 - acc: 0.9889Model saved to disk\n",
            "10000/10000 [==============================] - 4s 370us/sample - loss: 0.0387 - acc: 0.9877\n",
            "10000/10000 [==============================] - 4s 351us/sample - loss: 0.0352 - acc: 0.9892\n",
            "Model saved to disk\n",
            "Model saved to disk\n",
            "Epoch 1/2\n",
            " 8224/60000 [===>..........................] - ETA: 19s - loss: 0.3740 - acc: 0.8801Epoch 1/2\n",
            "  384/60000 [..............................] - ETA: 1:53 - loss: 1.7475 - acc: 0.4870Epoch 1/2\n",
            "  512/60000 [..............................] - ETA: 1:37 - loss: 1.5290 - acc: 0.5488Epoch 1/2\n",
            "60000/60000 [==============================] - 43s 722us/sample - loss: 0.1256 - acc: 0.9612\n",
            "47104/60000 [======================>.......] - ETA: 10s - loss: 0.1417 - acc: 0.9557Epoch 2/2\n",
            "60000/60000 [==============================] - 49s 821us/sample - loss: 0.1259 - acc: 0.9614\n",
            "58240/60000 [============================>.]\n",
            "60000/60000 [==============================] - 50s 829us/sample - loss: 0.1638 - acc: 0.9488\n",
            "12960/60000 [=====>........................] - ETA: 37s - loss: 0.0397 - acc: 0.9874Epoch 2/2\n",
            "60000/60000 [==============================] - 50s 834us/sample - loss: 0.1237 - acc: 0.9616\n",
            "  192/60000 [..............................] - ETA: 45s - loss: 0.0227 - acc: 1.0000Epoch 2/2\n",
            "60000/60000 [==============================] - 48s 795us/sample - loss: 0.0445 - acc: 0.9864\n",
            "10000/10000 [==============================] - 7s 651us/sample - loss: 0.0395 - acc: 0.9871\n",
            "57408/60000 [===========================>..] - ETA: 2s - loss: 0.0454 - acc: 0.9858Model saved to disk\n",
            "60000/60000 [==============================] - 47s 787us/sample - loss: 0.0450 - acc: 0.9859\n",
            "59072/60000 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9845Epoch 1/2\n",
            "60000/60000 [==============================] - 48s 794us/sample - loss: 0.0494 - acc: 0.9845\n",
            "60000/60000 [==============================] - 48s 793us/sample - loss: 0.0422 - acc: 0.9870\n",
            "10000/10000 [==============================] - 5s 450us/sample - loss: 0.0364 - acc: 0.9881\n",
            " 6528/10000 [==================>...........] - ETA: 1s - loss: 0.0496 - acc: 0.9833Model saved to disk\n",
            "10000/10000 [==============================] - 4s 398us/sample - loss: 0.0275 - acc: 0.9909\n",
            "10000/10000 [==============================] - 4s 407us/sample - loss: 0.0458 - acc: 0.9852\n",
            "10208/60000 [====>.........................] - ETA: 24s - loss: 0.3821 - acc: 0.8813Model saved to disk\n",
            "10368/60000 [====>.........................] - ETA: 24s - loss: 0.3781 - acc: 0.8825Model saved to disk\n",
            "21376/60000 [=========>....................] - ETA: 15s - loss: 0.2515 - acc: 0.9227\n",
            "26048/60000 [============>.................] - ETA: 14s - loss: 0.2300 - acc: 0.9294Epoch 1/2\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 1/2\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "60000/60000 [==============================] - 37s 624us/sample - loss: 0.1490 - acc: 0.9541\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 48s 802us/sample - loss: 0.1323 - acc: 0.9606\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 50s 838us/sample - loss: 0.1448 - acc: 0.9553\n",
            "30880/60000 [==============>...............] - ETA: 22s - loss: 0.0596 - acc: 0.9816Epoch 2/2\n",
            "60000/60000 [==============================] - 51s 846us/sample - loss: 0.1890 - acc: 0.9397\n",
            " 6272/60000 [==>...........................] - ETA: 43s - loss: 0.0538 - acc: 0.9841Epoch 2/2\n",
            "60000/60000 [==============================] - 47s 786us/sample - loss: 0.0555 - acc: 0.9827\n",
            "10000/10000 [==============================] - 7s 663us/sample - loss: 0.0408 - acc: 0.9864\n",
            "44512/60000 [=====================>........] - ETA: 12s - loss: 0.0497 - acc: 0.9848Model saved to disk\n",
            "42144/60000 [====================>.........] - ETA: 13s - loss: 0.0632 - acc: 0.9807Epoch 1/2\n",
            "60000/60000 [==============================] - 47s 776us/sample - loss: 0.0489 - acc: 0.9848\n",
            "60000/60000 [==============================] - 47s 785us/sample - loss: 0.0504 - acc: 0.9846\n",
            "60000/60000 [==============================] - 47s 779us/sample - loss: 0.0618 - acc: 0.9811\n",
            "10000/10000 [==============================] - 6s 585us/sample - loss: 0.0368 - acc: 0.9881\n",
            " 1472/10000 [===>..........................] - ETA: 5s - loss: 0.0419 - acc: 0.9851\n",
            "10000/10000 [==============================] - 4s 406us/sample - loss: 0.0337 - acc: 0.9889\n",
            "10000/10000 [==============================] - 4s 420us/sample - loss: 0.0291 - acc: 0.9899\n",
            "26976/60000 [============>.................] - ETA: 22s - loss: 0.2178 - acc: 0.9341Epoch 1/2\n",
            "27456/60000 [============>.................] - ETA: 21s - loss: 0.2154 - acc: 0.9347Model saved to disk\n",
            "27872/60000 [============>.................] - ETA: 21s - loss: 0.2142 - acc: 0.9350Model saved to disk\n",
            " 9888/60000 [===>..........................] - ETA: 29s - loss: 0.3649 - acc: 0.8828Epoch 1/2\n",
            "39520/60000 [==================>...........] - ETA: 12s - loss: 0.1787 - acc: 0.9457Epoch 1/2\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "60000/60000 [==============================] - 40s 670us/sample - loss: 0.1445 - acc: 0.9559\n",
            "30464/60000 [==============>...............] - ETA: 21s - loss: 0.1944 - acc: 0.9386Epoch 2/2\n",
            "60000/60000 [==============================] - 46s 774us/sample - loss: 0.1350 - acc: 0.9581\n",
            "30720/60000 [==============>...............] - ETA: 23s - loss: 0.0515 - acc: 0.9830Epoch 2/2\n",
            "60000/60000 [==============================] - 51s 843us/sample - loss: 0.1909 - acc: 0.9381\n",
            "12288/60000 [=====>........................] - ETA: 38s - loss: 0.0530 - acc: 0.9825Epoch 2/2\n",
            "60000/60000 [==============================] - 51s 844us/sample - loss: 0.1442 - acc: 0.9547\n",
            "  128/60000 [..............................] - ETA: 43s - loss: 0.0514 - acc: 0.9844 Epoch 2/2\n",
            "60000/60000 [==============================]16224/60000 [=======>......................] - 48s 796us/sample - loss: 0.0515 - acc: 0.9837\n",
            "10000/10000 [==============================] - 7s 653us/sample - loss: 0.0452 - acc: 0.9860\n",
            "Model saved to disk25984/60000 [===========>..................] - ETA: 26s - loss: 0.0672 - acc: 0.9791\n",
            "43904/60000 [====================>.........] - ETA: 12s - loss: 0.0510 - acc: 0.9838Epoch 1/2\n",
            "60000/60000 [==============================] - 47s 787us/sample - loss: 0.0505 - acc: 0.9841\n",
            "10000/10000 [==============================] - 6s 650us/sample - loss: 0.0311 - acc: 0.9901\n",
            "25920/60000 [===========>..................] - ETA: 26s - loss: 0.2459 - acc: 0.9267Model saved to disk\n",
            "60000/60000 [==============================] - 46s 775us/sample - loss: 0.0607 - acc: 0.9813\n",
            "60000/60000 [==============================] - 47s 776us/sample - loss: 0.0539 - acc: 0.9836\n",
            " 2368/10000 [======>.......................] - ETA: 3s - loss: 0.0474 - acc: 0.9852\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 1/2\n",
            "10000/10000 [==============================] - 5s 484us/sample - loss: 0.0311 - acc: 0.9904\n",
            "39680/60000 [==================>...........] - ETA: 14s - loss: 0.1974 - acc: 0.9410Model saved to disk\n",
            "10000/10000 [==============================] - 5s 526us/sample - loss: 0.0289 - acc: 0.9907\n",
            "41888/60000 [===================>..........] - ETA: 12s - loss: 0.1925 - acc: 0.9424Model saved to disk\n",
            "50432/60000 [========================>.....] - ETA: 6s - loss: 0.1747 - acc: 0.9475Epoch 1/2\n",
            "18464/60000 [========>.....................] - ETA: 23s - loss: 0.2745 - acc: 0.9122Epoch 1/2\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "60000/60000 [==============================] - 40s 674us/sample - loss: 0.1594 - acc: 0.9520\n",
            " 6848/60000 [==>...........................] - ETA: 47s - loss: 0.5387 - acc: 0.8259Epoch 2/2\n",
            "60000/60000 [==============================] - 44s 741us/sample - loss: 0.1494 - acc: 0.9534\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 2/2\n",
            "60000/60000 [==============================] - 50s 835us/sample - loss: 0.2287 - acc: 0.9272\n",
            "58176/60000 [============================>.] - ETA: 1s - loss: 0.1572 - acc: 0.9509Epoch 2/2\n",
            "60000/60000 [==============================] - 51s 843us/sample - loss: 0.1546 - acc: 0.9517\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 48s 799us/sample - loss: 0.0640 - acc: 0.9805\n",
            "10000/10000 [==============================] - 7s 685us/sample - loss: 0.0399 - acc: 0.9866\n",
            "35744/60000 [================>.............] - ETA: 18s - loss: 0.0611 - acc: 0.9808Model saved to disk\n",
            "40992/60000 [===================>..........] - ETA: 14s - loss: 0.0599 - acc: 0.9812Epoch 1/2\n",
            "60000/60000 [==============================] - 47s 776us/sample - loss: 0.0586 - acc: 0.9816\n",
            "10000/10000 [==============================] - 7s 679us/sample - loss: 0.0383 - acc: 0.9878\n",
            "50912/60000 [========================>.....] - ETA: 6s - loss: 0.0775 - acc: 0.9756Model saved to disk\n",
            "54016/60000 [==========================>...]35680/60000 [================>.............] - ETA: 4s - loss: 0.0576 - acc: 0.9822 - ETA: 18s - loss: 0.2101 - acc: 0.9344Epoch 1/2\n",
            "60000/60000 [==============================] - 45s 757us/sample - loss: 0.0761 - acc: 0.9762\n",
            "60000/60000 [==============================] - 46s 760us/sample - loss: 0.0575 - acc: 0.9823\n",
            "10000/10000 [==============================] - 6s 577us/sample - loss: 0.0355 - acc: 0.9888\n",
            "50080/60000 [========================>.....] - ETA: 7s - loss: 0.1779 - acc: 0.9446Model saved to disk\n",
            "14784/60000 [======>.......................]10000/10000 [==============================] - ETA: 30s - loss: 0.3160 - acc: 0.9050 - 5s 536us/sample - loss: 0.0338 - acc: 0.9897\n",
            "16320/60000 [=======>......................] - ETA: 29s - loss: 0.3002 - acc: 0.9099Model saved to disk\n",
            "60000/60000 [==============================] - 41s 684us/sample - loss: 0.1628 - acc: 0.9495\n",
            "Epoch 2/2\n",
            " 1120/60000 [..............................] - ETA: 30s - loss: 0.0775 - acc: 0.9759Epoch 1/2\n",
            " 3968/60000 [>.............................] - ETA: 30s - loss: 0.0725 - acc: 0.9783Epoch 1/2\n",
            "60000/60000 [==============================] - 43s 724us/sample - loss: 0.1534 - acc: 0.9537\n",
            "33408/60000 [===============>..............] - ETA: 22s - loss: 0.3297 - acc: 0.8931Epoch 2/2\n",
            "60000/60000 [==============================] - 47s 782us/sample - loss: 0.0659 - acc: 0.9800\n",
            "60000/60000 [==============================] - 50s 835us/sample - loss: 0.2341 - acc: 0.9254\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 50s 831us/sample - loss: 0.1573 - acc: 0.9507\n",
            "Epoch 2/2\n",
            "10000/10000 [==============================] - 7s 664us/sample - loss: 0.0400 - acc: 0.9865\n",
            " 5056/60000 [=>............................]\n",
            " - ETA: 38s - loss: 0.0831 - acc: 0.9741\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS3: 64\n",
            "\n",
            "\n",
            "37472/60000 [=================>............] - ETA: 17s - loss: 0.0610 - acc: 0.9809Epoch 1/2\n",
            "60000/60000 [==============================] - 49s 821us/sample - loss: 0.0605 - acc: 0.9813\n",
            "10000/10000 [==============================] - 7s 749us/sample - loss: 0.0371 - acc: 0.9879\n",
            "42752/60000 [====================>.........] - ETA: 14s - loss: 0.0826 - acc: 0.9742Model saved to disk\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "40960/60000 [===================>..........]\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS1: 64\n",
            "\n",
            " - ETA: 16s - loss: 0.0630 - acc: 0.9810\n",
            "47488/60000 [======================>.......] - ETA: 10s - loss: 0.0816 - acc: 0.9745Epoch 1/2\n",
            "60000/60000 [==============================] - 53s 879us/sample - loss: 0.0789 - acc: 0.9754\n",
            "60000/60000 [==============================] - 53s 882us/sample - loss: 0.0610 - acc: 0.9817\n",
            "10000/10000 [==============================] - 8s 760us/sample - loss: 0.0314 - acc: 0.9905\n",
            "60000/60000 [==============================] - 54s 898us/sample - loss: 0.1171 - acc: 0.9645\n",
            "Epoch 2/2\n",
            "22592/60000 [==========>...................] - ETA: 36s - loss: 0.1974 - acc: 0.9415\n",
            "\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS4: 64\n",
            "\n",
            "\n",
            "10000/10000 [==============================] - 7s 721us/sample - loss: 0.0346 - acc: 0.9890\n",
            "24384/60000 [===========>..................] - ETA: 33s - loss: 0.1888 - acc: 0.9441Model saved to disk\n",
            "\n",
            "\n",
            "epocas: 2\n",
            "CAMADAS2: 64\n",
            "\n",
            "\n",
            "30432/60000 [==============>...............] - ETA: 26s - loss: 0.1667 - acc: 0.9501Epoch 1/2\n",
            "32480/60000 [===============>..............] - ETA: 24s - loss: 0.1616 - acc: 0.9514Epoch 1/2\n",
            "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1163 - acc: 0.9649\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 75s 1ms/sample - loss: 0.0413 - acc: 0.9868\n",
            "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0389 - acc: 0.9876\n",
            "Model saved to disk56896/60000 [===========================>..]\n",
            "60000/60000 [==============================] - 82s 1ms/sample - loss: 0.1316 - acc: 0.9578\n",
            "Epoch 2/2\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "60000/60000 [==============================] - 83s 1ms/sample - loss: 0.1056 - acc: 0.9677\n",
            " 1888/60000 [..............................] - ETA: 1:05 - loss: 0.0456 - acc: 0.9873Epoch 2/2\n",
            "35104/60000 [================>.............] - ETA: 32s - loss: 0.0399 - acc: 0.9879\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 1/2\n",
            "24256/60000 [===========>..................] - ETA: 49s - loss: 0.1883 - acc: 0.941460000/60000 [==============================] - ETA: 50s - loss: 0.0418 - acc: 0.9869 - 79s 1ms/sample - loss: 0.0404 - acc: 0.9876\n",
            "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0387 - acc: 0.9879\n",
            "34848/60000 [================>.............] - ETA: 32s - loss: 0.1545 - acc: 0.9520Model saved to disk\n",
            "37792/60000 [=================>............] - ETA: 29s - loss: 0.0398 - acc: 0.9875Epoch 1/2\n",
            "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.0446 - acc: 0.9864\n",
            "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.1177 - acc: 0.9635\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 80s 1ms/sample - loss: 0.0401 - acc: 0.9877\n",
            "10000/10000 [==============================] - 9s 890us/sample - loss: 0.0343 - acc: 0.9881\n",
            " 9408/60000 [===>..........................] - ETA: 45s - loss: 0.0413 - acc: 0.9877\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bModel saved to disk\n",
            "10000/10000 [==============================] - 8s 822us/sample - loss: 0.0354 - acc: 0.9883\n",
            "33984/60000 [===============>..............] - ETA: 30s - loss: 0.1520 - acc: 0.9536Model saved to disk\n",
            "38112/60000 [==================>...........] - ETA: 24s - loss: 0.1423 - acc: 0.9564Epoch 1/2\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "40736/60000 [===================>..........] - ETA: 21s - loss: 0.1367 - acc: 0.9582Epoch 1/2\n",
            "60000/60000 [==============================] - 72s 1ms/sample - loss: 0.1139 - acc: 0.9654\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.0429 - acc: 0.9865\n",
            "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0439 - acc: 0.9858\n",
            "30848/60000 [==============>...............] - ETA: 37s - loss: 0.0420 - acc: 0.9870Model saved to disk\n",
            "34656/60000 [================>.............] - ETA: 31s - loss: 0.0417 - acc: 0.9870Epoch 1/2\n",
            "60000/60000 [==============================] - 80s 1ms/sample - loss: 0.1323 - acc: 0.9583\n",
            "40512/60000 [===================>..........] - ETA: 24s - loss: 0.0413 - acc: 0.9870\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 2/2\n",
            "60000/60000 [==============================] - 82s 1ms/sample - loss: 0.1072 - acc: 0.9669\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.0411 - acc: 0.9873\n",
            "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0354 - acc: 0.9889\n",
            "36480/60000 [=================>............]29440/60000 [=============>................]26400/60000 [============>.................] - ETA: 30s - loss: 0.1717 - acc: 0.9478 - ETA: 40s - loss: 0.0474 - acc: 0.9852 - ETA: 44s - loss: 0.0427 - acc: 0.9866Model saved to disk\n",
            "40128/60000 [===================>..........] - ETA: 25s - loss: 0.1647 - acc: 0.9499Epoch 1/2\n",
            "42496/60000 [====================>.........] - ETA: 22s - loss: 0.1611 - acc: 0.9510Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Melt9--4vdJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# q2Final.py\n",
        "import tensorflow as tf\n",
        "import os\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "# reshape to be [samples][width][height][pixels]\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
        " activation='relu',\n",
        "input_shape=(28, 28, 1)))\n",
        "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        " loss='sparse_categorical_crossentropy',\n",
        " metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=6)\n",
        "model.evaluate(x_test, y_test)\n",
        "model_json = model.to_json()\n",
        "json_file = open(\"model_CNN.json\", \"w\")\n",
        "json_file.write(model_json)\n",
        "json_file.close()\n",
        "model.save_weights(\"model_CNN.h5\")\n",
        "print(\"Model saved to disk\")\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}