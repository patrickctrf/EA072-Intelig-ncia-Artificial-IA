{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kelx6PRGvdIt"
   },
   "source": [
    "# Patrick de Carvalho Tavares Rezende Ferreira - RA: 175480 - EFC1\n",
    "\n",
    "Repositório: https://github.com/patrickctrf/EA072-Inteligencia-Artificial-IA/tree/master/EFC1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KoUFM86cvdJC"
   },
   "source": [
    "## Questão 4\n",
    "\n",
    "Inicialmente, executou-se 5 vezes o código sugerido inicial a fim de verificar o desempenho da proposta. Seu desempenho foi de:\n",
    "\n",
    "* Loss: 0.0260; Acurácia: 0.9909.\n",
    "\n",
    "Utilizando-se o método de tentativa e erro, foi criado um script que verificava o desempenho da rede para diferentes parâmetros alterados como dropout (0.1 a 0.6), número de filtros (32 a 64), épocas de treinamento (2 a 6) e formato dos kernel utilizados (2x2 ou 3x3). O script executava esta mudança de parâmetros dentro de loops \"for\" para executar todas as combinações possíveis e tirava também a média das múltiplas execuções com mesmos parâmetros, a fim de se obter uma média de desempenho mais confiável. Os resultados desta varredura eram salvos ao final das execuções em um arquivo \"listas.txt\", permitindo ao usuários verificar qual a configuração obteve melhor desempenho.\n",
    "Foram utilizadas 4 threads - para varredura de redes de 1 a 4 camadas - durante o treinamento, a fim de promover paralelismo e diminuir o tempo requerido, que era ainda maior que o demandado para a questão 1. Verificou-se que com 6 épocas de treinamento o resultado era levemente melhorado, mas não siginficativamente. A variação das demais grandezas fazia o desempenho diminuir nos testes. Então, após a varredura, foi realizado mais uma tentativa de treinamento com adição de uma camada convolucional e dropout de 0.3, o que elevou os resultados e nos trouxe à proposta final de código para esta questão.\n",
    "\n",
    "Através da varredura, foi possível perceber que as alterações que implicavam em aumento de desempenho eram: Maior número de filtros, 2 camadas convolucionais, taxa de dropout próxima de 0.3 e kernel 3x3 (com max pool 2x2).\n",
    "\n",
    "Portanto, para a proposta final deste modelo, os parâmetros alterados que resultaram no melhor desempenho durante a varredura foram:\n",
    "\n",
    "* Adição de duas camadaa convolucionais com kernel 3x3 (seguida de uma max pool em 2x2) após a saída da primeira layer de max pool; 8 épocas de treinamento. Camadas convolucionais todas com 512 filtros e dropout de 0,3. Os demais parâmetros foram mantidos por não apresentar vantagem média significativa.\n",
    "\n",
    "O desempenho médio obtido foi de:\n",
    "\n",
    "* Loss: 0.0190; Acurácia: 0.9935.\n",
    "\n",
    "Ambas as soluções consumiram um tempo de execução da ordem de poucos minutos e a diferença de desempenho foi cerca de 0,22% em ganho.\n",
    "\n",
    "Os arquivos utilizados foram (no diretório q2):\n",
    "\n",
    "Proposta Inicial: q4Inicial.py\n",
    "\n",
    "Script de Varredura de parâmetros: q4.py\n",
    "\n",
    "Proposta Final: q4Final.py\n",
    "\n",
    "## Comparação entre ELM, MLP e CNN\n",
    "\n",
    "Desempenho:\n",
    "* ELM: 91,09%\n",
    "* MLP: 98,23%\n",
    "* CNN: 99,35%\n",
    "\n",
    "Nota-se claramente que a CNN apresenta o melhor desempenho dentre as 3 melhores técnicas utilizadas. Porém, o processo de treinamento para otimização desta toma dezenas de horas, enquanto que a ELM requer apenas alguns minutos para ser ajustada e ficar cerca de 8% abaixo em desempenho. Logo, se houver recursos computacionais, a CNN é a melhor escolha para este tipo de problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "6PED7944vdJE",
    "outputId": "ca057a82-a6d9-4fa8-a31d-39d5224fd433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 16s 268us/sample - loss: 0.2004 - acc: 0.9403\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 15s 256us/sample - loss: 0.0851 - acc: 0.9741\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 15s 251us/sample - loss: 0.0628 - acc: 0.9809\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 15s 252us/sample - loss: 0.0532 - acc: 0.9837\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 15s 249us/sample - loss: 0.0461 - acc: 0.9857\n",
      "10000/10000 [==============================] - 1s 114us/sample - loss: 0.0294 - acc: 0.9906\n",
      "Model saved to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/content/drive/My Drive/PODE APAGAR/EA072-EF1'"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q4Inicial.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][width][height][pixels]\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
    " activation='relu',\n",
    "input_shape=(28, 28, 1)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    " loss='sparse_categorical_crossentropy',\n",
    " metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)\n",
    "model_json = model.to_json()\n",
    "json_file = open(\"model_CNN.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "model.save_weights(\"model_CNN.h5\")\n",
    "print(\"Model saved to disk\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "RY-q7n4UvdJH",
    "outputId": "21f68464-4a4f-43f0-8707-9fbbf90c7416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epocas: 2\n",
      "CAMADAS1: 32\n",
      "\n",
      "\n",
      "\n",
      "epocas: 2\n",
      "CAMADAS2: 32\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epocas: 2\n",
      "CAMADAS3: 32\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epocas: 2\n",
      "CAMADAS4: 32\n",
      "\n",
      "\n",
      "Epoch 1/2\n",
      "  288/60000 [..............................] - ETA: 1:33 - loss: 1.8135 - acc: 0.4167Epoch 1/2\n",
      "  992/60000 [..............................] - ETA: 1:21 - loss: 2.0883 - acc: 0.2984Epoch 1/2\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      " 1504/60000 [..............................] - ETA: 1:04 - loss: 1.7619 - acc: 0.4116Epoch 1/2\n",
      "60000/60000 [==============================] - 49s 822us/sample - loss: 0.1270 - acc: 0.9617\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 52s 870us/sample - loss: 0.1650 - acc: 0.9486\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 52s 860us/sample - loss: 0.1242 - acc: 0.9617\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 52s 866us/sample - loss: 0.1229 - acc: 0.9619\n",
      "  768/60000 [..............................] - ETA: 48s - loss: 0.0613 - acc: 0.9831Epoch 2/2\n",
      "60000/60000 [==============================] - 51s 851us/sample - loss: 0.0459 - acc: 0.9858\n",
      "60000/60000 [==============================] - 52s 859us/sample - loss: 0.0510 - acc: 0.9843\n",
      "60000/60000 [==============================] - 51s 847us/sample - loss: 0.0430 - acc: 0.9866\n",
      "60000/60000 [==============================] - 51s 856us/sample - loss: 0.0432 - acc: 0.9869\n",
      "10000/10000 [==============================] - 6s 559us/sample - loss: 0.0365 - acc: 0.9876\n",
      " 1376/10000 [===>..........................] 3904/10000 [==========>...................] - ETA: 4s - loss: 0.0401 - acc: 0.9855 - ETA: 2s - loss: 0.0512 - acc: 0.9834Model saved to disk\n",
      "10000/10000 [==============================] - 4s 381us/sample - loss: 0.0422 - acc: 0.9862\n",
      "10000/10000 [==============================] - 4s 403us/sample - loss: 0.0334 - acc: 0.9891\n",
      " 7872/10000 [======================>.......] - ETA: 0s - loss: 0.0334 - acc: 0.9892Model saved to disk\n",
      " 8672/10000 [=========================>....] - ETA: 0s - loss: 0.0328 - acc: 0.9895\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bModel saved to disk\n",
      "10000/10000 [==============================] - 3s 327us/sample - loss: 0.0323 - acc: 0.9896\n",
      "Model saved to disk\n",
      "Epoch 1/2\n",
      " 9504/60000 [===>..........................] - ETA: 20s - loss: 0.3375 - acc: 0.8932Epoch 1/2\n",
      "10592/60000 [====>.........................] - ETA: 19s - loss: 0.3201 - acc: 0.8991Epoch 1/2\n",
      "11136/60000 [====>.........................] - ETA: 19s - loss: 0.3094 - acc: 0.9027Epoch 1/2\n",
      "60000/60000 [==============================] - 46s 769us/sample - loss: 0.1238 - acc: 0.9611\n",
      "47488/60000 [======================>.......] - ETA: 11s - loss: 0.1915 - acc: 0.9385Epoch 2/2\n",
      "60000/60000 [==============================] - 52s 872us/sample - loss: 0.1215 - acc: 0.9631\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 53s 876us/sample - loss: 0.1654 - acc: 0.9469\n",
      "  768/60000 [..............................] - ETA: 51s - loss: 0.0354 - acc: 0.9909Epoch 2/2\n",
      " 1856/60000 [..............................]60000/60000 [==============================] - 53s 888us/sample - loss: 0.1258 - acc: 0.9614\n",
      " - ETA: 50s - loss: 0.0392 - acc: 0.9898 - ETA: 38s - loss: 0.0414 - acc: 0.9876Epoch 2/2\n",
      "44512/60000 [=====================>........] - ETA: 13s - loss: 0.0422 - acc: 0.987660000/60000 [==============================] - 50s 839us/sample - loss: 0.0434 - acc: 0.9865\n",
      "10000/10000 [==============================] - 7s 686us/sample - loss: 0.0344 - acc: 0.9885\n",
      "54240/60000 [==========================>...] - ETA: 4s - loss: 0.0418 - acc: 0.9875Model saved to disk\n",
      "60000/60000 [==============================] - 50s 840us/sample - loss: 0.0427 - acc: 0.9863\n",
      "60000/60000 [==============================] - 50s 839us/sample - loss: 0.0496 - acc: 0.9850\n",
      "59136/60000 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9875Epoch 1/2\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "60000/60000 [==============================] - 50s 839us/sample - loss: 0.0416 - acc: 0.9875\n",
      "10000/10000 [==============================] 5696/60000 [=>............................] - 4s 435us/sample - loss: 0.0402 - acc: 0.9862\n",
      " 6464/10000 [==================>...........] - ETA: 1s - loss: 0.0276 - acc: 0.9901\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bModel saved to disk\n",
      "10000/10000 [==============================] - 5s 499us/sample - loss: 0.0395 - acc: 0.9864\n",
      " 8480/60000 [===>..........................] - ETA: 32s - loss: 0.4509 - acc: 0.8598Model saved to disk\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "10000/10000 [==============================] 8864/60000 [===>..........................] - 5s 490us/sample - loss: 0.0226 - acc: 0.9920\n",
      "10176/60000 [====>.........................] - ETA: 30s - loss: 0.4048 - acc: 0.8755Model saved to disk\n",
      "16736/60000 [=======>......................] - ETA: 22s - loss: 0.3068 - acc: 0.9071Epoch 1/2\n",
      " 3808/60000 [>.............................] - ETA: 41s - loss: 0.6087 - acc: 0.8072Epoch 1/2\n",
      "23264/60000 [==========>...................] - ETA: 19s - loss: 0.2558 - acc: 0.9233Epoch 1/2\n",
      "60000/60000 [==============================] - 44s 727us/sample - loss: 0.1517 - acc: 0.9544\n",
      "42400/60000 [====================>.........] - ETA: 14s - loss: 0.1568 - acc: 0.9520Epoch 2/2\n",
      "60000/60000 [==============================]17280/60000 [=======>......................] - 50s 831us/sample - loss: 0.1301 - acc: 0.9600\n",
      " - ETA: 37s - loss: 0.0582 - acc: 0.9823Epoch 2/2\n",
      "60000/60000 [==============================] - 53s 879us/sample - loss: 0.1961 - acc: 0.9370\n",
      "24256/60000 [===========>..................] - ETA: 30s - loss: 0.0578 - acc: 0.9824\n",
      "60000/60000 [==============================] - 53s 886us/sample - loss: 0.1372 - acc: 0.9575\n",
      " 1120/60000 [..............................] - ETA: 53s - loss: 0.0643 - acc: 0.9812Epoch 2/2\n",
      "60000/60000 [==============================] - 51s 858us/sample - loss: 0.0561 - acc: 0.9828\n",
      "10000/10000 [==============================] - 7s 663us/sample - loss: 0.0444 - acc: 0.9855\n",
      "44224/60000 [=====================>........] - ETA: 13s - loss: 0.0500 - acc: 0.9847Model saved to disk\n",
      "51232/60000 [========================>.....] - ETA: 7s - loss: 0.0617 - acc: 0.9809Epoch 1/2\n",
      "60000/60000 [==============================] - 49s 811us/sample - loss: 0.0476 - acc: 0.9852\n",
      "60000/60000 [==============================] - 49s 812us/sample - loss: 0.0613 - acc: 0.9812\n",
      "10000/10000 [==============================] - 7s 687us/sample - loss: 0.0332 - acc: 0.9880\n",
      "60000/60000 [==============================] - 50s 826us/sample - loss: 0.0504 - acc: 0.9844\n",
      " - ETA: 36s - loss: 0.3671 - acc: 0.8889 2688/10000 [=======>......................] - ETA: 4s - loss: 0.0465 - acc: 0.9855Model saved to disk\n",
      "10000/10000 [==============================]16960/60000 [=======>......................] - 5s 456us/sample - loss: 0.0287 - acc: 0.9910\n",
      "18208/60000 [========>.....................] - ETA: 27s - loss: 0.2861 - acc: 0.9138Model saved to disk\n",
      " 9856/10000 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9915\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 0.0251 - acc: 0.9916\n",
      "  992/60000 [..............................] - ETA: 54s - loss: 1.2646 - acc: 0.5978Model saved to disk\n",
      " 8960/60000 [===>..........................] - ETA: 30s - loss: 0.3878 - acc: 0.8802Epoch 1/2\n",
      "10528/60000 [====>.........................]30944/60000 [==============>...............] - ETA: 28s - loss: 0.3538 - acc: 0.8903 - ETA: 17s - loss: 0.2135 - acc: 0.9359Epoch 1/2\n",
      "60000/60000 [==============================] - 43s 716us/sample - loss: 0.1497 - acc: 0.9546\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 48s 803us/sample - loss: 0.1308 - acc: 0.9597\n",
      "22080/60000 [==========>...................] - ETA: 31s - loss: 0.0559 - acc: 0.9824Epoch 2/2\n",
      "60000/60000 [==============================] - 52s 870us/sample - loss: 0.1849 - acc: 0.9408\n",
      "59712/60000 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9552Epoch 2/2\n",
      "60000/60000 [==============================] - 52s 861us/sample - loss: 0.1431 - acc: 0.9552\n",
      "  384/60000 [..............................] - ETA: 43s - loss: 0.0399 - acc: 0.9870Epoch 2/2\n",
      "25632/60000 [===========>..................]60000/60000 [==============================] - ETA: 30s - loss: 0.0634 - acc: 0.9808 - 50s 833us/sample - loss: 0.0557 - acc: 0.9826\n",
      "10000/10000 [==============================] - 7s 692us/sample - loss: 0.0404 - acc: 0.9863\n",
      "46592/60000 [======================>.......]35456/60000 [================>.............] - ETA: 11s - loss: 0.0487 - acc: 0.9851 - ETA: 20s - loss: 0.0542 - acc: 0.9831Model saved to disk\n",
      "40320/60000 [===================>..........] - ETA: 16s - loss: 0.0610 - acc: 0.9809Epoch 1/2\n",
      "60000/60000 [==============================] - 51s 849us/sample - loss: 0.0487 - acc: 0.9850\n",
      "10000/10000 [==============================] - 7s 680us/sample - loss: 0.0326 - acc: 0.9892\n",
      "18240/60000 [========>.....................] - ETA: 34s - loss: 0.2834 - acc: 0.9126Model saved to disk\n",
      "60000/60000 [==============================] - 50s 833us/sample - loss: 0.0607 - acc: 0.9815\n",
      "60000/60000 [==============================] - 50s 835us/sample - loss: 0.0509 - acc: 0.9845\n",
      " 8288/10000 [=======================>......] - ETA: 0s - loss: 0.0270 - acc: 0.9906Epoch 1/2\n",
      "10000/10000 [==============================] - 5s 457us/sample - loss: 0.0252 - acc: 0.9914\n",
      "10000/10000 [==============================] - 5s 486us/sample - loss: 0.0356 - acc: 0.9888\n",
      "29696/60000 [=============>................]\n",
      "31072/60000 [==============>...............] - ETA: 20s - loss: 0.2140 - acc: 0.9350Model saved to disk\n",
      "41152/60000 [===================>..........] - ETA: 12s - loss: 0.1846 - acc: 0.9438Epoch 1/2\n",
      "  416/60000 [..............................] - ETA: 2:32 - loss: 2.2737 - acc: 0.1514\n",
      "60000/60000 [==============================] - 43s 712us/sample - loss: 0.1542 - acc: 0.9530\n",
      "30432/60000 [==============>...............] - ETA: 22s - loss: 0.2103 - acc: 0.9360Epoch 2/2\n",
      "60000/60000 [==============================] - 49s 809us/sample - loss: 0.1484 - acc: 0.9548\n",
      "45248/60000 [=====================>........] - ETA: 13s - loss: 0.2581 - acc: 0.9160\n",
      "60000/60000 [==============================] - 53s 890us/sample - loss: 0.2196 - acc: 0.9287\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 53s 886us/sample - loss: 0.1605 - acc: 0.9515\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 50s 834us/sample - loss: 0.0629 - acc: 0.9803\n",
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.0408 - acc: 0.9868\n",
      "39520/60000 [==================>...........]Model saved to disk - ETA: 16s - loss: 0.0585 - acc: 0.9827\n",
      "28416/60000 [=============>................] - ETA: 26s - loss: 0.0802 - acc: 0.9753Epoch 1/2\n",
      "60000/60000 [==============================] - 50s 827us/sample - loss: 0.0571 - acc: 0.9830\n",
      "10000/10000 [==============================] - 7s 707us/sample - loss: 0.0322 - acc: 0.9888\n",
      "53760/60000 [=========================>....] - ETA: 5s - loss: 0.0749 - acc: 0.9769\n",
      "58976/60000 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9823Epoch 1/2\n",
      "60000/60000 [==============================] - 50s 826us/sample - loss: 0.0742 - acc: 0.9773\n",
      "60000/60000 [==============================] - 49s 815us/sample - loss: 0.0595 - acc: 0.9824\n",
      "10000/10000 [==============================] - 6s 598us/sample - loss: 0.0306 - acc: 0.9902\n",
      "10000/10000 [==============================] - 6s 606us/sample - loss: 0.0297 - acc: 0.9896\n",
      "11744/60000 [====>.........................]44608/60000 [=====================>........] - ETA: 33s - loss: 0.3251 - acc: 0.8971 - ETA: 11s - loss: 0.1879 - acc: 0.9429Model saved to disk\n",
      "12544/60000 [=====>........................] - ETA: 32s - loss: 0.3172 - acc: 0.8996Model saved to disk\n",
      "22912/60000 [==========>...................] - ETA: 23s - loss: 0.2340 - acc: 0.9270Epoch 1/2\n",
      "57312/60000 [===========================>..] - ETA: 1s - loss: 0.1650 - acc: 0.9501Epoch 1/2\n",
      "60000/60000 [==============================] - 43s 709us/sample - loss: 0.1613 - acc: 0.9513\n",
      "Epoch 2/2\n",
      "34720/60000 [================>.............] - 46s 767us/sample - loss: 0.1442 - acc: 0.9555\n",
      " - ETA: 22s - loss: 0.3195 - acc: 0.8963Epoch 2/2\n",
      "60000/60000 [==============================] - 53s 891us/sample - loss: 0.2312 - acc: 0.9258\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 52s 860us/sample - loss: 0.0632 - acc: 0.9804\n",
      "60000/60000 [==============================] - 54s 895us/sample - loss: 0.1590 - acc: 0.9503\n",
      "Epoch 2/2\n",
      "10000/10000 [==============================] - 7s 702us/sample - loss: 0.0400 - acc: 0.9870\n",
      "10368/60000 [====>.........................] - ETA: 39s - loss: 0.0609 - acc: 0.9797Model saved to disk\n",
      "\n",
      "\n",
      "epocas: 2\n",
      "CAMADAS3: 64\n",
      "\n",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "17536/60000 [=======>......................] - ETA: 32s - loss: 0.0863 - acc: 0.9731Epoch 1/2\n",
      "60000/60000 [==============================] - 52s 859us/sample - loss: 0.0584 - acc: 0.9825\n",
      "10000/10000 [==============================] - 8s 801us/sample - loss: 0.0332 - acc: 0.9883\n",
      "43712/60000 [====================>.........] - ETA: 14s - loss: 0.0614 - acc: 0.9814\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bModel saved to disk\n",
      "44832/60000 [=====================>........]\n",
      "\n",
      "epocas: 2\n",
      "CAMADAS1: 64\n",
      "\n",
      "26208/60000 [============>.................] - ETA: 13s - loss: 0.0811 - acc: 0.9744\n",
      "49568/60000 [=======================>......] - ETA: 9s - loss: 0.0608 - acc: 0.9817Epoch 1/2\n",
      "60000/60000 [==============================] - 53s 877us/sample - loss: 0.0794 - acc: 0.9753\n",
      "60000/60000 [==============================] - 54s 892us/sample - loss: 0.0594 - acc: 0.9821\n",
      "10000/10000 [==============================] - 8s 786us/sample - loss: 0.0337 - acc: 0.9896\n",
      " 9888/10000 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9887Model saved to disk\n",
      "\n",
      "\n",
      "epocas: 2\n",
      "CAMADAS4: 64\n",
      "\n",
      "\n",
      "10000/10000 [==============================] - 8s 778us/sample - loss: 0.0337 - acc: 0.9888\n",
      "53056/60000 [=========================>....] - ETA: 6s - loss: 0.1227 - acc: 0.9617Model saved to disk\n",
      "20256/60000 [=========>....................] - ETA: 39s - loss: 0.1842 - acc: 0.9417\n",
      "\n",
      "epocas: 2\n",
      "CAMADAS2: 64\n",
      "\n",
      "\n",
      "27136/60000 [============>.................]60000/60000 [==============================] - ETA: 30s - loss: 0.1564 - acc: 0.9507 - 55s 917us/sample - loss: 0.1148 - acc: 0.9643\n",
      "Epoch 2/2\n",
      " 1024/60000 [..............................] - ETA: 41s - loss: 0.0322 - acc: 0.9922Epoch 1/2\n",
      "29760/60000 [=============>................] - ETA: 27s - loss: 0.1474 - acc: 0.9535Epoch 1/2\n",
      "60000/60000 [==============================] - 69s 1ms/sample - loss: 0.1064 - acc: 0.9671\n",
      "30304/60000 [==============>...............]33920/60000 [===============>..............] - ETA: 42s - loss: 0.2047 - acc: 0.9343 - ETA: 33s - loss: 0.0416 - acc: 0.9872Epoch 2/2\n",
      "60000/60000 [==============================] - 80s 1ms/sample - loss: 0.0412 - acc: 0.9872\n",
      "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.1353 - acc: 0.9567\n",
      " 4352/10000 [============>.................] - ETA: 6s - loss: 0.0638 - acc: 0.9789Epoch 2/2\n",
      "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.1124 - acc: 0.9646\n",
      "31616/60000 [==============>...............]\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0398 - acc: 0.9861\n",
      " 5760/60000 [=>............................] - ETA: 1:06 - loss: 0.0546 - acc: 0.9828Model saved to disk\n",
      "10272/60000 [====>.........................] - ETA: 57s - loss: 0.0485 - acc: 0.9849Epoch 1/2\n",
      "60000/60000 [==============================] - 81s 1ms/sample - loss: 0.0393 - acc: 0.9875\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0373 - acc: 0.9891\n",
      "Model saved to disk37952/60000 [=================>............]\n",
      "34912/60000 [================>.............] - ETA: 32s - loss: 0.1598 - acc: 0.9506Epoch 1/2\n",
      "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.0448 - acc: 0.9862\n",
      "60000/60000 [==============================] - 80s 1ms/sample - loss: 0.0415 - acc: 0.9869\n",
      "60000/60000 [==============================] - 75s 1ms/sample - loss: 0.1205 - acc: 0.9623\n",
      "24320/60000 [===========>..................] - ETA: 44s - loss: 0.1738 - acc: 0.9461Epoch 2/2\n",
      "10000/10000 [==============================] - 9s 869us/sample - loss: 0.0270 - acc: 0.9911\n",
      "26368/60000 [============>.................] - ETA: 41s - loss: 0.1664 - acc: 0.9483Model saved to disk\n",
      "10000/10000 [==============================] - 9s 881us/sample - loss: 0.0364 - acc: 0.9884\n",
      "30080/60000 [==============>...............] 5984/60000 [=>............................] - ETA: 35s - loss: 0.1552 - acc: 0.9515 - ETA: 45s - loss: 0.0450 - acc: 0.9881\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bModel saved to disk\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "10688/60000 [====>.........................] - ETA: 39s - loss: 0.0437 - acc: 0.9864Epoch 1/2\n",
      "37920/60000 [=================>............] - ETA: 24s - loss: 0.1374 - acc: 0.9573Epoch 1/2\n",
      "60000/60000 [==============================] - 73s 1ms/sample - loss: 0.1104 - acc: 0.9661\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 73s 1ms/sample - loss: 0.0420 - acc: 0.9869\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0402 - acc: 0.9868\n",
      "55584/60000 [==========================>...]33376/60000 [===============>..............] - ETA: 6s - loss: 0.1401 - acc: 0.9562 - ETA: 35s - loss: 0.0365 - acc: 0.9887Model saved to disk\n",
      "57024/60000 [===========================>..] - ETA: 4s - loss: 0.1119 - acc: 0.9652Epoch 1/2\n",
      "60000/60000 [==============================] - 83s 1ms/sample - loss: 0.1340 - acc: 0.9581\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 84s 1ms/sample - loss: 0.1090 - acc: 0.9659\n",
      "Epoch 2/2\n",
      " 5856/60000 [=>............................] - ETA: 1:17 - loss: 0.0470 - acc: 0.9841\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bBuffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "# q4.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import threading\n",
    "\n",
    "myMutex = threading.Lock()\n",
    "value = \"teste\"\n",
    "\n",
    "numeroDeNeuronios = []\n",
    "numeroDeEpocas = []\n",
    "numeroDeCamadas = []\n",
    "numeroDeDropout = []\n",
    "taxaDeAcertos = []\n",
    "\n",
    "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
    "def thread1Camadas(camadas):\n",
    "\t\n",
    "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
    "\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\n",
    "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
    "\tvaloresDropout = range(10, 40, 10)# Variaremos de 10% em 10%.\n",
    "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
    "\t\n",
    "\t# Testando resultados com diferentes quantidades de epocas.\n",
    "\tfor epocas in [2, 6]:\n",
    "\t\n",
    "\t\t# Testando resultados com diferentes quantidades de filtros.\n",
    "\t\tfor filtros in [32, 64]:\n",
    "\t\t\n",
    "\t\t\t# So para indicar em que passo da execucao estamos.\n",
    "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(filtros) + \"\\n\\n\")\n",
    "\t\n",
    "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
    "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
    "\t\t\tfor taxaDropout in valoresDropout:\n",
    "\t\t\t\n",
    "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
    "\t\t\t\tfor iteracaoMedia in range(1,3):\n",
    "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
    "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\t\t\t\t\t# reshape to be [samples][width][height][pixels]\n",
    "\t\t\t\t\tx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "\t\t\t\t\tx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\t\t\t\t\tmodel = tf.keras.models.Sequential()\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros, kernel_size=(3, 3),\n",
    "\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\tinput_shape=(28, 28, 1)))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (3, 3), activation='relu'))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Flatten())\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
    "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
    "\t\t\t\t\t metrics=['accuracy'])\n",
    "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
    "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
    "\t\t\t\t\tmodel_json = model.to_json()\n",
    "\t\t\t\t\tjson_file = open(\"model_CNN1.json\", \"w\")\n",
    "\t\t\t\t\tjson_file.write(model_json)\n",
    "\t\t\t\t\tjson_file.close()\n",
    "\t\t\t\t\tmodel.save_weights(\"model_CNN1.h5\")\n",
    "\t\t\t\t\tprint(\"Model saved to disk\")\n",
    "\t\t\t\t\tos.getcwd()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\tmyMutex.acquire()\n",
    "\t\t\t\tnumeroDeNeuronios.append(filtros)\n",
    "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
    "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
    "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
    "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
    "\t\t\t\tmyMutex.release()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Reiniciamos a soma.\n",
    "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\t\t\t\n",
    "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
    "def thread2Camadas(camadas):\n",
    "\t\n",
    "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
    "\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\n",
    "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
    "\tvaloresDropout = range(10, 40, 10)# Variaremos de 10% em 10%.\n",
    "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
    "\t\n",
    "\t# Testando resultados com diferentes quantidades de epocas.\n",
    "\tfor epocas in [2, 6]:\n",
    "\t\n",
    "\t\t# Testando resultados com diferentes quantidades de filtros.\n",
    "\t\tfor filtros in [32, 64]:\n",
    "\t\t\n",
    "\t\t\t# So para indicar em que passo da execucao estamos.\n",
    "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(filtros) + \"\\n\\n\")\n",
    "\t\n",
    "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
    "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
    "\t\t\tfor taxaDropout in valoresDropout:\n",
    "\t\t\t\n",
    "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
    "\t\t\t\tfor iteracaoMedia in range(1,3):\n",
    "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
    "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\t\t\t\t\t# reshape to be [samples][width][height][pixels]\n",
    "\t\t\t\t\tx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "\t\t\t\t\tx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\t\t\t\t\tmodel = tf.keras.models.Sequential()\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros, kernel_size=(2, 2),\n",
    "\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\tinput_shape=(28, 28, 1)))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (3, 3), activation='relu'))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (3, 3), activation='relu'))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Flatten())\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
    "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
    "\t\t\t\t\t metrics=['accuracy'])\n",
    "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
    "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
    "\t\t\t\t\tmodel_json = model.to_json()\n",
    "\t\t\t\t\tjson_file = open(\"model_CNN2.json\", \"w\")\n",
    "\t\t\t\t\tjson_file.write(model_json)\n",
    "\t\t\t\t\tjson_file.close()\n",
    "\t\t\t\t\tmodel.save_weights(\"model_CNN2.h5\")\n",
    "\t\t\t\t\tprint(\"Model saved to disk\")\n",
    "\t\t\t\t\tos.getcwd()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\tmyMutex.acquire()\n",
    "\t\t\t\tnumeroDeNeuronios.append(filtros)\n",
    "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
    "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
    "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
    "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
    "\t\t\t\tmyMutex.release()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Reiniciamos a soma.\n",
    "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\t\t\t\n",
    "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
    "def thread3Camadas(camadas):\n",
    "\t\n",
    "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
    "\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\n",
    "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
    "\tvaloresDropout = range(10, 40, 10)# Variaremos de 10% em 10%.\n",
    "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
    "\t\n",
    "\t# Testando resultados com diferentes quantidades de epocas.\n",
    "\tfor epocas in [2, 6]:\n",
    "\t\n",
    "\t\t# Testando resultados com diferentes quantidades de filtros.\n",
    "\t\tfor filtros in [32, 64]:\n",
    "\t\t\n",
    "\t\t\t# So para indicar em que passo da execucao estamos.\n",
    "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(filtros) + \"\\n\\n\")\n",
    "\t\n",
    "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
    "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
    "\t\t\tfor taxaDropout in valoresDropout:\n",
    "\t\t\t\n",
    "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
    "\t\t\t\tfor iteracaoMedia in range(1,3):\n",
    "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
    "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\t\t\t\t\t# reshape to be [samples][width][height][pixels]\n",
    "\t\t\t\t\tx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "\t\t\t\t\tx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\t\t\t\t\tmodel = tf.keras.models.Sequential()\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros, kernel_size=(3, 3),\n",
    "\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\tinput_shape=(28, 28, 1)))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (2, 2), activation='relu'))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Flatten())\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
    "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
    "\t\t\t\t\t metrics=['accuracy'])\n",
    "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
    "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
    "\t\t\t\t\tmodel_json = model.to_json()\n",
    "\t\t\t\t\tjson_file = open(\"model_CNN3.json\", \"w\")\n",
    "\t\t\t\t\tjson_file.write(model_json)\n",
    "\t\t\t\t\tjson_file.close()\n",
    "\t\t\t\t\tmodel.save_weights(\"model_CNN3.h5\")\n",
    "\t\t\t\t\tprint(\"Model saved to disk\")\n",
    "\t\t\t\t\tos.getcwd()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\tmyMutex.acquire()\n",
    "\t\t\t\tnumeroDeNeuronios.append(filtros)\n",
    "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
    "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
    "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
    "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
    "\t\t\t\tmyMutex.release()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Reiniciamos a soma.\n",
    "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\t\t\t\n",
    "# Vamos colocar uma thread para treinar cada rede com um numero especifico de camadas.\n",
    "def thread4Camadas(camadas):\n",
    "\t\n",
    "\t# Para tirar a media das iteracoes, somaremos todas aqui e dividiremos pelo total.\n",
    "\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\n",
    "\t# Os valores que utilizaremos para dropout variarao de 10% a 90% (instrucao abaixo).\n",
    "\tvaloresDropout = range(10, 40, 10)# Variaremos de 10% em 10%.\n",
    "\tvaloresDropout = [i/100 for i in valoresDropout]# Converte de porcentagem para escala de 0 a 1.\n",
    "\t\n",
    "\t# Testando resultados com diferentes quantidades de epocas.\n",
    "\tfor epocas in [2, 6]:\n",
    "\t\n",
    "\t\t# Testando resultados com diferentes quantidades de filtros.\n",
    "\t\tfor filtros in [32, 64]:\n",
    "\t\t\n",
    "\t\t\t# So para indicar em que passo da execucao estamos.\n",
    "\t\t\tprint(\"\\n\\nepocas: \" + str(epocas) + \"\\nCAMADAS\" + str(camadas) + \": \" + str(filtros) + \"\\n\\n\")\n",
    "\t\n",
    "\t\t\t# Este loop fica responsável por treinar com diferentes taxas de dropout.\n",
    "\t\t\t# \"i\" eh o valor a cada iteracao.\n",
    "\t\t\tfor taxaDropout in valoresDropout:\n",
    "\t\t\t\n",
    "\t\t\t\t# Repetimos o treinamento algumas vezes para tirar uma media da eficiencia\n",
    "\t\t\t\tfor iteracaoMedia in range(1,3):\n",
    "\t\t\t\t\tmnist = tf.keras.datasets.mnist\n",
    "\t\t\t\t\t(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\t\t\t\t\t# reshape to be [samples][width][height][pixels]\n",
    "\t\t\t\t\tx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "\t\t\t\t\tx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\t\t\t\t\tx_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\t\t\t\t\tmodel = tf.keras.models.Sequential()\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros, kernel_size=(3, 3),\n",
    "\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\tinput_shape=(28, 28, 1)))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (3, 3), activation='relu'))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3)))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Conv2D(filtros*2, (3, 3), activation='relu'))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3)))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Flatten())\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dropout(taxaDropout))\n",
    "\t\t\t\t\tmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\t\t\t\t\tmodel.compile(optimizer='adam',\n",
    "\t\t\t\t\t loss='sparse_categorical_crossentropy',\n",
    "\t\t\t\t\t metrics=['accuracy'])\n",
    "\t\t\t\t\tmodel.fit(x_train, y_train, epochs=epocas)\n",
    "\t\t\t\t\tvalue = model.evaluate(x_test, y_test)\n",
    "\t\t\t\t\tmodel_json = model.to_json()\n",
    "\t\t\t\t\tjson_file = open(\"model_CNN4.json\", \"w\")\n",
    "\t\t\t\t\tjson_file.write(model_json)\n",
    "\t\t\t\t\tjson_file.close()\n",
    "\t\t\t\t\tmodel.save_weights(\"model_CNN4.h5\")\n",
    "\t\t\t\t\tprint(\"Model saved to disk\")\n",
    "\t\t\t\t\tos.getcwd()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tsomaDasEficienciasDeCadaIteracao = value[1] + somaDasEficienciasDeCadaIteracao\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\tmyMutex.acquire()\n",
    "\t\t\t\tnumeroDeNeuronios.append(filtros)\n",
    "\t\t\t\tnumeroDeEpocas.append(epocas)\n",
    "\t\t\t\tnumeroDeCamadas.append(camadas)\n",
    "\t\t\t\tnumeroDeDropout.append(taxaDropout)\n",
    "\t\t\t\ttaxaDeAcertos.append(somaDasEficienciasDeCadaIteracao/iteracaoMedia)\n",
    "\t\t\t\tmyMutex.release()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Reiniciamos a soma.\n",
    "\t\t\t\tsomaDasEficienciasDeCadaIteracao = 0\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\t\n",
    "\tcamadas1 = threading.Thread(target=thread1Camadas,args=(1,))\n",
    "\tcamadas2 = threading.Thread(target=thread2Camadas,args=(2,))\n",
    "\tcamadas3 = threading.Thread(target=thread3Camadas,args=(3,))\n",
    "\tcamadas4 = threading.Thread(target=thread4Camadas,args=(4,))\n",
    "\t\n",
    "\tcamadas1.start()\n",
    "\tcamadas2.start()\n",
    "\tcamadas3.start()\n",
    "\tcamadas4.start()\n",
    "\t\n",
    "\ttry:\n",
    "\t\tcamadas4.join(); \n",
    "\texcept:\n",
    "\t\tpass;\n",
    "\t\t\n",
    "\ttry:\n",
    "\t\tcamadas3.join(); \n",
    "\texcept:\n",
    "\t\tpass;\n",
    "\t\t\n",
    "\ttry:\n",
    "\t\tcamadas2.join(); \n",
    "\texcept:\n",
    "\t\tpass;\n",
    "\t\t\n",
    "\ttry:\n",
    "\t\tcamadas1.join(); \n",
    "\texcept:\n",
    "\t\tpass;\n",
    "\t\t\n",
    "\tlistasFile = open(\"listasCONV.txt\", \"w\")\n",
    "\tlistasFile.write(str(numeroDeNeuronios) + \"\\n\")\n",
    "\tlistasFile.write(str(numeroDeEpocas) + \"\\n\")\n",
    "\tlistasFile.write(str(numeroDeCamadas) + \"\\n\")\n",
    "\tlistasFile.write(str(numeroDeDropout) + \"\\n\")\n",
    "\tlistasFile.write(str(taxaDeAcertos) + \"\\n\")\n",
    "\tlistasFile.close()\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "Melt9--4vdJK",
    "outputId": "b777fe68-c310-4ad7-9181-6f9636dbe637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 79s 1ms/sample - loss: 0.2189 - acc: 0.9315\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0785 - acc: 0.9787\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0625 - acc: 0.9830\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0532 - acc: 0.9855\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0457 - acc: 0.9871\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0438 - acc: 0.9879\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0376 - acc: 0.9899\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0353 - acc: 0.9904\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0340 - acc: 0.9907\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 78s 1ms/sample - loss: 0.0335 - acc: 0.9909\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 0.0298 - acc: 0.9935\n",
      "Model saved to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q4Final.py\n",
    "import tensorflow as tf\n",
    "import os\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][width][height][pixels]\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
    " activation='relu',\n",
    "input_shape=(28, 28, 1)))\n",
    "model.add(tf.keras.layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    " loss='sparse_categorical_crossentropy',\n",
    " metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "model.evaluate(x_test, y_test)\n",
    "model_json = model.to_json()\n",
    "json_file = open(\"model_CNN.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "model.save_weights(\"model_CNN.h5\")\n",
    "print(\"Model saved to disk\")\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "4373987a9a754fee8a260ba619c4e8b3",
   "lastKernelId": "374183ba-9524-411d-89a2-c62ca5287fa9"
  },
  "accelerator": "GPU",
  "celltoolbar": "Attachments",
  "colab": {
   "collapsed_sections": [],
   "name": "EA072_EFC1_Patrick_de_Carvalho_Tavares_Rezende_Ferreira_175480.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
